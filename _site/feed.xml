<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.8.5">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2020-03-13T15:45:51+02:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Performance Matters</title><subtitle>A blog about low-level software and hardware performance.</subtitle><author><name>Travis Downs</name><email>travis.downs@gmail.com</email></author><entry><title type="html">Adding Staticman Comments</title><link href="http://localhost:4000/blog/2020/02/05/now-with-comments.html" rel="alternate" type="text/html" title="Adding Staticman Comments" /><published>2020-02-05T00:00:00+02:00</published><updated>2020-02-05T00:00:00+02:00</updated><id>http://localhost:4000/blog/2020/02/05/now-with-comments</id><content type="html" xml:base="http://localhost:4000/blog/2020/02/05/now-with-comments.html">&lt;p&gt;I’ve added comments to my blog. You can find the existing comments, if any, and the new comment form &lt;a href=&quot;#comment-section&quot;&gt;at the bottom&lt;/a&gt; of any post.&lt;/p&gt;

&lt;p&gt;I thought this would take a couple hours, but it actually took &lt;strong&gt;[REDACTED]&lt;/strong&gt;. Estimates are hard.&lt;/p&gt;

&lt;p&gt;Here’s what I did.&lt;/p&gt;

&lt;h2 id=&quot;table-of-contents&quot;&gt;Table of Contents&lt;/h2&gt;

&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#table-of-contents&quot; id=&quot;markdown-toc-table-of-contents&quot;&gt;Table of Contents&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#introduction&quot; id=&quot;markdown-toc-introduction&quot;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#set-up-github-bot-account&quot; id=&quot;markdown-toc-set-up-github-bot-account&quot;&gt;Set Up GitHub Bot Account&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#generate-personal-access-token&quot; id=&quot;markdown-toc-generate-personal-access-token&quot;&gt;Generate Personal Access Token&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#set-up-recaptcha&quot; id=&quot;markdown-toc-set-up-recaptcha&quot;&gt;Set Up reCAPTCHA&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#set-up-the-blog-repository-configuration&quot; id=&quot;markdown-toc-set-up-the-blog-repository-configuration&quot;&gt;Set Up the Blog Repository Configuration&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#configuring-staticmanyml&quot; id=&quot;markdown-toc-configuring-staticmanyml&quot;&gt;Configuring staticman.yml&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#configuring-_configyml&quot; id=&quot;markdown-toc-configuring-_configyml&quot;&gt;Configuring _config.yml&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#set-up-the-api-bridge&quot; id=&quot;markdown-toc-set-up-the-api-bridge&quot;&gt;Set Up the API Bridge&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#generate-an-rsa-keypair&quot; id=&quot;markdown-toc-generate-an-rsa-keypair&quot;&gt;Generate an RSA Keypair&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#sign-up-for-heroku&quot; id=&quot;markdown-toc-sign-up-for-heroku&quot;&gt;Sign Up for Heroku&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#deploy-staticman-bridge-to-heroku&quot; id=&quot;markdown-toc-deploy-staticman-bridge-to-heroku&quot;&gt;Deploy Staticman Bridge to Heroku&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#configure-bridge-secrets&quot; id=&quot;markdown-toc-configure-bridge-secrets&quot;&gt;Configure Bridge Secrets&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#invite-and-accept-bot-to-blog-repo&quot; id=&quot;markdown-toc-invite-and-accept-bot-to-blog-repo&quot;&gt;Invite and Accept Bot to Blog Repo&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#integrate-comments-into-site&quot; id=&quot;markdown-toc-integrate-comments-into-site&quot;&gt;Integrate Comments Into Site&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#markdown-part&quot; id=&quot;markdown-toc-markdown-part&quot;&gt;Markdown Part&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#thanks&quot; id=&quot;markdown-toc-thanks&quot;&gt;Thanks&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#references&quot; id=&quot;markdown-toc-references&quot;&gt;References&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;I am using &lt;a href=&quot;https://staticman.net/&quot;&gt;staticman&lt;/a&gt;, created by &lt;a href=&quot;https://github.com/eduardoboucas&quot;&gt;Eduardo Bouças&lt;/a&gt;, as my comments system for this static site.&lt;/p&gt;

&lt;p&gt;The basic flow for comment submission is as follows:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;A reader submits the comment form on a blog post.&lt;/li&gt;
  &lt;li&gt;Javascript&lt;sup id=&quot;fnref:backup&quot;&gt;&lt;a href=&quot;#fn:backup&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt; attached to the form submits it to my &lt;em&gt;staticman API bridge&lt;sup id=&quot;fnref:bridge&quot;&gt;&lt;a href=&quot;#fn:bridge&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;/em&gt; running on Heroku.&lt;/li&gt;
  &lt;li&gt;The API bridge does some validation of the request and submits a &lt;a href=&quot;https://github.com/travisdowns/travisdowns.github.io/issues&quot;&gt;pull request&lt;/a&gt; to the github repo hosting my blog, consisting of a .yml file with the post content and meta data.&lt;/li&gt;
  &lt;li&gt;When I accept the pull request, it triggers a regeneration and republishing of the content (this is a GitHub pages feature), so the reply appears almost immediately&lt;sup id=&quot;fnref:cache&quot;&gt;&lt;a href=&quot;#fn:cache&quot; class=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt;.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Here are the detailed steps to get this working. There are several other tutorials out there, with varying states of exhaustiveness, some of which
I found only after writing most of this, but I’m going to add the pile anyways. There have been several changes to deploying staticman which mean that existing resources (and this one, of course) are marked by which “era” they were written in.&lt;/p&gt;

&lt;p&gt;The major changes are:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;At one point the idea was that everyone would use the public staticman API bridge, but this proved unsustainable. A large amount of the work in setting up staticman is associated with running your own instance of the bridge.&lt;/li&gt;
  &lt;li&gt;There are three version of the staticman API: v1, v2 and v3. This guide uses v2 (although v3 is almost identical&lt;sup id=&quot;fnref:v3&quot;&gt;&lt;a href=&quot;#fn:v3&quot; class=&quot;footnote&quot;&gt;4&lt;/a&gt;&lt;/sup&gt;), but the v1 version is considerably different.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;set-up-github-bot-account&quot;&gt;Set Up GitHub Bot Account&lt;/h2&gt;

&lt;p&gt;You’ll want to create a GitHub &lt;em&gt;bot account&lt;/em&gt; which will be the account that the API bridge uses to actually submit the pull requests to your blog repository. In principle, you can skip this step entirely and simply use your existing GitHub account, but I wouldn’t recommend it:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;You’ll be generating a &lt;em&gt;personal access token&lt;/em&gt; for this account, and uploading it to the cloud (Heroku) and if this somehow gets compromised, it’s better that it’s a throwaway bot account than your real account.&lt;/li&gt;
  &lt;li&gt;Having a dedicated account makes it easy to segregate work done by the bot, versus what you’ve done yourself. That is, you probably don’t want all the commits and pushes the bot does to show up on your personal account.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The &lt;em&gt;bot account&lt;/em&gt; is nothing special: it is just a regular personal account that you’ll only be using from the API bridge. So, open a private browser window, go to &lt;a href=&quot;https://github.com&quot;&gt;GitHub&lt;/a&gt; and choose “Sign Up”. Call your bot something specific, which I’ll refer to as &lt;em&gt;GITHUB-BOT-NAME&lt;/em&gt; from here forwards.&lt;/p&gt;

&lt;h3 id=&quot;generate-personal-access-token&quot;&gt;Generate Personal Access Token&lt;/h3&gt;

&lt;p&gt;Next, you’ll need to generate a GitHub &lt;em&gt;personal access token&lt;/em&gt;, for your bot account. The &lt;a href=&quot;https://help.github.com/en/github/authenticating-to-github/creating-a-personal-access-token-for-the-command-line&quot;&gt;GitHub doc&lt;/a&gt; does a better job of explaining this than I can. If you just want everything to work for sure now and in the future, select every single scope when it prompts you, but if you care about security you should only need the &lt;em&gt;repo&lt;/em&gt; and &lt;em&gt;user&lt;/em&gt; scopes (today):&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Repo scope:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/now-with-comments/scopes-repo.png&quot; alt=&quot;Repo scope&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;User scope:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/now-with-comments/scopes-user.png&quot; alt=&quot;User scope&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Copy and paste the displayed token somewhere safe: you’ll need this token in a later step where I’ll refer to it as  &lt;em&gt;${github_token}&lt;/em&gt;. Once you close this page there is no way to recover the access token.&lt;/p&gt;

&lt;h2 id=&quot;set-up-recaptcha&quot;&gt;Set Up reCAPTCHA&lt;/h2&gt;

&lt;p&gt;You are going to gate comment submission being reCAPTCHA or a similar system so you don’t get destroyed by spam (even if you have moderation enabled, dealing with all the pull requests will probably be tiring).&lt;/p&gt;

&lt;p&gt;Go to &lt;a href=&quot;https://developers.google.com/recaptcha&quot;&gt;reCAPTCHA&lt;/a&gt; and sign up if you haven’t already, and create a new site. We are going to use the “v2, Checkbox” variant (&lt;a href=&quot;https://developers.google.com/recaptcha/docs/display&quot;&gt;docs here&lt;/a&gt;), although I’m interested to hear how it works out with other variants.&lt;/p&gt;

&lt;p&gt;You will need the reCAPTCHA &lt;em&gt;site key&lt;/em&gt; and &lt;em&gt;secret key&lt;/em&gt; for configuration later on.&lt;/p&gt;

&lt;h2 id=&quot;set-up-the-blog-repository-configuration&quot;&gt;Set Up the Blog Repository Configuration&lt;/h2&gt;

&lt;p&gt;You’ll need to include configuration for staticman in two separate places in your blog repository: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;_config.yml&lt;/code&gt; (the primary Jekyll config file) and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;staticman.yml&lt;/code&gt;, both at the top level of the repository.&lt;/p&gt;

&lt;p&gt;In general, the stuff that goes in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;_config.yml&lt;/code&gt; is for use within the static generation phase of your site, e.g., controlling the generation of the comment form and the associated javascipt. The stuff in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;staticman.yml&lt;/code&gt; isn’t used during generation, but is used dynamically by the API bridge (read directly from GitHub on each request) to configure the activities of the bridge. A few thigns are duplicated in both places.&lt;/p&gt;

&lt;h3 id=&quot;configuring-staticmanyml&quot;&gt;Configuring staticman.yml&lt;/h3&gt;

&lt;p&gt;Most of the configuration for the ABI bridge is set in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;staticman.yml&lt;/code&gt; which lives in the top level of your &lt;em&gt;blog repository&lt;/em&gt;. This means that one API bridge can support many different blog repositories, each with their own configuration (indeed, this feature was critical for the original design of a shared ABI bridge).&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/eduardoboucas/staticman/blob/master/staticman.sample.yml&quot;&gt;Here’s a sample file&lt;/a&gt; from the staticman GitHub repository, but you might want to use this one (TODO link) from my repository as it is a bit more fleshed out.&lt;/p&gt;

&lt;p&gt;The main things you want to change are shown below.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;reCaptcha.secret&lt;/code&gt; property is an &lt;a href=&quot;https://staticman.net/docs/encryption&quot;&gt;&lt;em&gt;encrypted&lt;/em&gt;&lt;/a&gt; version of the &lt;em&gt;site secret&lt;/em&gt; you get from the reCAPTCHA admin console. Copy the secret from the admin console, and paste it at the end of the following URL in your browser:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;https://${bridge_app_name}.herokuapp.com/v2/encrypt/{$recaptcha-site-secret}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;You should get a blob of characters as a result (considerably longer than the original secret) – it is &lt;em&gt;this&lt;/em&gt; value that you need to include as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;reCaptcha.secret&lt;/code&gt; in the configuration below (and again in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;_config.yml&lt;/code&gt;).&lt;/p&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
&lt;span class=&quot;c1&quot;&gt;# all of these fields are nested under the comments key, which corresponds to the final element&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# of the API bridge enpoint, i.e., you can different configurations even within the same staticman.yml&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# file all under different keys&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;comments&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;

  &lt;span class=&quot;c1&quot;&gt;# There are many more required config values here, not shown: &lt;/span&gt;
  &lt;span class=&quot;c1&quot;&gt;# use the file linked above as a template&lt;/span&gt;

  &lt;span class=&quot;c1&quot;&gt;# I guess used only for email notifications?&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Performance&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Matters&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Blog&quot;&lt;/span&gt;

  &lt;span class=&quot;c1&quot;&gt;# You may want a different set of &quot;required fields&quot;. Staticman will&lt;/span&gt;
  &lt;span class=&quot;c1&quot;&gt;# reject posts without all of these fields&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;requiredFields&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;pi&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;name&quot;&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;email&quot;&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;message&quot;&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;]&lt;/span&gt;

  &lt;span class=&quot;c1&quot;&gt;# you are going to want reCaptcha set up&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;reCaptcha&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;enabled&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;no&quot;&gt;true&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;siteKey&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;6LcWstQUAAAAALoGBcmKsgCFbMQqkiGiEt361nK1&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;secret&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;a big encrypted secret (see Note above)&lt;/span&gt;


&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;configuring-_configyml&quot;&gt;Configuring _config.yml&lt;/h3&gt;

&lt;p&gt;The remainder of the configuration goes in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;_config.yml&lt;/code&gt;. Here’s the configuration I had to add:&lt;/p&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# The URL for the staticman API bridge endpoint&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# You will want to modify some of the values:&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;#  ${github-username}: the username of the account with which you publish your blog&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;#  ${blog-repo}: the name of your blog repository in github&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;#  master: this the branch out of which your blog is published, often master or gh-pages&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;#  ${bridge_app_name}: the name you chose in Heroku for your bridge API&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;#  comments: the so-called property, this defines the key in staticman.yml where the configuration is found&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;#&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# for me, this line reads:&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# https://staticman-travisdownsio.herokuapp.com/v2/entry/travisdowns/travisdowns.github.io/master/comments&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;staticman_url&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;https://${bridge_app_name}.herokuapp.com/v2/entry/${github-username}/${blog-repo}/master/comments&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# reCaptcha configuration info: the exact same site key and *encrypted* secret that you used in staticman.yml&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# I personally don't think the secret needs to be included in the generated site, but the staticman API bridge uses&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# it to ensure the site configuration and bridge configuration match (but why not just compare the site key?)&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;reCaptcha&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;siteKey&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;6LcWstQUAAAAALoGBcmKsgCFbMQqkiGiEt361nK1&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;secret&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;exactly the same secret as the staticman.yml file&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;set-up-the-api-bridge&quot;&gt;Set Up the API Bridge&lt;/h2&gt;

&lt;p&gt;This section covers deploying a private instances of the API bridge to Heroku.&lt;/p&gt;

&lt;h3 id=&quot;generate-an-rsa-keypair&quot;&gt;Generate an RSA Keypair&lt;/h3&gt;

&lt;p&gt;This keypair will be used to encrypt secrets that will be stored in public places, such as your reCAPTCHA site secret. The sececrets will be encrypted with the public half of the keypair, and decriped in the Bridge API server with the private part.&lt;/p&gt;

&lt;p&gt;Use the following on your local to generate to generate the pair:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ssh-keygen -m PEM -t rsa -b 4096 -C &quot;staticman key&quot; -f ~/.ssh/staticman_key
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Don’t use any passphrase&lt;sup id=&quot;fnref:pass&quot;&gt;&lt;a href=&quot;#fn:pass&quot; class=&quot;footnote&quot;&gt;5&lt;/a&gt;&lt;/sup&gt;. You can change the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-f&lt;/code&gt; argument if you want to save the key somewhere else, in which case you’ll have to use the new location when setting up the Heroku config below.&lt;/p&gt;

&lt;p&gt;You can verify the key was genreated by running:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;head -2 ~/.ssh/staticman_key
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Which should output something like:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;-----BEGIN RSA PRIVATE KEY-----
MIIJKAIBAAKCAgEAud7+fPWXzuxCoyyGbQTYCGi9C1N984roI/Tr7yJi074F+Cfp
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Your second line will vary of course, but the first line must be &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-----BEGIN RSA PRIVATE KEY-----&lt;/code&gt;. If you see something else, perhaps mentioning &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;OPENSSH PRIVATE KEY&lt;/code&gt;, it won’t work.&lt;/p&gt;

&lt;h3 id=&quot;sign-up-for-heroku&quot;&gt;Sign Up for Heroku&lt;/h3&gt;

&lt;p&gt;The original idea of staticman was to have a public API bridge that everyone uses for free. However, in practice this hasn’t proved sustainable as whatever free tier the thing was running on tends to hit its limits and then the fun stops. So the current recommendation is to set up a free instance of the API bridge on Heroku. So let’s do that.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://signup.heroku.com/&quot;&gt;Sign up&lt;/a&gt; for a free account on Heroku. No credit card is required and a free account should give you enough juice for at least 1,000 comments a month&lt;sup id=&quot;fnref:juice&quot;&gt;&lt;a href=&quot;#fn:juice&quot; class=&quot;footnote&quot;&gt;6&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;h3 id=&quot;deploy-staticman-bridge-to-heroku&quot;&gt;Deploy Staticman Bridge to Heroku&lt;/h3&gt;

&lt;p&gt;The easiest way to do this is simply to click the &lt;em&gt;Deploy to Heroku&lt;/em&gt; button in the &lt;a href=&quot;https://github.com/eduardoboucas/staticman&quot;&gt;README on the staticman repo&lt;/a&gt;:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/now-with-comments/deploy.png&quot; alt=&quot;Deploy&quot; width=&quot;50%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;You’ll probably see some building stuff (TODO: try this).&lt;/p&gt;

&lt;h3 id=&quot;configure-bridge-secrets&quot;&gt;Configure Bridge Secrets&lt;/h3&gt;

&lt;p&gt;The bridge needs a couple of secrets to do its job:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The &lt;em&gt;GitHub personal access token&lt;/em&gt; of your bot account. This lets it do work on behalf of your bot account (in particular, submit pull requests to your blog repository).&lt;/li&gt;
  &lt;li&gt;The private key of the keypair you generated earlier.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;If you want, you can add both of these through the Heroku web dashboard: go to Settings -&amp;gt; Reveal Config Vars, and enter them &lt;a href=&quot;/assets/now-with-comments/config-vars.png&quot;&gt;like this&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;However, you might as well get familiar with the Heroku command line, because it’s pretty cool and allows you to complete this flow without having your GitHub token flow through your clipboard and makes it easy to remove the newline characters in the private key.&lt;/p&gt;

&lt;p&gt;Follow &lt;a href=&quot;https://devcenter.heroku.com/articles/heroku-cli&quot;&gt;the instructions&lt;/a&gt; to install and login to the Heroku CLI, then issue the following commands from any directory (note that &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;${github_token}&lt;/code&gt; is the &lt;em&gt;personal access token&lt;/em&gt; you generated earlier: copy and paste it into the command):&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;heroku config:add &lt;span class=&quot;nt&quot;&gt;--app&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;bridge_app_name&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;RSA_PRIVATE_KEY=&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;$(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;cat&lt;/span&gt; ~/.ssh/staticman_key | &lt;span class=&quot;nb&quot;&gt;tr&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-d&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'\n'&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;
heroku config:add &lt;span class=&quot;nt&quot;&gt;--app&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;bridge_app_name&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;GITHUB_TOKEN=&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;github_token&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Here, the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;tr -d '\n'&lt;/code&gt; part of the pipeline is removing the newlines from the private key, since Heroku config variables can’t handle them and/or the API bridge can’t handle them.&lt;/p&gt;

&lt;p&gt;You can check that the config was correctly set by outputting it as follows:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;heroku config &lt;span class=&quot;nt&quot;&gt;--app&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;bridge_app_name&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;invite-and-accept-bot-to-blog-repo&quot;&gt;Invite and Accept Bot to Blog Repo&lt;/h2&gt;

&lt;p&gt;Finally, you need to invite your GitHub &lt;em&gt;bot account&lt;/em&gt; that you created earlier to your blog repository&lt;sup id=&quot;fnref:whycollab&quot;&gt;&lt;a href=&quot;#fn:whycollab&quot; class=&quot;footnote&quot;&gt;7&lt;/a&gt;&lt;/sup&gt; and accept the invite.&lt;/p&gt;

&lt;p&gt;Open your blog repository, go to &lt;em&gt;Settings -&amp;gt; Collaborators&lt;/em&gt; and search for and add the GitHub bot account that you created earlier as a collaborator:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/now-with-comments/add-collab.png&quot; alt=&quot;Adding Collaborators&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Next, accept&lt;sup id=&quot;fnref:invite&quot;&gt;&lt;a href=&quot;#fn:invite&quot; class=&quot;footnote&quot;&gt;8&lt;/a&gt;&lt;/sup&gt; the invitation using the bridge API, by going to the following URL:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;https://${bridge_app_name}.herokuapp.com/v2/connect/${github-username}/${blog-repo}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;You should see &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;OK!&lt;/code&gt; as the output if it worked: this only appears &lt;em&gt;once&lt;/em&gt; when the invitation got accepted, at all other times it will show &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Invitation not found&lt;/code&gt;.&lt;/p&gt;

&lt;h2 id=&quot;integrate-comments-into-site&quot;&gt;Integrate Comments Into Site&lt;/h2&gt;

&lt;p&gt;Finally, you need to integrate code to display the existing comments and submit new comments.&lt;/p&gt;

&lt;p&gt;I used a mash up of commenting code from the &lt;a href=&quot;https://spinningnumbers.org&quot;&gt;spinningnumbers.org&lt;/a&gt; blog as well as the staticman integration in the &lt;a href=&quot;https://mmistakes.github.io/minimal-mistakes/docs/configuration/#static-based-comments-via-staticman&quot;&gt;minimal mistakes theme&lt;/a&gt;. The advantage the former has over the latter is that the comments allow one level of nesting (replies to top-level comments are nested beneath it).&lt;/p&gt;

&lt;p&gt;I planed to extract the associated markdown, liquid and JavaScript code to a separate repository as a single point where people could collaborate on this part of the integration, but man I’ve already spent way to long on this. I may still do it, but for now here’s how I did the integration.&lt;/p&gt;

&lt;h3 id=&quot;markdown-part&quot;&gt;Markdown Part&lt;/h3&gt;

&lt;p&gt;The key thing you need to do is include a blob of HTML and associated JavaScript in any page where you want to display and accept comments. I do this as follows:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;{% if page.comments == true %}
  {% include comments.html %}
{% endif %}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;You can paste it into any post, or better add it to the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;footer.html&lt;/code&gt; include or something like that (details depend on your theme). The invariant is that wherever this appears, the existing comments appear, followed by a form to submit new comments. You can see the &lt;a href=&quot;https://github.com/travisdowns/blog-test/blob/master/_includes/comments.html&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;comments.html&lt;/code&gt; include here&lt;/a&gt; – in turn, it includes &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;comment.html&lt;/code&gt; (once per comment, generates the comment html) and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;comment_form.html&lt;/code&gt; which generates the new comment form.&lt;/p&gt;

&lt;p&gt;This ultimately includes &lt;a href=&quot;https://github.com/travisdowns/blog-test/blob/master/_includes/comments.html#L35&quot;&gt;external JavaScript&lt;/a&gt; for JQuery and reCAPTCHA, as well as &lt;a href=&quot;https://github.com/travisdowns/blog-test/blob/master/assets/main.js&quot;&gt;main.js&lt;/a&gt; which includes the JavaScript to implement the replies (moving the form when the “reply to” button is clicked, and submitting the form via AJAX to the API bridge).&lt;/p&gt;

&lt;p&gt;So to use this integration in your &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Jekyll&lt;/code&gt; blow you need to:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Copy the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;_includes/comment.html&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;_includes/comments.html&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;_includes/comment_form.html&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;assets/main.js&lt;/code&gt;, and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;_sass/comment-styles.css&lt;/code&gt; files to your blog repository.&lt;/li&gt;
  &lt;li&gt;Include &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;@import &quot;comment-styles&quot;;&lt;/code&gt; in your &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;assets/main.scss&lt;/code&gt; file. If you don’t have one, you’ll need to create it following the rules for your theme. Usually this just means a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;main.scss&lt;/code&gt; with empty front-matter and an &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;@import &quot;your-theme&quot;;&lt;/code&gt; line to import the theme SCSS. Alternately, you could avoid putting anything in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;main.scss&lt;/code&gt; and just include the comment styles as a separate file, but this adds another request to each post.&lt;/li&gt;
  &lt;li&gt;Do the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;include comments.html&lt;/code&gt; thing shown above in an appropriate place in your template/theme.&lt;/li&gt;
  &lt;li&gt;Set &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;comments: true&lt;/code&gt; in the front matter of posts you want to have comments (or set it as a default in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;_config.yml&lt;/code&gt;).&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;thanks&quot;&gt;Thanks&lt;/h2&gt;

&lt;p&gt;Thanks to Eduardo Boucas for creating staticman.&lt;/p&gt;

&lt;p&gt;Thanks to &lt;a href=&quot;https://spinningnumbers.org/&quot;&gt;Willy McAllister&lt;/a&gt; for nested comment display work I unabashedly cribbed, and helping me sort out an RSA key genreation problem.&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;p&gt;Things that were handy references while getting this working.&lt;/p&gt;

&lt;p&gt;This comment on &lt;a href=&quot;https://github.com/eduardoboucas/staticman/issues/318#issuecomment-552755165&quot;&gt;GitHub issue #318&lt;/a&gt; was the list that I more or less follwed (I didn’t use the dev branch though).&lt;/p&gt;

&lt;p&gt;Willy McAllister describes setting up staticman &lt;a href=&quot;https://spinningnumbers.org/a/staticman.html&quot;&gt;in this post&lt;/a&gt; – his implemented of nested comments forms the basis the one I used.&lt;/p&gt;

&lt;p&gt;Another [list of steps])https://gist.github.com/jannispaul/3787603317fc9bbb96e99c51fe169731) to get staticman working and some troubleshooting.&lt;/p&gt;

&lt;p&gt;Michael Rose, the author of minimal mistakes Jekyll theme &lt;a href=&quot;https://mademistakes.com/articles/improving-jekyll-static-comments/&quot;&gt;describes setting up nested staticman comments&lt;/a&gt; – I cribbed some stuff from here such as the submitting spinner.&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;div class=&quot;footnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:backup&quot;&gt;
      &lt;p&gt;If javascript is disabled, a regular POST action takes over. &lt;a href=&quot;#fnref:backup&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:bridge&quot;&gt;
      &lt;p&gt;I don’t think you’ll find this &lt;em&gt;bridge&lt;/em&gt; term in the official documentation, but I’m going to use it here. &lt;a href=&quot;#fnref:bridge&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:cache&quot;&gt;
      &lt;p&gt;Well, subject to whatever edge caching GitHub pages is using – btw you can bust the cache by appending any random query parameter to the page: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;...post.html?foo=1234&lt;/code&gt;. &lt;a href=&quot;#fnref:cache&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:v3&quot;&gt;
      &lt;p&gt;v3 mostly just extends to the URL format for the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/event&lt;/code&gt; endpoint to include the hosting provider (either GitHub or GitLab), allowing the use of GitHub in addition to GitLab. Almost everything in this guide would remain unchanged. &lt;a href=&quot;#fnref:v3&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:pass&quot;&gt;
      &lt;p&gt;You could use a passphrase, but then you’ll have to change the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;cat&lt;/code&gt; used below to echo the key into the Heroku config. If you want to be super safe, best is to generate the key to a transient location like ramfs and then simply delete the private portion after you’ve uploaded it to the Heroku config. &lt;a href=&quot;#fnref:pass&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:juice&quot;&gt;
      &lt;p&gt;In particular, the &lt;em&gt;unverified&lt;/em&gt; (no credit card) free tier gives you 550 hours of uptime a month, and since the &lt;em&gt;dyno&lt;/em&gt; (heroku speak for their on-demand host) sleeps after 30 minutes, I figure you can handle 550/0.5 = 1100 sparsely submitted comments. Of course, if comments come in bursts, you could handle much more than that, since you’ve already “paid” for the 30 minute uptime. &lt;a href=&quot;#fnref:juice&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:whycollab&quot;&gt;
      &lt;p&gt;The bot needs to be a collaborator to, at a minimum, commit comments to the repository, and to delete branches (using the delete branches webhook which cleans up comment related branches). However, it is possible to not use either of these features if you have moderation enabled (in which case comments arrive as a PR, which doesn’t require any particular permissions), and aren’t using the webhook. So maybe you could do without the collaborator status in that case? I haven’t tested it. &lt;a href=&quot;#fnref:whycollab&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:invite&quot;&gt;
      &lt;p&gt;I guess you can also just accept the invitation by opening the email sent to you by github and following the link there. This workflow involving the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;v2/connect&lt;/code&gt; endpoint probably made more sense when the API was meant to be shared among many uses using a common github bot account. &lt;a href=&quot;#fnref:invite&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><author><name>Travis Downs</name><email>travis.downs@gmail.com</email></author><category term="blog" /><category term="meta" /><category term="staticman" /><summary type="html"></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/assets/now-with-comments/twitter-card.png" /><media:content medium="image" url="http://localhost:4000/assets/now-with-comments/twitter-card.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">The Hunt for the Fastest Zero</title><link href="http://localhost:4000/blog/2020/01/20/zero.html" rel="alternate" type="text/html" title="The Hunt for the Fastest Zero" /><published>2020-01-20T00:00:00+02:00</published><updated>2020-01-20T00:00:00+02:00</updated><id>http://localhost:4000/blog/2020/01/20/zero</id><content type="html" xml:base="http://localhost:4000/blog/2020/01/20/zero.html">&lt;p&gt;Let’s say I ask you to fill a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;char&lt;/code&gt; array of size &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;n&lt;/code&gt; with zeros. I don’t know why, exactly, but please play along for now.&lt;/p&gt;

&lt;p&gt;If this were C, we would probably reach for &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;memset&lt;/code&gt;, but let’s pretend we are trying to write idiomatic C++ instead.&lt;/p&gt;

&lt;p&gt;You might come up with something like&lt;sup id=&quot;fnref:function&quot;&gt;&lt;a href=&quot;#fn:function&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;:&lt;/p&gt;

&lt;div class=&quot;language-c++ highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;fill1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;char&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;size_t&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fill&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;I’d give this solution full marks. In fact, I’d call it more or less the canonical modern C++ solution to the problem.&lt;/p&gt;

&lt;p&gt;What if told you there was a solution that was up to about 29 times faster? It doesn’t even require sacrificing any goats to the C++ gods, either: just adding three characters:&lt;/p&gt;

&lt;div class=&quot;language-c++ highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;fill2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;char&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;size_t&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fill&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;sc&quot;&gt;'\0'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Yes, switching &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;0&lt;/code&gt; to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;'\0'&lt;/code&gt; speeds this up by nearly a factor of &lt;em&gt;thirty&lt;/em&gt; on my &lt;abbr title=&quot;Intel's Skylake (client) architecture, aka 6th Generation Intel Core i3,i5,i7&quot;&gt;SKL&lt;/abbr&gt; box&lt;sup id=&quot;fnref:qb&quot;&gt;&lt;a href=&quot;#fn:qb&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;, at least with my default compiler (gcc) and optimization level (-O2):&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Function&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;Bytes / Cycle&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;fill1&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;1.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;fill2&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;29.1&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;The why is becomes obvious if you look at the &lt;a href=&quot;https://godbolt.org/z/O_f3Jx&quot;&gt;assembly&lt;/a&gt;:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;fill1:&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&quot;language-nasm highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;fill1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;char&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;unsigned&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;long&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;add&lt;/span&gt;     &lt;span class=&quot;n&quot;&gt;rsi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rdi&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;cmp&lt;/span&gt;     &lt;span class=&quot;n&quot;&gt;rsi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rdi&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;je&lt;/span&gt;      &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;L1&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;L3&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;mov&lt;/span&gt;     &lt;span class=&quot;n&quot;&gt;BYTE&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;PTR&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[rdi],&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;  &lt;span class=&quot;c&quot;&gt;; store 0 into memory at [rdi]             &lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;add&lt;/span&gt;     &lt;span class=&quot;n&quot;&gt;rdi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;             &lt;span class=&quot;c&quot;&gt;; increment rdi&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;cmp&lt;/span&gt;     &lt;span class=&quot;n&quot;&gt;rsi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rdi&lt;/span&gt;           &lt;span class=&quot;c&quot;&gt;; compare rdi to size    &lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;jne&lt;/span&gt;     &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;L3&lt;/span&gt;                &lt;span class=&quot;c&quot;&gt;; keep going if rdi &amp;lt; size&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;L1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;ret&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This version is using a byte-by-byte copy loop, which I’ve annotated – it is more or less a 1:1 translation of how you’d imagine &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;std::fill&lt;/code&gt; is written. The result of 1 cycle per byte is exactly what we’d expect using &lt;a href=&quot;//blog/2019/06/11/speed-limits.html&quot;&gt;speed limit analysis&lt;/a&gt;: it is simultaneously limited by two different bottlenecks: 1 taken branch per cycle, and 1 store per cycle.&lt;/p&gt;

&lt;p&gt;The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;fill2&lt;/code&gt; version doesn’t have a loop at all:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;fill2:&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&quot;language-nasm highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;fill2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;char&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;unsigned&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;long&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;test&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;rsi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rsi&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;jne&lt;/span&gt;     &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;L8&lt;/span&gt;                &lt;span class=&quot;c&quot;&gt;; skip the memcpy call if size == 0&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;ret&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;L8&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;mov&lt;/span&gt;     &lt;span class=&quot;n&quot;&gt;rdx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rsi&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;xor&lt;/span&gt;     &lt;span class=&quot;n&quot;&gt;esi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;esi&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;jmp&lt;/span&gt;     &lt;span class=&quot;n&quot;&gt;memset&lt;/span&gt;             &lt;span class=&quot;c&quot;&gt;; tailcall to memset&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Rather, it simply defers immediately to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;memset&lt;/code&gt;. We aren’t going to dig into the assembly for &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;memset&lt;/code&gt; here, but the fastest possible &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;memset&lt;/code&gt; would run at 32 bytes/cycle, limited by 1 store/cycle and maximum vector the width of 32 bytes on my machine, so the measured value of 29 bytes/cycle indicates it’s using an implementation something along those lines.&lt;/p&gt;

&lt;p&gt;So that’s the &lt;em&gt;why&lt;/em&gt;, but what’s the &lt;em&gt;why of the why&lt;/em&gt; (second order why)?&lt;/p&gt;

&lt;p&gt;I thought this had something to do with the optimizer. After all, at &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-O3&lt;/code&gt; even the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;fill1&lt;/code&gt; version using the plain &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;0&lt;/code&gt; constant calls &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;memset&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;I was wrong, however. The answer actually lies in the implementation of the C++ standard library (there are various, gcc is using libstdc++ in this case). Let’s take a look at the implementation of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;std::fill&lt;/code&gt; (I’ve reformatted the code for clarity and removed some compile-time concept checks):&lt;/p&gt;

&lt;div class=&quot;language-c++ highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  &lt;span class=&quot;cm&quot;&gt;/*
   *  ...
   *
   *  This function fills a range with copies of the same value.  For char
   *  types filling contiguous areas of memory, this becomes an inline call
   *  to @c memset or @c wmemset.
  */&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;template&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;typename&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_ForwardIterator&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;typename&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_Tp&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;
  &lt;span class=&quot;kr&quot;&gt;inline&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;fill&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_ForwardIterator&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;__first&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_ForwardIterator&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;__last&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_Tp&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;amp;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;__value&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;__fill_a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;__niter_base&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;__first&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;__niter_base&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;__last&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;__value&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The included part of the comment&lt;sup id=&quot;fnref:wmem&quot;&gt;&lt;a href=&quot;#fn:wmem&quot; class=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt; already hints at what is to come: the implementor of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;std::fill&lt;/code&gt; has apparently considered specifically optimizing the call to a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;memset&lt;/code&gt; in some scenarios. So we keep following the trail, which brings us to the helper method &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;std::__fill_a&lt;/code&gt;. There are two overloads that are relevant here, the general method and an overload which handles the special case:&lt;/p&gt;

&lt;div class=&quot;language-c++ highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  &lt;span class=&quot;k&quot;&gt;template&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;typename&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_ForwardIterator&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;typename&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_Tp&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;
  &lt;span class=&quot;kr&quot;&gt;inline&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;typename&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;__gnu_cxx&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;__enable_if&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;!&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;__is_scalar&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_Tp&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;__value&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;__type&lt;/span&gt;
  &lt;span class=&quot;nf&quot;&gt;__fill_a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_ForwardIterator&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;__first&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_ForwardIterator&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;__last&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_Tp&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;amp;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;__value&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;__first&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;!=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;__last&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;++&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;__first&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
      &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;__first&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;__value&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
    
  &lt;span class=&quot;c1&quot;&gt;// Specialization: for char types we can use memset.&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;template&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;typename&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_Tp&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;
  &lt;span class=&quot;kr&quot;&gt;inline&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;typename&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;__gnu_cxx&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;__enable_if&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;__is_byte&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_Tp&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;__value&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;__type&lt;/span&gt;
  &lt;span class=&quot;nf&quot;&gt;__fill_a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_Tp&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;__first&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_Tp&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;__last&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_Tp&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;amp;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;__c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_Tp&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;__tmp&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;__c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;size_t&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;__len&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;__last&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;__first&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
      &lt;span class=&quot;n&quot;&gt;__builtin_memset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;__first&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;static_cast&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;unsigned&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;char&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;__tmp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;__len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Now we see how the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;memset&lt;/code&gt; appears. It is called explicitly by the second implementation shown above, selected by &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;enable_if&lt;/code&gt; when the SFINAE condition &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;__is_byte&amp;lt;_Tp&amp;gt;&lt;/code&gt; is true. Note, however, that unlike the general function, this variant has a single template argument: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;template&amp;lt;typename _Tp&amp;gt;&lt;/code&gt;, and the function signature is:&lt;/p&gt;

&lt;div class=&quot;language-c++ highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;__fill_a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_Tp&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;__first&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_Tp&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;__last&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_Tp&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;amp;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;__c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Hence, it will only be considered when the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;__first&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;__last&lt;/code&gt; pointers which delimit the range have the &lt;em&gt;exact same type as the value being filled&lt;/em&gt;. When when you write &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;std::fill(p, p + n, 0)&lt;/code&gt; where &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;p&lt;/code&gt; is &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;char *&lt;/code&gt;, you rely on template type deduction for the parameters, which ends up deducing &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;char *&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;int&lt;/code&gt; for the iterator type and value-to-fill type, &lt;em&gt;because &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;0&lt;/code&gt; is an integer constant&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;That is, it is if you had written:&lt;/p&gt;

&lt;div class=&quot;language-c++ highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fill&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;char&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This prevents the clever &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;memset&lt;/code&gt; optimization from taking place: the overload that does it is never called because the iterator value type is different than the value-to-fill type.&lt;/p&gt;

&lt;p&gt;This suggests a fix: we can simply force the template argument types rather than rely on type deduction:&lt;/p&gt;

&lt;div class=&quot;language-c++ highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;fill3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;char&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;size_t&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fill&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;char&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;char&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This way, we &lt;a href=&quot;https://godbolt.org/z/VTssh9&quot;&gt;get the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;memset&lt;/code&gt; version&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Finally, why does &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;fill2&lt;/code&gt; using &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;'\0'&lt;/code&gt; get the fast version, without forcing the template arguments? Well &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;'\0'&lt;/code&gt; is a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;char&lt;/code&gt; constant, so the value-to-assign type is &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;char&lt;/code&gt;. You could achieve the same effect with a cast, e.g., &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;static_cast&amp;lt;char&amp;gt;(0)&lt;/code&gt; – and for buffers which have types like &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;unsigned char&lt;/code&gt; this is necessary because &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;'\0'&lt;/code&gt; does not have the same type as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;unsigned char&lt;/code&gt; (at least &lt;a href=&quot;https://godbolt.org/z/YQKp7V&quot;&gt;on gcc&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;One might reasonably ask if this could be fixed in the standard library. I think so.&lt;/p&gt;

&lt;p&gt;One idea would be to keying off of &lt;em&gt;only&lt;/em&gt; the type of the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;first&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;last&lt;/code&gt; pointers, like this:&lt;/p&gt;

&lt;div class=&quot;language-c++ highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  &lt;span class=&quot;k&quot;&gt;template&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;typename&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_Tp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;typename&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_Tvalue&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;
  &lt;span class=&quot;kr&quot;&gt;inline&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;typename&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;__gnu_cxx&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;__enable_if&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;__is_byte&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_Tp&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;__value&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;__type&lt;/span&gt;
  &lt;span class=&quot;nf&quot;&gt;__fill_a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_Tp&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;__first&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_Tp&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;__last&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_Tvalue&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;amp;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;__c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_Tvalue&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;__tmp&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;__c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;size_t&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;__len&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;__last&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;__first&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
      &lt;span class=&quot;n&quot;&gt;__builtin_memset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;__first&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;static_cast&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;unsigned&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;char&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;__tmp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;__len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This says: who cares about the type of the value, it is going to get converted during assignment to the value type of the pointer anyways, so just look at the pointer type. E.g., if the type of the value-to-assign &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;_Tvalue&lt;/code&gt; is &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;int&lt;/code&gt;, but &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;_Tp&lt;/code&gt; is &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;char&lt;/code&gt; then this expands to this version, which is totally equivalent:&lt;/p&gt;

&lt;div class=&quot;language-c++ highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  &lt;span class=&quot;n&quot;&gt;__fill_a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;char&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;__first&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;char&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;__last&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;amp;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;__c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;__tmp&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;__c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;size_t&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;__len&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;__last&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;__first&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
      &lt;span class=&quot;n&quot;&gt;__builtin_memset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;__first&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;static_cast&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;unsigned&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;char&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;__tmp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;__len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This works … for simple types like &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;int&lt;/code&gt;. Where it fails is if the value to fill has a tricky, non-primitive type, like this:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;struct conv_counting_int {
    int v_;
    mutable size_t count_ = 0;

    operator char() const {
        count_++;
        return (char)v_;
    }
};

size_t fill5(char *p, size_t n) {
    conv_counting_int zero{0};
    std::fill(p, p + n, zero);
    return zero.count_;
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Here, the pointer type passed to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;std::fill&lt;/code&gt; is &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;char&lt;/code&gt;, but you cannot safely apply the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;memset&lt;/code&gt; optimization above, since the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;conv_counting_int&lt;/code&gt; counts the number of times it is converted to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;char&lt;/code&gt;, and this value will be wrong (in particular, it will be &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;1&lt;/code&gt;, not &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;n&lt;/code&gt;) if you perform the above optimization.&lt;/p&gt;

&lt;p&gt;This can be fixed. You could limit the optimization to the case where the pointer type is char-like &lt;em&gt;and&lt;/em&gt; the value-to-assign type is “simple” in the sense that it won’t notice how many times it has been converted. A sufficient check would be that the type is scalar, i.e. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;std::is_scalar&amp;lt;T&amp;gt;&lt;/code&gt; – although there is probably a less conservative check possible. So something like this for the SNIFAE check:&lt;/p&gt;

&lt;div class=&quot;language-c++ highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  &lt;span class=&quot;k&quot;&gt;template&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;typename&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_Tpointer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;typename&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_Tp&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;kr&quot;&gt;inline&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;typename&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;__gnu_cxx&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;__enable_if&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;__is_byte&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_Tpointer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;__value&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;__is_scalar&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_Tp&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;__value&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;__type&lt;/span&gt;
    &lt;span class=&quot;nf&quot;&gt;__fill_a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_Tpointer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;__first&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;_Tpointer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;__last&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_Tp&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;amp;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;__value&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
      &lt;span class=&quot;p&quot;&gt;...&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Here’s &lt;a href=&quot;https://godbolt.org/z/PXRWSB&quot;&gt;an example&lt;/a&gt; of how that would work. It’s not fully fleshed out but shows the idea.&lt;/p&gt;

&lt;p&gt;Finally, one might ask why &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;memset&lt;/code&gt; &lt;em&gt;is&lt;/em&gt; used when gcc is run at &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-O3&lt;/code&gt; or when clang is used (&lt;a href=&quot;https://godbolt.org/z/9nhWAh&quot;&gt;like this&lt;/a&gt;). The answer is the optimizer. Even if the compile-time semantics of the language select what appears to a byte-by-byte copy loop, the compiler itself can transform that into &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;memset&lt;/code&gt;, or something else like a vectorized loop, if it can prove it is &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;as-if&lt;/code&gt; equivalent. That recognition happens at &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-O3&lt;/code&gt; for &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;gcc&lt;/code&gt; but at &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-O2&lt;/code&gt; for clang.&lt;/p&gt;

&lt;h3 id=&quot;what-does-it-mean&quot;&gt;What Does It Mean&lt;/h3&gt;

&lt;p&gt;So what does it all mean? Is there a moral to this story?&lt;/p&gt;

&lt;p&gt;Some use this as evidence that the somehow C++ and/or the STL are irreparably broken. I don’t agree. Some other languages, even “fast” ones, will &lt;em&gt;never&lt;/em&gt; give you the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;memset&lt;/code&gt; speed, although many will - but many of those that do (e.g., &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;java.util.Arrays.fill()&lt;/code&gt;) do it via special recognition or handling of the function by the compiler or runtime. In the C++ standard library, the optimization the library writers have done is available to anyone, which is a big advantage. That the optimization fails, perhaps unexpectedly, in some cases is unfortunate but it’s nice that you can fix it yourself.&lt;/p&gt;

&lt;p&gt;Also, C++ gets &lt;em&gt;two&lt;/em&gt; shots at this one: many other languages rely on the compiler to optimize these patterns, and this also occurs in C++. It’s just a bit of a quirk of gcc that optimization doesn’t help here: it doesn’t vectorize at -O2, nor does it do &lt;em&gt;idiom recognition&lt;/em&gt;. Both of those result in much faster code: we’ve seen the effect of idiom recognition already: it results in a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;memset&lt;/code&gt;. Even if idiom recognition wasn’t enabled or didn’t work, vectorization would help a lot: here’s [gcc at -O3], but with idiom recognition disabled. It uses 32-byte stores (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;vmovdqu YMMWORD PTR [rax], ymm0&lt;/code&gt;) which will be close to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;memcpy&lt;/code&gt; speed. In many other languages it would only be up to the compiler: there wouldn’t be a chance to get &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;memset&lt;/code&gt; even with no optimization as there is in C++.&lt;/p&gt;

&lt;p&gt;Do we throw out modern C++ idioms, at least where performance matters, for example by replacing &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;std::fill&lt;/code&gt; with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;memset&lt;/code&gt;? I don’t think so. It is far from clear where &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;memset&lt;/code&gt; can even be used safely in C++. Unlike say &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;memcpy&lt;/code&gt; and &lt;em&gt;trivially copyable&lt;/em&gt;, there is no type trait for “memset is equivalent to zeroing”. It’s probably OK for byte-like types, and is widely used for other primitive types (which we can be sure are trivial, but can’t always be sure of the representation), but even that may not be safe. Once you introduced even simple structures or classes, the footguns multiply. I recommend &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;std::fill&lt;/code&gt; and more generally sticking to modern idioms, except in very rare cases where profiling has identified a hotspot, and even then you should take the safest approach that still provides the performance you need (e.g., by passing &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;(char)0&lt;/code&gt; in this case).&lt;/p&gt;

&lt;h3 id=&quot;source&quot;&gt;Source&lt;/h3&gt;

&lt;p&gt;The source for my benchmark is &lt;a href=&quot;https://github.com/travisdowns/fill-bench&quot;&gt;available on GitHub&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;thanks&quot;&gt;Thanks&lt;/h3&gt;

&lt;p&gt;Thanks to &lt;a href=&quot;https://twitter.com/mattgodbolt&quot;&gt;Matt Godbolt&lt;/a&gt; for creating &lt;a href=&quot;https://godbolt.org/&quot;&gt;Compiler Explorer&lt;/a&gt;, without which this type of investigation would be much more painful – to the point where it often wouldn’t happen at all.&lt;/p&gt;

&lt;p&gt;Matt Godbolt, tc, Nathan Kurz and Pocak for finding typos.&lt;/p&gt;

&lt;h3 id=&quot;discuss&quot;&gt;Discuss&lt;/h3&gt;

&lt;p&gt;I am &lt;em&gt;still&lt;/em&gt; working on my comments system (no, I don’t want Disqus), but in the meantime you can discuss this post on &lt;a href=&quot;https://news.ycombinator.com/item?id=22104576&quot;&gt;Hacker News&lt;/a&gt;, &lt;a href=&quot;https://www.reddit.com/r/cpp/comments/erialk/the_hunt_for_the_fastest_zero/&quot;&gt;Reddit&lt;/a&gt; or &lt;a href=&quot;https://lobste.rs/s/bylri4/hunt_for_fastest_zero&quot;&gt;lobste.rs&lt;/a&gt;.&lt;/p&gt;

&lt;p class=&quot;info&quot;&gt;If you liked this post, check out the &lt;a href=&quot;/&quot;&gt;homepage&lt;/a&gt; for others you might enjoy&lt;sup id=&quot;fnref:noenjoy&quot;&gt;&lt;a href=&quot;#fn:noenjoy&quot; class=&quot;footnote&quot;&gt;4&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;hr /&gt;
&lt;hr /&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;div class=&quot;footnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:function&quot;&gt;
      &lt;p&gt;Of course, you wouldn’t wrap the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;std::fill&lt;/code&gt; function in another &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;fill&lt;/code&gt; function that just forwards directly to the standard function: you’d just call &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;std::fill&lt;/code&gt; directly. We use a function here so you can see the parameter types and we can examine the disassembly easily. &lt;a href=&quot;#fnref:function&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:qb&quot;&gt;
      &lt;p&gt;On &lt;a href=&quot;http://quick-bench.com/yGy2Mzlr2ZZhWVxoH7HscmbEC94&quot;&gt;quickbench&lt;/a&gt;, the difference varies slightly from run to run but is usually around 31 to 32 times. &lt;a href=&quot;#fnref:qb&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:wmem&quot;&gt;
      &lt;p&gt;Interestingly, the comment mentions &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;wmemset&lt;/code&gt; in addition to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;memset&lt;/code&gt; which would presumably be applied for values of type &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;wchar_t&lt;/code&gt; (32-bits on this platform), but I don’t find any evidence that is actually the case via experiment or by examining the code – the optimization appears to only be currently implemented for byte-like values and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;memset&lt;/code&gt;. &lt;a href=&quot;#fnref:wmem&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:noenjoy&quot;&gt;
      &lt;p&gt;On the other hand, if you didn’t enjoy it, I have to wonder how you made it this far! Maybe you started off liking it, then felt invested and compelled to finish it? Beware the sunk cost fallacy! &lt;a href=&quot;#fnref:noenjoy&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><author><name>Travis Downs</name><email>travis.downs@gmail.com</email></author><category term="blog" /><category term="performance" /><category term="c++" /><category term="gcc" /><category term="compiler-optimization" /><summary type="html">Let’s say I ask you to fill a char array of size n with zeros. I don’t know why, exactly, but please play along for now.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/assets/zero/twitter-card.png" /><media:content medium="image" url="http://localhost:4000/assets/zero/twitter-card.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Gathering Intel on Intel AVX-512 Transitions</title><link href="http://localhost:4000/blog/2020/01/17/avxfreq1.html" rel="alternate" type="text/html" title="Gathering Intel on Intel AVX-512 Transitions" /><published>2020-01-17T00:00:00+02:00</published><updated>2020-01-17T00:00:00+02:00</updated><id>http://localhost:4000/blog/2020/01/17/avxfreq1</id><content type="html" xml:base="http://localhost:4000/blog/2020/01/17/avxfreq1.html">&lt;p class=&quot;info&quot;&gt;&lt;strong&gt;Note:&lt;/strong&gt; For the really short version, you can &lt;a href=&quot;#summary&quot;&gt;skip to the summary&lt;/a&gt;, but then what will do you for the rest of the day?&lt;/p&gt;

&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;This is a post about AVX and AVX-512 related frequency scaling&lt;sup id=&quot;fnref:first&quot;&gt;&lt;a href=&quot;#fn:first&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;p&gt;Now, something more than nothing has been written about this already, including &lt;a href=&quot;https://blog.cloudflare.com/on-the-dangers-of-intels-frequency-scaling/&quot;&gt;cautionary tales&lt;/a&gt; of performance loss and some &lt;a href=&quot;https://lemire.me/blog/2018/09/07/avx-512-when-and-how-to-use-these-new-instructions/&quot;&gt;broad guidelines&lt;/a&gt;&lt;sup id=&quot;fnref:dmore&quot;&gt;&lt;a href=&quot;#fn:dmore&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;, so do we really need to add to the pile?&lt;/p&gt;

&lt;p&gt;Perhaps not, but I’m doing it anyway. My angle is a lower level look, almost microscopic really, at the specific transition behaviors. One would hope that this will lead to specific, &lt;em&gt;quantitative&lt;/em&gt; advice about exactly when various instruction types are likely to pay off, but (spoiler) I didn’t make it there in this post.&lt;/p&gt;

&lt;p&gt;Now I wasn’t really planning on writing about this just now, but I got off on a (nested) tangent&lt;sup id=&quot;fnref:intro&quot;&gt;&lt;a href=&quot;#fn:intro&quot; class=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt;, so let’s examine the AVX-512 downclocking behavior using target tests. At a minimum, this is necessary background for the next post, but I hope that it is also standalone interesting.&lt;/p&gt;

&lt;p class=&quot;info&quot;&gt;&lt;strong&gt;Note:&lt;/strong&gt; If you are here because of your footnote fetish, skip straight to the &lt;a href=&quot;#footnotes&quot;&gt;good 🦶 stuff&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;table-of-contents&quot;&gt;Table of Contents&lt;/h3&gt;

&lt;p&gt;You could perhaps trying skipping ahead to a section that interests you using this obligatory table of contents, but sections are not self contained, so you’ll be better off reading the whole thing linearly.&lt;/p&gt;

&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#introduction&quot; id=&quot;markdown-toc-introduction&quot;&gt;Introduction&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#table-of-contents&quot; id=&quot;markdown-toc-table-of-contents&quot;&gt;Table of Contents&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#the-source&quot; id=&quot;markdown-toc-the-source&quot;&gt;The Source&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#test-structure&quot; id=&quot;markdown-toc-test-structure&quot;&gt;Test Structure&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#hardware&quot; id=&quot;markdown-toc-hardware&quot;&gt;Hardware&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#tests&quot; id=&quot;markdown-toc-tests&quot;&gt;Tests&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#256-bit-integer-simd-avx&quot; id=&quot;markdown-toc-256-bit-integer-simd-avx&quot;&gt;256-bit Integer &lt;abbr title=&quot;Single Instruction Multiple Data: an ISA type or ISA extension like Intel's AVX or ARM's NEON that can perform multiple identical operations on elements packed into a SIMD register.&quot;&gt;SIMD&lt;/abbr&gt; (AVX)&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#512-bit-integer-simd-avx-512&quot; id=&quot;markdown-toc-512-bit-integer-simd-avx-512&quot;&gt;512-bit Integer &lt;abbr title=&quot;Single Instruction Multiple Data: an ISA type or ISA extension like Intel's AVX or ARM's NEON that can perform multiple identical operations on elements packed into a SIMD register.&quot;&gt;SIMD&lt;/abbr&gt; (AVX-512)&lt;/a&gt;        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#enter-ipc&quot; id=&quot;markdown-toc-enter-ipc&quot;&gt;Enter &lt;abbr title=&quot;Instructions per cycle: calculated over an interval by measuring the number of instructions executed and the duration in cycles.&quot;&gt;IPC&lt;/abbr&gt;&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#voltage-only-transitions&quot; id=&quot;markdown-toc-voltage-only-transitions&quot;&gt;Voltage Only Transitions&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#attenuation&quot; id=&quot;markdown-toc-attenuation&quot;&gt;Attenuation&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#what-was-left-out&quot; id=&quot;markdown-toc-what-was-left-out&quot;&gt;What Was Left Out&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#summary&quot; id=&quot;markdown-toc-summary&quot;&gt;Summary&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#thanks&quot; id=&quot;markdown-toc-thanks&quot;&gt;Thanks&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#discuss&quot; id=&quot;markdown-toc-discuss&quot;&gt;Discuss&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;the-source&quot;&gt;The Source&lt;/h2&gt;

&lt;p&gt;All of the code underlying this post is available in the &lt;a href=&quot;https://github.com/travisdowns/freq-bench/tree/post1&quot;&gt;post1 branch of freq-bench&lt;/a&gt;, so you can follow along at home, check my work, and check out the behavior on your own hardware. It requires Linux and the &lt;a href=&quot;https://github.com/travisdowns/freq-bench/blob/post1/README.md&quot;&gt;README&lt;/a&gt; gives basic clues on getting started.&lt;/p&gt;

&lt;p&gt;The source includes the &lt;a href=&quot;https://github.com/travisdowns/freq-bench/blob/post1/scripts/data.sh&quot;&gt;data generation scripts&lt;/a&gt; as well as those to &lt;a href=&quot;https://github.com/travisdowns/freq-bench/blob/post1/scripts/plots.sh&quot;&gt;generate the plots&lt;/a&gt;. Neither shell scripting nor Python are my forte, so be gentle.&lt;/p&gt;

&lt;h2 id=&quot;test-structure&quot;&gt;Test Structure&lt;/h2&gt;

&lt;p&gt;We want to investigate what happens when instruction stream related performance transitions occur. The most famous example is what happens when you execute an AVX-512 instruction&lt;sup id=&quot;fnref:widthmatters&quot;&gt;&lt;a href=&quot;#fn:widthmatters&quot; class=&quot;footnote&quot;&gt;4&lt;/a&gt;&lt;/sup&gt; for the first time in a while, but as we will see there are other cases.&lt;/p&gt;

&lt;p&gt;The basic idea is that the test has a &lt;em&gt;duty period&lt;/em&gt; and every time this period elapses, we run a test-specific payload for the duration of the &lt;em&gt;payload period&lt;/em&gt; which consists of one or more “interesting” instructions (which depend on the test). During the entire test we sample various metrics at a best-effort fixed frequency. This repeats for the entire test period. The sample period will generally be much smaller than the duty period&lt;sup id=&quot;fnref:speriod&quot;&gt;&lt;a href=&quot;#fn:speriod&quot; class=&quot;footnote&quot;&gt;5&lt;/a&gt;&lt;/sup&gt;: in our tests we use a 5,000 μs duty period and a sample period of 1 μs, mostly.&lt;/p&gt;

&lt;p&gt;Visually, it is something like this (showing a single duty period: one benchmark is composed of multiple duty cycles back to back):&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/avxfreq1/test-structure.png&quot; alt=&quot;Test Structure&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This diagram shows the payload period as occupying a non-negligible amount of time. However, in the first few first tests, the payload period is essentially zero: we run the payload function (which consists of only a couple instructions) only once, so it is really a payload &lt;em&gt;moment&lt;/em&gt; rather than period.&lt;/p&gt;

&lt;h3 id=&quot;hardware&quot;&gt;Hardware&lt;/h3&gt;

&lt;p&gt;We are running these tests on a &lt;abbr title=&quot;Intel's Skylake (server) architecture including Skylake-SP, Skylake-X and Skylake-W&quot;&gt;SKX&lt;/abbr&gt; architecture W-series CPU: a W-2104&lt;sup id=&quot;fnref:f2104&quot;&gt;&lt;a href=&quot;#fn:f2104&quot; class=&quot;footnote&quot;&gt;6&lt;/a&gt;&lt;/sup&gt; with the following &lt;a href=&quot;https://stackoverflow.com/a/56861355/149138&quot;&gt;license-based&lt;/a&gt; frequencies&lt;sup id=&quot;fnref:avxt&quot;&gt;&lt;a href=&quot;#fn:avxt&quot; class=&quot;footnote&quot;&gt;7&lt;/a&gt;&lt;/sup&gt;:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Name&lt;/th&gt;
      &lt;th&gt;License&lt;/th&gt;
      &lt;th&gt;Frequency&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Non-AVX Turbo&lt;/td&gt;
      &lt;td&gt;L0&lt;/td&gt;
      &lt;td&gt;3.2 GHz&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;AVX Turbo&lt;/td&gt;
      &lt;td&gt;L1&lt;/td&gt;
      &lt;td&gt;2.8 GHz&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;AVX-512 Turbo&lt;/td&gt;
      &lt;td&gt;L2&lt;/td&gt;
      &lt;td&gt;2.4 GHz&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;For one (voltage) test I also use my Skylake (mobile) &lt;a href=&quot;https://ark.intel.com/content/www/us/en/ark/products/88967/intel-core-i7-6700hq-processor-6m-cache-up-to-3-50-ghz.html&quot;&gt;i7-6700HQ&lt;/a&gt;, running at either it’s nominal frequency of 2.6 GHz, or the turbo frequency of 3.5 GHz.&lt;/p&gt;

&lt;h2 id=&quot;tests&quot;&gt;Tests&lt;/h2&gt;

&lt;p&gt;The basic approach this post will take is examining the CPU behavior using the test framework above, primarily varying what the payload is, and what metrics we look at. Let’s get the ball rolling with 256-bit instructions.&lt;/p&gt;

&lt;h3 id=&quot;256-bit-integer-simd-avx&quot;&gt;256-bit Integer &lt;abbr title=&quot;Single Instruction Multiple Data: an ISA type or ISA extension like Intel's AVX or ARM's NEON that can perform multiple identical operations on elements packed into a SIMD register.&quot;&gt;SIMD&lt;/abbr&gt; (AVX)&lt;/h3&gt;

&lt;p&gt;For the first test will use as payload the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;vporymm_vz&lt;/code&gt; function, which is just a single 256-bit &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;vpor&lt;/code&gt; instruction, followed by a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;vzeroupper&lt;/code&gt;&lt;sup id=&quot;fnref:vz&quot;&gt;&lt;a href=&quot;#fn:vz&quot; class=&quot;footnote&quot;&gt;8&lt;/a&gt;&lt;/sup&gt;:&lt;/p&gt;

&lt;div class=&quot;language-nasm highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;vporymm_vz&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;vpor&lt;/span&gt;   &lt;span class=&quot;n&quot;&gt;ymm0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ymm0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ymm0&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;vzeroupper&lt;/span&gt; 
  &lt;span class=&quot;k&quot;&gt;ret&lt;/span&gt;    
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We call the payload function only once at the start of each duty period&lt;sup id=&quot;fnref:pperiod&quot;&gt;&lt;a href=&quot;#fn:pperiod&quot; class=&quot;footnote&quot;&gt;9&lt;/a&gt;&lt;/sup&gt;. The duty period is set to 5000 μs and the sample period to 1 μs, and the total test time is set to 31,000 μs (so the payload will execute 7 times).&lt;/p&gt;

&lt;p&gt;Here’s the result (plot notes&lt;sup id=&quot;fnref:plotnotes&quot;&gt;&lt;a href=&quot;#fn:plotnotes&quot; class=&quot;footnote&quot;&gt;10&lt;/a&gt;&lt;/sup&gt;), with time along the x axis&lt;sup id=&quot;fnref:falk&quot;&gt;&lt;a href=&quot;#fn:falk&quot; class=&quot;footnote&quot;&gt;11&lt;/a&gt;&lt;/sup&gt;, showing the measured frequency at each sample (there are three separate test runs shown&lt;sup id=&quot;fnref:threerun&quot;&gt;&lt;a href=&quot;#fn:threerun&quot; class=&quot;footnote&quot;&gt;12&lt;/a&gt;&lt;/sup&gt;):&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/avxfreq1/fig-vporvz256.svg&quot; alt=&quot;256-bit vpor transitions&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Well that’s really boring. The entire test runs consistently at 3.2 GHz, the nominal (L0 license) frequency, if we ignore the a few uninteresting outliers&lt;sup id=&quot;fnref:outlie&quot;&gt;&lt;a href=&quot;#fn:outlie&quot; class=&quot;footnote&quot;&gt;13&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;h3 id=&quot;512-bit-integer-simd-avx-512&quot;&gt;512-bit Integer &lt;abbr title=&quot;Single Instruction Multiple Data: an ISA type or ISA extension like Intel's AVX or ARM's NEON that can perform multiple identical operations on elements packed into a SIMD register.&quot;&gt;SIMD&lt;/abbr&gt; (AVX-512)&lt;/h3&gt;

&lt;p&gt;Before the crowd gets too rowdy, let’s quickly move on to the next test, which is identical except that it uses 512-bit &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;zmm&lt;/code&gt; registers:&lt;/p&gt;

&lt;div class=&quot;language-nasm highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;vporzmm_vz&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;vpor&lt;/span&gt;   &lt;span class=&quot;n&quot;&gt;zmm0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zmm0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zmm0&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;vzeroupper&lt;/span&gt; 
  &lt;span class=&quot;k&quot;&gt;ret&lt;/span&gt;    
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Here is the result:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/avxfreq1/fig-vporvz512.svg&quot; alt=&quot;512-bit vpor transitions&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We’ve got something to sink our teeth into!&lt;/p&gt;

&lt;p&gt;Remember that the duty cycle is 5000 μs, so at each x-axis tick we execute the payload. Now the behavior is clear: every time the payload instruction executes (at multiples of 5000 μs), the frequency drops from the 3.2 GHz L0 license down to 2.8 GHz L1 license frequency. So far this is all pretty much as expected.&lt;/p&gt;

&lt;p&gt;Let’s zoom in on one of the transition points at 15,000 μs:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/avxfreq1/fig-vpor-zoomed.svg&quot; alt=&quot;512-bit vpor transitions (zoomed)&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We can make the following observations:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;There is a transition period (the rightmost of the two shaded regions, in orange&lt;sup id=&quot;fnref:peachpuff&quot;&gt;&lt;a href=&quot;#fn:peachpuff&quot; class=&quot;footnote&quot;&gt;14&lt;/a&gt;&lt;/sup&gt;) of ~11 μs&lt;sup id=&quot;fnref:resnote&quot;&gt;&lt;a href=&quot;#fn:resnote&quot; class=&quot;footnote&quot;&gt;15&lt;/a&gt;&lt;/sup&gt; where the CPU is halted: no samples occur during this period&lt;sup id=&quot;fnref:halted&quot;&gt;&lt;a href=&quot;#fn:halted&quot; class=&quot;footnote&quot;&gt;16&lt;/a&gt;&lt;/sup&gt;. For fun, I’ll call this a &lt;em&gt;frequency transition&lt;/em&gt;.&lt;/li&gt;
  &lt;li&gt;The leftmost shaded region, shown in purple&lt;sup id=&quot;fnref:thistle&quot;&gt;&lt;a href=&quot;#fn:thistle&quot; class=&quot;footnote&quot;&gt;17&lt;/a&gt;&lt;/sup&gt;, immediately following the payload execution at 15,000 μs and prior to the halted region, is ~9 μs long and the frequency remains unchanged. This is not just a test issue or measurement error: this period occurs after the payload and is consistently reproducible&lt;sup id=&quot;fnref:confirm&quot;&gt;&lt;a href=&quot;#fn:confirm&quot; class=&quot;footnote&quot;&gt;18&lt;/a&gt;&lt;/sup&gt;. Although it looks like nothing interesting is going on in this region, we’ll soon see it is indeed special and will call this region a &lt;em&gt;voltage-only&lt;/em&gt; transition.&lt;/li&gt;
  &lt;li&gt;Although not fully shown in the zoomed plot, the lower 2.8 GHz frequency period lasts for ~650 μs.&lt;/li&gt;
  &lt;li&gt;Not shown in the zoomed plot (but seen as a second downwards spike on the full plot, after the ~650 μs period of low frequency), there is another fully halted period of ~11 us, after which the CPU returns to it’s maximum speed of 3.2 GHz (L0 license).&lt;/li&gt;
  &lt;li&gt;These attributes are mostly consistent across the three runs (so much that the series, in green, mostly overlaps and obscures the others) – but there are a few outliers in where the return to 3.2 GHz takes somewhat longer. This is consistent across runs: recovery is never &lt;em&gt;faster&lt;/em&gt; than ~650 μs, but sometimes longer. I believe it occurs when an interrupt during the L1 region “resets the timer”.&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;enter-ipc&quot;&gt;Enter &lt;abbr title=&quot;Instructions per cycle: calculated over an interval by measuring the number of instructions executed and the duration in cycles.&quot;&gt;IPC&lt;/abbr&gt;&lt;/h4&gt;

&lt;p&gt;Although it is not visible in this plot, there is something special about the behavior of 512-bit instructions in the first shaded (purple) region – that is, in the 9 microseconds between the execution of the payload instruction and before the subsequent halted period: &lt;em&gt;they execute much slower than usual&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;This is easiest to see if we extend the payload period: instead executing the payload function once every 5000 μs, then looping on &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;rdtsc&lt;/code&gt;, waiting for the next sample, we will continue to execute the payload function for 100 μs after a new duty period starts (that is, the &lt;em&gt;payload period&lt;/em&gt; is set to 100 μs). During this time we still take samples as usual, every 1 μs – but in between samples we are executing the payload instruction(s)&lt;sup id=&quot;fnref:whynot&quot;&gt;&lt;a href=&quot;#fn:whynot&quot; class=&quot;footnote&quot;&gt;19&lt;/a&gt;&lt;/sup&gt;. So one duty period now looks like 100 μs of payload followed by 4850 μs of normal payload-free hot spinning.&lt;/p&gt;

&lt;p&gt;We lengthen the payload period in order to examine the performance of the payload instructions. There are several metrics we could look at, but a simple one is to look at &lt;em&gt;instructions per second&lt;/em&gt;. As long as we make sure the large majority of the executed instructions are payload instructions, the &lt;abbr title=&quot;Instructions per cycle: calculated over an interval by measuring the number of instructions executed and the duration in cycles.&quot;&gt;IPC&lt;/abbr&gt; will largely reflect the execution of the payload&lt;sup id=&quot;fnref:ideal&quot;&gt;&lt;a href=&quot;#fn:ideal&quot; class=&quot;footnote&quot;&gt;20&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;p&gt;As payload, we will use a function composed simply of 1,000 dependent 512-bit &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;vpord&lt;/code&gt; instructions:&lt;/p&gt;

&lt;div class=&quot;language-nasm highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;vpord&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;zmm0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zmm0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zmm0&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;vpord&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;zmm0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zmm0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zmm0&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;; ... 997 more like this&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;vpord&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;zmm0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zmm0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zmm0&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;vzeroupper&lt;/span&gt; 
&lt;span class=&quot;k&quot;&gt;ret&lt;/span&gt;    
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We &lt;a href=&quot;https://uops.info/table.html?search=vpord%20(zmm%2C%20zmm%2C%20zmm)&amp;amp;cb_lat=on&amp;amp;cb_tp=on&amp;amp;cb_uops=on&amp;amp;cb_ports=on&amp;amp;cb_SKX=on&amp;amp;cb_measurements=on&amp;amp;cb_iaca30=on&amp;amp;cb_avx512=on&quot;&gt;know&lt;/a&gt; these &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;vpord&lt;/code&gt; instructions have a latency of 1 cycle and here they are serially dependent so we expect this function to take 1,000 cycles, give or take&lt;sup id=&quot;fnref:give&quot;&gt;&lt;a href=&quot;#fn:give&quot; class=&quot;footnote&quot;&gt;21&lt;/a&gt;&lt;/sup&gt;, for an &lt;abbr title=&quot;Instructions per cycle: calculated over an interval by measuring the number of instructions executed and the duration in cycles.&quot;&gt;IPC&lt;/abbr&gt; of 1.0.&lt;/p&gt;

&lt;p&gt;Here’s what a the same zoomed transition point for this looks like, with &lt;abbr title=&quot;Instructions per cycle: calculated over an interval by measuring the number of instructions executed and the duration in cycles.&quot;&gt;IPC&lt;/abbr&gt; plotted on the secondary axis:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/avxfreq1/fig-ipc-zoomed-zmm.svg&quot; alt=&quot;512-bit vpor transitions (with IPC)&quot; /&gt;&lt;/p&gt;

&lt;p&gt;First, note that in the unshaded regions on the left (before 15,000 μs) and right (after 15,100 μs), the &lt;abbr title=&quot;Instructions per cycle: calculated over an interval by measuring the number of instructions executed and the duration in cycles.&quot;&gt;IPC&lt;/abbr&gt; is basically irrelevant: no payload instructions are being executed, so the &lt;abbr title=&quot;Instructions per cycle: calculated over an interval by measuring the number of instructions executed and the duration in cycles.&quot;&gt;IPC&lt;/abbr&gt; there is just the whatever the measurement code happens to net out to. We only care about the &lt;abbr title=&quot;Instructions per cycle: calculated over an interval by measuring the number of instructions executed and the duration in cycles.&quot;&gt;IPC&lt;/abbr&gt; in the shaded regions, where the payload is executing.&lt;/p&gt;

&lt;p&gt;Let’s tackle the regions from right to left, which happens to correspond to obvious to less obvious.&lt;/p&gt;

&lt;p&gt;We have the blue region, running from ~15020 μs to 15100 μs (where the extra payload period ends). Here the &lt;abbr title=&quot;Instructions per cycle: calculated over an interval by measuring the number of instructions executed and the duration in cycles.&quot;&gt;IPC&lt;/abbr&gt; is right at 1 instruction per cycle. So the payload is executing right at the expected rate, i.e., &lt;em&gt;full speed&lt;/em&gt;. Keeners may point out that the very beginning of the blue period, the &lt;abbr title=&quot;Instructions per cycle: calculated over an interval by measuring the number of instructions executed and the duration in cycles.&quot;&gt;IPC&lt;/abbr&gt; (and the measured frequency) is a bit noisier and slightly above 1. This is not a CPU effect, but rather a measurement one: during this phase the benchmark is &lt;em&gt;catching up&lt;/em&gt; on samples missed during the previous halted period, which changes the payload to overhead ratio and bumps up the &lt;abbr title=&quot;Instructions per cycle: calculated over an interval by measuring the number of instructions executed and the duration in cycles.&quot;&gt;IPC&lt;/abbr&gt; (details&lt;sup id=&quot;fnref:catchup&quot;&gt;&lt;a href=&quot;#fn:catchup&quot; class=&quot;footnote&quot;&gt;22&lt;/a&gt;&lt;/sup&gt;).&lt;/p&gt;

&lt;p&gt;The middle, orange, region shows us what we’ve already seen: the CPU is halted, so no samples occur. &lt;abbr title=&quot;Instructions per cycle: calculated over an interval by measuring the number of instructions executed and the duration in cycles.&quot;&gt;IPC&lt;/abbr&gt; doesn’t tell us much here.&lt;/p&gt;

&lt;h4 id=&quot;voltage-only-transitions&quot;&gt;Voltage Only Transitions&lt;/h4&gt;

&lt;p&gt;The most interesting part is the first shaded region (purple): after the payload starts running but before the halt which I call a &lt;em&gt;voltage only&lt;/em&gt; transition for reasons that will soon become clear.&lt;/p&gt;

&lt;p&gt;Here, we see that the payload executes &lt;em&gt;much&lt;/em&gt; more slowly, with an &lt;abbr title=&quot;Instructions per cycle: calculated over an interval by measuring the number of instructions executed and the duration in cycles.&quot;&gt;IPC&lt;/abbr&gt; of ~0.25. So in this region, the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;vpord&lt;/code&gt; instructions are apparently executing at &lt;em&gt;four times&lt;/em&gt; their normal latency. I also observe an identical 4x slowdown for &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;vpord&lt;/code&gt; throughput, using an identical &lt;a href=&quot;https://github.com/travisdowns/freq-bench/blob/434c7cf5db73e2d48061e78525c7bbf7eb7757a3/basic-impls.cpp#L64&quot;&gt;test&lt;/a&gt; except with independent &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;vpord&lt;/code&gt; instructions&lt;sup id=&quot;fnref:tput&quot;&gt;&lt;a href=&quot;#fn:tput&quot; class=&quot;footnote&quot;&gt;23&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;p&gt;Perhaps surprisingly, this same slowdown occurs for 256-bit &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ymm&lt;/code&gt; instructions as well. This contradicts the conventional wisdom that on AVX-512 chips there is no penalty to using light 256-bit instructions:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/avxfreq1/fig-ipc-zoomed-ymm.svg&quot; alt=&quot;256-bit vpor transitions (with IPC)&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The results shown above are for a test identical to the 512-version except that it uses 256-bit &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;vpor ymm0, ymm0, ymm0&lt;/code&gt; as the payload. It shows the same slowdown for ~9 μs after the payload starts executing, but no subsequent halt and no frequency transition. That is, it shows a voltage-only transition (lack of frequency transition is expected because we don’t expect a turbo license change for light 256-bit instructions).&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;xmmeffect&quot;&gt;&lt;/a&gt;By now, you are probably wondering about 128-bit &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;xmm&lt;/code&gt; registers. The good news is that these show no effect at all:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/avxfreq1/fig-ipc-zoomed-xmm.svg&quot; alt=&quot;128-bit vpor transitions (with IPC)&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Here, the &lt;abbr title=&quot;Instructions per cycle: calculated over an interval by measuring the number of instructions executed and the duration in cycles.&quot;&gt;IPC&lt;/abbr&gt; jumps immediately to the expected value. So it appears that the CPU runs in a state where the 128-bit lanes are ready to go at all times&lt;sup id=&quot;fnref:orisit&quot;&gt;&lt;a href=&quot;#fn:orisit&quot; class=&quot;footnote&quot;&gt;24&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;p&gt;The conventional wisdom regarding this “warmup” period is that the upper part&lt;sup id=&quot;fnref:upper&quot;&gt;&lt;a href=&quot;#fn:upper&quot; class=&quot;footnote&quot;&gt;25&lt;/a&gt;&lt;/sup&gt; of the vector units is shut down when not in use, and takes time to power up. The story goes that during this power-up period the CPU does not need to halt but it runs &lt;abbr title=&quot;Single Instruction Multiple Data: an ISA type or ISA extension like Intel's AVX or ARM's NEON that can perform multiple identical operations on elements packed into a SIMD register.&quot;&gt;SIMD&lt;/abbr&gt; instructions at a reduced throughput &lt;em&gt;by splitting up the input&lt;/em&gt; into 128-bit chunks and passing the data two or more times through the powered-on 128-bit lanes&lt;sup id=&quot;fnref:amd&quot;&gt;&lt;a href=&quot;#fn:amd&quot; class=&quot;footnote&quot;&gt;26&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;p&gt;However, there are some observations that seem to contradict this hypothesis (in rough order from least to most convincing):&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;The observed impact to latency and throughput is ~4x, whereas I would expect 2x for simple instructions such as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;vpor&lt;/code&gt;.&lt;/li&gt;
  &lt;li&gt;The timing is the same for 256-bit and 512-bit instructions: despite that 512-bit instructions take at least 2x the work, i.e., need to be passed through the 128-bit unit at least 4 times.&lt;/li&gt;
  &lt;li&gt;Some instructions are more difficult to implement using this type of splitting, e.g., instructions where both high and low output lanes depend on all of the input lanes&lt;sup id=&quot;fnref:ewise&quot;&gt;&lt;a href=&quot;#fn:ewise&quot; class=&quot;footnote&quot;&gt;27&lt;/a&gt;&lt;/sup&gt; (see how slow they are on Zen). I expected that maybe these instructions would be slower when running in split mode, but I tested &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;vpermd&lt;/code&gt; and found that it runs at 4L4T&lt;sup id=&quot;fnref:lt&quot;&gt;&lt;a href=&quot;#fn:lt&quot; class=&quot;footnote&quot;&gt;28&lt;/a&gt;&lt;/sup&gt;, compared to 3L1T normally. So &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;vpermd&lt;/code&gt; (including the 512-bit version) didn’t slow more than &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;vpor&lt;/code&gt;, and in fact in a relative sense it slowed down &lt;em&gt;less&lt;/em&gt; (e.g., the latency only changed from 3 to 4). The fact that the latency and throughput reacted differently for this instruction seems odd, and that it has now the exact same 4L4T timing as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;vpor&lt;/code&gt; seems like a strange coincidence.&lt;/li&gt;
  &lt;li&gt;Oddly, when I tried to time the slowdown more precisely, I kept coming with fractional value around 4.2x, not 4.0x, kind of contradicting the idea that the instruction is simply operating in a different mode, which should still have an integral latency.&lt;/li&gt;
  &lt;li&gt;As it turns out, &lt;em&gt;all ALU&lt;sup id=&quot;fnref:alu&quot;&gt;&lt;a href=&quot;#fn:alu&quot; class=&quot;footnote&quot;&gt;29&lt;/a&gt;&lt;/sup&gt; instructions&lt;/em&gt; are slower in this mode, not just wide &lt;abbr title=&quot;Single Instruction Multiple Data: an ISA type or ISA extension like Intel's AVX or ARM's NEON that can perform multiple identical operations on elements packed into a SIMD register.&quot;&gt;SIMD&lt;/abbr&gt; ones.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;It was 5 that sealed the deal on this not being a slowdown related to split execution. I believe what is actually happening is the CPU is doing very fine-grained throttling when wider instructions are executing in the core. That is, the upper lanes &lt;em&gt;are&lt;/em&gt; being used in this mode (they are either not gated at at all, or are gated but enabling them is very quick, less than 1 μs) but execution frequency is reduced by 4x because CPU power delivery is not a state that can handle full-speed execution of these wider instructions, yet. While the CPU waits (e.g., for voltage to rise, fattening the guardband) for higher power execution to be allowed, this fine-grained throttling occurs.&lt;/p&gt;

&lt;p&gt;This throttling affects non-&lt;abbr title=&quot;Single Instruction Multiple Data: an ISA type or ISA extension like Intel's AVX or ARM's NEON that can perform multiple identical operations on elements packed into a SIMD register.&quot;&gt;SIMD&lt;/abbr&gt; instructions too, causing them to execute at 4x their normal latency and inverse throughput. We can show with the following test, which combines a single &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;vpor ymm0, ymm0, ymm0&lt;/code&gt; with N chained &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;add eax, 0&lt;/code&gt; instructions, shown here for N = 3:&lt;/p&gt;

&lt;div class=&quot;language-nasm highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;vpor&lt;/span&gt;   &lt;span class=&quot;n&quot;&gt;ymm0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ymm0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ymm0&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;add&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;eax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mh&quot;&gt;0x0&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;add&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;eax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mh&quot;&gt;0x0&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;add&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;eax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mh&quot;&gt;0x0&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;; repeated 9 more times&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;If only &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;vpor&lt;/code&gt; is slowed down, each block of 4 instructions will take 4 cycles, limited by the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;vpor&lt;/code&gt; chain (the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;add&lt;/code&gt; chain is 3 cycles long). However, I actually measure ~12 cycles, indicating that we are instead limited by the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;add&lt;/code&gt; chain, each of which takes 4 cycles for a total of 12.&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;throttle-anchor&quot;&gt;&lt;/a&gt;We can vary the number of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;add&lt;/code&gt; instructions (N) to see how long this effect persists. This table is the result:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;ADD instructions (N)&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;Cycles/ADD&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;Delta Cycles (slow)&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;Delta Cycles (fast)&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;2&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;4.1&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;2.3&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;-0.2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;3&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;4.1&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;4.0&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.8&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;4&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;4.1&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;4.1&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;1.1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;5&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;4.0&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;3.9&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;1.1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;6&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;4.1&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;4.3&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.7&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;7&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;4.0&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;3.4&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;1.1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;8&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;4.0&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;4.2&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.9&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;9&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;4.1&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;3.9&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.8&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;10&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;4.1&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;4.3&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;1.1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;20&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;4.0&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;4.0&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;1.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;30&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;4.0&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;4.0&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;1.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;40&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;4.1&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;4.3&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;1.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;50&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;4.1&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;3.9&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;1.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;60&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;4.1&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;4.4&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;1.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;70&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;4.2&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;4.5&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;1.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;80&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;4.1&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;3.4&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;1.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;90&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;3.6&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;-0.2&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;1.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;100&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;3.3&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;1.1&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;1.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;120&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;2.9&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.9&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;1.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;140&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;2.7&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;1.1&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;1.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;160&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;2.5&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;1.2&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;1.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;180&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;2.3&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.7&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.9&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;200&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;2.2&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.8&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;1.0&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;The &lt;strong&gt;Cycles/ADD&lt;/strong&gt; column shows the number of cycles taken per add instruction over the entire slow region (roughly the first 8-10 μs after the payload starts executing). The &lt;strong&gt;Delta Cycles (slow)&lt;/strong&gt; shows how many cycles each additional &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;add&lt;/code&gt; instruction took compared to the previous row: i.e., for row N = 30, it determines how much longer the 10 additional &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;add&lt;/code&gt; instructions took compared to the row N = 20. The &lt;strong&gt;Delta Cycles (fast)&lt;/strong&gt; column is the same thing, but applies to the samples after ~10 μs when the CPU is back up to full speed (that column shows the expected 1.0 cycles per additional add).&lt;/p&gt;

&lt;p&gt;Here we clearly see that up to roughly 70 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;add&lt;/code&gt; instructions, interleaved with a single &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;vpor&lt;/code&gt;, all the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;add&lt;/code&gt; instructions are taking 4 cycles, i.e., the CPU is throttled. Somewhere between 80 and 90 a transition happens: &lt;em&gt;additional&lt;/em&gt; &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;add&lt;/code&gt; instructions now take 1 cycle, but the overall time per &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;add&lt;/code&gt; is (initially) close to 4. This shows that when &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;add&lt;/code&gt; (and presumably any non-wide instruction) is far enough away from the closest wide &lt;abbr title=&quot;Single Instruction Multiple Data: an ISA type or ISA extension like Intel's AVX or ARM's NEON that can perform multiple identical operations on elements packed into a SIMD register.&quot;&gt;SIMD&lt;/abbr&gt; instruction, they start executing at full speed. So the timings for the larger N values can be understood as a blend of a slow section of ~70-80 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;add&lt;/code&gt; instructions near the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;vpor&lt;/code&gt; which run at 1 per 4 cycles, and the remaining section where they run at full speed: 1 per cycle.&lt;/p&gt;

&lt;p&gt;We can probably conclude the CPU is not just throttling frequency or “duty cycling”: in that case every instruction would be slowed down by the same factor, but instead the rule is more like “latency extended to the next multiple of 4 cycles”, e.g., a latency 3 instruction like &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;imul eax, eax, 0&lt;/code&gt; ends up taking 4 cycles when the CPU is throttling. It is likely that the throttling happens at some part of the pipeline before execution, e.g., at issue or dispatch.&lt;/p&gt;

&lt;p&gt;The transition to fast mode when the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;vpor&lt;/code&gt; instructions are spread sufficiently apart probably reflects the size of some structure such as the &lt;abbr title=&quot;Queue that collects incoming instructions from the decoder, uop cache or microcode engine and delivers them to the renamer (RAT).&quot;&gt;IDQ&lt;/abbr&gt; (64 entries in Skylake) or scheduler (97 entries claimed&lt;sup id=&quot;fnref:ratentries&quot;&gt;&lt;a href=&quot;#fn:ratentries&quot; class=&quot;footnote&quot;&gt;30&lt;/a&gt;&lt;/sup&gt;). The core could track whether &lt;em&gt;any&lt;/em&gt; wide instruction currently in that structure, and enforce the slow mode if so. The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;vpor&lt;/code&gt; instructions are close enough together, there is &lt;em&gt;always&lt;/em&gt; at least one present, but once they are spaced out enough, you get periods of fast mode.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Voltage Effects&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;We can actually test the theory that this transition is associated with waiting for a change in power delivery configuration. Specifically, we can observe the CPU core voltage, using bits 47:32 of the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;MSR_PERF_STATUS&lt;/code&gt; MSR. Volume 4 of the Intel Software Development Manual let’s us on a secret: these bits expose the &lt;em&gt;core voltage&lt;sup id=&quot;fnref:vcc&quot;&gt;&lt;a href=&quot;#fn:vcc&quot; class=&quot;footnote&quot;&gt;31&lt;/a&gt;&lt;/sup&gt;&lt;/em&gt;:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/avxfreq1/msr_198h.png&quot; alt=&quot;Intel SDM Volume 4: Table 2-20&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Let’s zoom as usual on a transition point, in this case using a 256-bit (ymm) payload of 1000 dependent &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;vpor&lt;/code&gt; instructions. This 256-bit payload means no frequency transition, only a dispatch throttling period associated with running 256-bit instructions for the first time in a while. We plot the time it takes to run an iteration of the payload&lt;sup id=&quot;fnref:whyp&quot;&gt;&lt;a href=&quot;#fn:whyp&quot; class=&quot;footnote&quot;&gt;32&lt;/a&gt;&lt;/sup&gt;, along with the measured voltage:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/avxfreq1/fig-volts256-1.svg&quot; alt=&quot;Voltage Changes&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The length of the throttling period is around 10 μs as usual, as shown by the period where the payload takes ~4,000 cycles (the usual 4x throttling), and the voltage is unchanged from the pre-transition period (at about 0.951 V) during the throttling period. At the moment the throttling stops, the voltage jumps to about 0.957, a change of about 6 mV. This happens at 2.6 GHz, the nominal non-turbo speed of my i7-6700HQ. At 3.5 GHz, the transition is from 1.167 to 1.182, so both the absolute voltages and the difference (about 15 mV) are larger, consistent the basic idea that higher frequencies need higher voltages.&lt;/p&gt;

&lt;p&gt;So one theory is that this type of transition represents the period between when the CPU has requested a higher voltage (because wider 256-bit instructions imply a larger worst-case current delta event, hence a worst-case voltage drop) and when the higher voltage is delivered. While the core waits for the change to take effect, throttling is in effect in order to reduce the worst-case drop: without throttling&lt;sup id=&quot;fnref:cthrottle&quot;&gt;&lt;a href=&quot;#fn:cthrottle&quot; class=&quot;footnote&quot;&gt;33&lt;/a&gt;&lt;/sup&gt; there is no guarantee that a burst of wide &lt;abbr title=&quot;Single Instruction Multiple Data: an ISA type or ISA extension like Intel's AVX or ARM's NEON that can perform multiple identical operations on elements packed into a SIMD register.&quot;&gt;SIMD&lt;/abbr&gt; instructions won’t drop the voltage below the minimum voltage required for safe operation at this frequency.&lt;/p&gt;

&lt;h4 id=&quot;attenuation&quot;&gt;Attenuation&lt;/h4&gt;

&lt;p&gt;We might check if there is any &lt;em&gt;attenuation&lt;/em&gt; of either type of transition. By &lt;em&gt;attenuation&lt;/em&gt; I mean that if a core is transitioning between frequencies too frequently, the power management algorithm may decide to simply keep the core at the lower frequency, which can provide more overall performance when considering the halted periods needed in each transition. This is exactly what happens for active core count&lt;sup id=&quot;fnref:acc&quot;&gt;&lt;a href=&quot;#fn:acc&quot; class=&quot;footnote&quot;&gt;34&lt;/a&gt;&lt;/sup&gt; transitions: too many transitions in a short period of time and the CPU will just decide to run at the lower frequency rather than incurring the halts need to transition between e.g. the 1-core and 2-core turbos&lt;sup id=&quot;fnref:hiddenbo&quot;&gt;&lt;a href=&quot;#fn:hiddenbo&quot; class=&quot;footnote&quot;&gt;35&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;p&gt;We check this by setting a duty period which is just above the observed recovery time from 2.8 to 3.2 GHz, to see if we still see transitions. Here’s a duty cycle of 760 μs, about 10 μs more than the observed recovery period for this test&lt;sup id=&quot;fnref:recovery&quot;&gt;&lt;a href=&quot;#fn:recovery&quot; class=&quot;footnote&quot;&gt;36&lt;/a&gt;&lt;/sup&gt;:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/avxfreq1/fig-vporvz512-ipc-p760.svg&quot; alt=&quot;760 μs period closeup&quot; /&gt;&lt;/p&gt;

&lt;p&gt;I’m not going to color the regions here, as by now I think you are probably (over?) familiar with them. The key points are:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The payload starts executing at 7600 μs, which is &lt;em&gt;before&lt;/em&gt; the upwards frequency transition, we are still executing at 2.8 GHz - so initially the &lt;abbr title=&quot;Instructions per cycle: calculated over an interval by measuring the number of instructions executed and the duration in cycles.&quot;&gt;IPC&lt;/abbr&gt; is high, 1 per cycle.&lt;/li&gt;
  &lt;li&gt;Despite the fact that we are already executing again 512-bit instructions, the frequency adjusts upwards a few μs later. Most likely what happened is that the power logic already evaluated earlier (say at ~7558 μs, just before the payload started) that an upwards transition should occur, but as we’ve seen the response is generally delayed by 8 to 10 μs so it occurs after the payload has already started executing.&lt;/li&gt;
  &lt;li&gt;Of course, as soon as the transition occurs, the core is no longer in a suitable state for full-speed wide &lt;abbr title=&quot;Single Instruction Multiple Data: an ISA type or ISA extension like Intel's AVX or ARM's NEON that can perform multiple identical operations on elements packed into a SIMD register.&quot;&gt;SIMD&lt;/abbr&gt; execution, so &lt;abbr title=&quot;Instructions per cycle: calculated over an interval by measuring the number of instructions executed and the duration in cycles.&quot;&gt;IPC&lt;/abbr&gt; drops to ~0.25.&lt;/li&gt;
  &lt;li&gt;Another transition back to low frequency occurs ~10 μs later and then full speed execution can resume.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;So there is no attenuation, but attenuation isn’t really needed: the long (~650 μs) cooldown period between the last wide instruction and subsequent frequency boost means that the damage from halt periods are fairly limited: this is unlike the active core count scenario where the CPU has no control over the transition frequency (rather it is driven by interrupts and changes in runnable processes and threads). Here, we have the worst case scenario of transitions packed as closely as possible, but we lose only ~20 μs (for 2 transitions) out of 760 μs, less than a 3% impact. The impact of running at the lower frequency is much higher: 2.8 vs 3.2 GHz: a 12.5% impact in the case that the lowered frequency was not useful (i.e., because the wide &lt;abbr title=&quot;Single Instruction Multiple Data: an ISA type or ISA extension like Intel's AVX or ARM's NEON that can perform multiple identical operations on elements packed into a SIMD register.&quot;&gt;SIMD&lt;/abbr&gt; payload represents a vanishingly small part of the total work).&lt;/p&gt;

&lt;h2 id=&quot;what-was-left-out&quot;&gt;What Was Left Out&lt;/h2&gt;

&lt;p&gt;There’s lots we’ve left out. We haven’t even touched:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Checking whether xmm registers also cause a voltage-only transition, if they haven’t been used for a while. We didn’t find &lt;a href=&quot;#xmmeffect&quot;&gt;any effect&lt;/a&gt;, but it also certain that some 128-bit instructions appear in the measurement loop which would hide the effect.&lt;/li&gt;
  &lt;li&gt;Checking whether the voltage-only transition implied by 256-bit instructions are disjoint from those for 512-bit. That is, if you execute a 256-bit instruction after a while without any, you get a voltage-only transition (confirmed above). If you then execute a 512-bit instruction, before the relaxation period expires, do you get a second throttling period prior to the frequency transition? I believe so but I haven’t checked it.&lt;/li&gt;
  &lt;li&gt;Any type of investigation of “heavy” 256-bit or 512-bit instructions. These require a license one level (numerically) higher than light instructions, and knowing if any of the key timings change would be interesting&lt;sup id=&quot;fnref:heavy&quot;&gt;&lt;a href=&quot;#fn:heavy&quot; class=&quot;footnote&quot;&gt;37&lt;/a&gt;&lt;/sup&gt;.&lt;/li&gt;
  &lt;li&gt;Almost no investigation was made how any of these timings (and the magnitude of voltage changes) vary with frequency. For example, if we are already running at a lower frequency, frequency transitions are presumably not needed, and voltage-only transitions may be shorter.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;summary&quot;&gt;Summary&lt;/h2&gt;

&lt;p&gt;For the benefit of anyone who just skipped to the bottom, or whose eyes glazed over at some point, here’s a summary of the key findings:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;After a period of about 680 μs not using the &lt;em&gt;AVX upper bits&lt;/em&gt;  (255:128) or &lt;em&gt;AVX-512 upper bits&lt;/em&gt; (511:256) the processor enters a mode where using those bits again requires at least a voltage transition, and sometimes a frequency transition.&lt;/li&gt;
  &lt;li&gt;The processor continues executing instructions during a voltage transition, but at a greatly reduced speed: 1/4th the usual instruction dispatch rate. However, this throttling is fine-grained: it only applies when wide instructions are &lt;em&gt;in flight&lt;/em&gt; (&lt;a href=&quot;#throttle-anchor&quot;&gt;details&lt;/a&gt;).&lt;/li&gt;
  &lt;li&gt;Voltage transitions end when the voltage reaches the desired level, this depends on the magnitude of the transition but 8 to 20 μs is common on the hardware I tested.&lt;/li&gt;
  &lt;li&gt;In some cases a frequency transitions is also required, e.g., because the involved instruction requires a higher power license. These transitions seem to &lt;em&gt;first&lt;/em&gt; incur a throttling period similar to a voltage-only transition, and then a halted period of 8 to 10 μs while the frequency changes.&lt;/li&gt;
  &lt;li&gt;A key motivator for this post was to give concrete, qualitative guidance on how to write code that is as fast as possible given this behavior. It got bumped to part 2.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We also summarize the key timings in this beautifully rendered table:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;What&lt;/th&gt;
      &lt;th&gt;Time&lt;/th&gt;
      &lt;th&gt;Description&lt;/th&gt;
      &lt;th&gt;Details&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Voltage Transition&lt;/td&gt;
      &lt;td&gt;~8 to 20 μs&lt;/td&gt;
      &lt;td&gt;Time required for a voltage transition, depends on frequency&lt;/td&gt;
      &lt;td&gt;&lt;sup id=&quot;fnref:t1deets&quot;&gt;&lt;a href=&quot;#fn:t1deets&quot; class=&quot;footnote&quot;&gt;38&lt;/a&gt;&lt;/sup&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Frequency Transition&lt;/td&gt;
      &lt;td&gt;~10 μs&lt;/td&gt;
      &lt;td&gt;Time required for the halted part of a frequency transition&lt;/td&gt;
      &lt;td&gt;&lt;sup id=&quot;fnref:fdeets&quot;&gt;&lt;a href=&quot;#fn:fdeets&quot; class=&quot;footnote&quot;&gt;39&lt;/a&gt;&lt;/sup&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Relaxation Period&lt;/td&gt;
      &lt;td&gt;~680 μs&lt;/td&gt;
      &lt;td&gt;Time required to go back to a lower power license, measured from the last instruction requiring the higher license&lt;/td&gt;
      &lt;td&gt;&lt;sup id=&quot;fnref:lldeets&quot;&gt;&lt;a href=&quot;#fn:lldeets&quot; class=&quot;footnote&quot;&gt;40&lt;/a&gt;&lt;/sup&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&quot;thanks&quot;&gt;Thanks&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://lemire.me&quot;&gt;Daniel Lemire&lt;/a&gt; who provided access to the AVX-512 system I used for testing.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://twitter.com/thekanter&quot;&gt;David Kanter&lt;/a&gt; of &lt;a href=&quot;http://www.realworldtech.com&quot;&gt;RWT&lt;/a&gt; for a fruitful discussion on power and voltage management in modern chips.&lt;/p&gt;

&lt;p&gt;RWT forum members anon³, Ray, Etienne, Ricardo B, Foyle and Tim McCaffrey who provided feedback on this post and helped me understand the VR landscape for recent Intel chips.&lt;/p&gt;

&lt;p&gt;Alexander Monakov, Kharzette and Justin Lebar for finding typos.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://twitter.com/JeffSmith888&quot;&gt;Jeff Smith&lt;/a&gt; for teaching me about spread spectrum clocking.&lt;/p&gt;

&lt;h2 id=&quot;discuss&quot;&gt;Discuss&lt;/h2&gt;

&lt;p&gt;Discussion on &lt;a href=&quot;https://news.ycombinator.com/item?id=22077974&quot;&gt;Hacker News&lt;/a&gt;, &lt;a href=&quot;https://twitter.com/trav_downs/status/1218238653354344449&quot;&gt;Twitter&lt;/a&gt; and &lt;a href=&quot;https://lobste.rs/s/qaqmyo/gathering_intel_on_intel_avx_512&quot;&gt;lobste.rs&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Direct feedback also welcomed by &lt;a href=&quot;mailto:travis.downs@gmail.com&quot;&gt;email&lt;/a&gt; or as &lt;a href=&quot;https://github.com/travisdowns/travisdowns.github.io/issues&quot;&gt;a GitHub issue&lt;/a&gt;.&lt;/p&gt;

&lt;p class=&quot;info&quot;&gt;If you liked this post, check out the &lt;a href=&quot;/&quot;&gt;homepage&lt;/a&gt; for others you might enjoy&lt;sup id=&quot;fnref:noenjoy&quot;&gt;&lt;a href=&quot;#fn:noenjoy&quot; class=&quot;footnote&quot;&gt;41&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;hr /&gt;
&lt;hr /&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;footnotes&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;div class=&quot;footnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:first&quot;&gt;
      &lt;p&gt;… and also &lt;em&gt;non frequency&lt;/em&gt; related performance events, which I mention only in a footnote not to spoil the fun for non-footnote type people and also to pad by footnote count. That’s why I call this &lt;em&gt;Performance&lt;/em&gt; Transitions, instead of Frequency Transitions. &lt;a href=&quot;#fnref:first&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:dmore&quot;&gt;
      &lt;p&gt;Note that Daniel has &lt;a href=&quot;https://lemire.me/blog/2018/08/25/avx-512-throttling-heavy-instructions-are-maybe-not-so-dangerous/&quot;&gt;written&lt;/a&gt; &lt;a href=&quot;https://lemire.me/blog/2018/08/13/the-dangers-of-avx-512-throttling-myth-or-reality/&quot;&gt;much&lt;/a&gt; &lt;a href=&quot;https://lemire.me/blog/2018/08/15/the-dangers-of-avx-512-throttling-a-3-impact/&quot;&gt;more&lt;/a&gt; &lt;a href=&quot;https://lemire.me/blog/2018/08/24/trying-harder-to-make-avx-512-look-bad-my-quantified-and-reproducible-results/&quot;&gt;than&lt;/a&gt; &lt;a href=&quot;https://lemire.me/blog/2018/04/19/by-how-much-does-avx-512-slow-down-your-cpu-a-first-experiment/&quot;&gt;just&lt;/a&gt; &lt;a href=&quot;https://lemire.me/blog/2018/09/04/per-core-frequency-scaling-and-avx-512-an-experiment/&quot;&gt;that&lt;/a&gt; one. &lt;a href=&quot;#fnref:dmore&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:intro&quot;&gt;
      &lt;p&gt;This was going to be the actual post I was trying to write when I went off on &lt;a href=&quot;//blog/2019/11/19/toupper.html&quot;&gt;this tangent about clang-format&lt;/a&gt;. In fact I &lt;em&gt;was&lt;/em&gt; writing that post, when I went off this current tangent, but then a footnote turned into several paragraphs, then got its own section and ultimately graduated into a whole post: the one you are reading. So consider this background reading for the “interesting” post still to come, although honestly the stuff here is probably more generally useful than the next part. &lt;a href=&quot;#fnref:intro&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:widthmatters&quot;&gt;
      &lt;p&gt;We should be clear here: when I say AVX-512 instruction in this context, I mean specifically a &lt;em&gt;512-bit wide instruction&lt;/em&gt; (which currently only exist in AVX-512). The distinction is that AVX-512 includes 128-bit and 256-bit versions of almost every instruction it introduces, yet these narrower instructions behave just like 128-bit SSE* and 256-bit AVX* instructions in terms of performance transitions. So, for example, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;vpermw&lt;/code&gt; is unabiguously &lt;em&gt;AVX-512&lt;/em&gt; instruction: it only exists in AVX-512BW, but only the version that takes &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;zmm&lt;/code&gt; registers causes “AVX-512 like” performance transitions: the versions that take &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ymm&lt;/code&gt; or &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;xmm&lt;/code&gt; registers act as any other integer 256-bit AVX2 or 128-bit SSE instruction. &lt;a href=&quot;#fnref:widthmatters&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:speriod&quot;&gt;
      &lt;p&gt;In fact, we generally want the sample period to be as small as possible, to give the best resolution and insight into short-lived events. We can’t make it &lt;em&gt;too&lt;/em&gt; short though as the samples themselves have a minimum time to capture, and very short samples tend to be noisy due to non-atomicity, quantization effects, etc. &lt;a href=&quot;#fnref:speriod&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:f2104&quot;&gt;
      &lt;p&gt;The CPU is an &lt;a href=&quot;https://en.wikichip.org/wiki/intel/xeon_w/w-2104&quot;&gt;Intel W-2104&lt;/a&gt;, a Xeon-W chip based on the &lt;abbr title=&quot;Intel's Skylake (server) architecture including Skylake-SP, Skylake-X and Skylake-W&quot;&gt;SKX&lt;/abbr&gt; &lt;abbr title=&quot;Microarchitecture: a specific implementation of an ISA, e.g., &amp;quot;Haswell microarchitecture&amp;quot;.&quot;&gt;uarch&lt;/abbr&gt;. It has no turbo and an on-the-box speed of 3200 MHz, but accurate tools will probably report it running at 3192 MHz due to &lt;em&gt;&lt;a href=&quot;https://twitter.com/JeffSmith888/status/1211821823035351043&quot;&gt;spread spectrum clocking&lt;/a&gt;&lt;/em&gt; (SSC). In fact, we can see the typical 0.5% spread spectrum triangle wave on almost any of the plots in this post, at the right zoom level, &lt;a href=&quot;/assets/avxfreq1/fig-ssc.svg&quot;&gt;like this one&lt;/a&gt;. This occurs because we measure time (the x-axis) based on &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;rdtsc&lt;/code&gt; which is based off of a different clock not subject to SSC, while the unhalted cycles counter counts CPU cycles, which are based off of the 100 MHz BLCK which is subject to SSC. &lt;a href=&quot;#fnref:f2104&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:avxt&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://www.realworldtech.com/forum/?threadid=179358&amp;amp;curpostid=179652&quot;&gt;Measured&lt;/a&gt; with &lt;a href=&quot;https://github.com/travisdowns/avx-turbo&quot;&gt;avx-turbo&lt;/a&gt;. &lt;a href=&quot;#fnref:avxt&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:vz&quot;&gt;
      &lt;p&gt;This is the part where I just gloss over what that &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;vzeroupper&lt;/code&gt; is doing there. It’s there due to &lt;em&gt;implicit widening&lt;/em&gt;. That’s a new term I just invented and it’s the first and last time I’m mentioning it this post, because it really deserves an entire post of its own. The very short version is that any time an &lt;abbr title=&quot;Single Instruction Multiple Data: an ISA type or ISA extension like Intel's AVX or ARM's NEON that can perform multiple identical operations on elements packed into a SIMD register.&quot;&gt;SIMD&lt;/abbr&gt; instruction writes to an N-bit register (N in {256, 512}), all subsequent &lt;abbr title=&quot;Single Instruction Multiple Data: an ISA type or ISA extension like Intel's AVX or ARM's NEON that can perform multiple identical operations on elements packed into a SIMD register.&quot;&gt;SIMD&lt;/abbr&gt; instructions are &lt;em&gt;implicitly&lt;/em&gt; N bits wide, regardless of their actual width, for the purposes of determining turbo licenses and other transitions discussed here. This sounds a bit like the &lt;a href=&quot;https://stackoverflow.com/q/41303780/149138&quot;&gt;mixed-VEX penalties thing&lt;/a&gt;, but it is very different. This a mini-bombshell hidden in a footnote, so if you want to scoop me you can, but I’m not coming to your birthday party. &lt;a href=&quot;#fnref:vz&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:pperiod&quot;&gt;
      &lt;p&gt;I.e., the &lt;em&gt;payload period&lt;/em&gt; is zero, but the structure of the test ensures the function is called once, at the start of the payload period, no matter how small the payload period is. &lt;a href=&quot;#fnref:pperiod&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:plotnotes&quot;&gt;
      &lt;p&gt;All of the plots in this post are SVG images, meaning they can be zoomed arbitrarily: so if you want to zoom in on any region feel free (the browser limits the zoom amount, but just save as a file and open it with any SVG viewer). &lt;a href=&quot;#fnref:plotnotes&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:falk&quot;&gt;
      &lt;p&gt;These are basically a poor man’s version of the &lt;em&gt;Falk Diagrams&lt;/em&gt; that Brandon Falk describes in &lt;a href=&quot;https://gamozolabs.github.io/metrology/2019/08/19/sushi_roll.html&quot;&gt;this post&lt;/a&gt; among others. Poor in the sense that they have ~3000 cycle resolution instead of 1 cycle resolution, and because the measurement code has to be integrated directly into the system under test. Basically they are nothing like &lt;em&gt;Falk Diagrams&lt;/em&gt; except that they have time on the x-axis and some performance counter event on the y-axis, but they are good enough for our purposes. &lt;a href=&quot;#fnref:falk&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:threerun&quot;&gt;
      &lt;p&gt;Specifically, all three runs are identical and I show them just to give a rough impression of which effects are reproducible and which are outliers. The second and third runs have the suffixes &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;_1&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;_2&lt;/code&gt;, respectively, in the legends. &lt;a href=&quot;#fnref:threerun&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:outlie&quot;&gt;
      &lt;p&gt;These outliers usually occur when an interrupt occurs during the measurement. Originally I used a 3 μs sample time, and there were almost no visible outliers with that period, but the 1 μs value I settled on is much better in most respects other than outliers. The main problem is when an interrupt takes more than the sample time of ~1 μs: this causes one or more samples to be very short, because a long sample will cause subsequent ones to be short (maybe &lt;em&gt;very&lt;/em&gt; short) until we catch up to the fixed sample schedule, and very short samples are subject to much more noise because the absolute metric values are much smaller but the error sources usually have fixed absolute error. Another source of outliers is when an interrupt &lt;em&gt;splits the stamp&lt;/em&gt;: the &lt;em&gt;stamp&lt;/em&gt; is the series of metrics we calculate at the sample point. These various metrics aren’t sampled atomically: if an interrupt occurs in the middle of the sampling, some metrics will reflect a much shorter time period (before the interrupt) and some a longer one (after). This effect tends to cause bidirectional spike outliers: where an upspike and downspike occur in consecutive samples. I try to avoid this by retrying the stamp if I think I’ve detected an interrupt during measurement (up to a retry limit). We could avoid all this nonsense by running the benchmark itself in the kernel, where we can disable interrupts (although some SMIs might still sneak in). Maybe next time! &lt;a href=&quot;#fnref:outlie&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:peachpuff&quot;&gt;
      &lt;p&gt;More precisely, the color is &lt;a href=&quot;https://encycolorpedia.com/ffdab9&quot;&gt;peachpuff&lt;/a&gt;. &lt;a href=&quot;#fnref:peachpuff&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:resnote&quot;&gt;
      &lt;p&gt;Note that the resolution of the sampling is 1 μs, so when we say things like &lt;em&gt;9 μs&lt;/em&gt; and &lt;em&gt;11 μs&lt;/em&gt; it could be off by up to 1 μs. This 1 μs “error” isn’t exactly randomly distributed, because the sampling interval is “exact” and in phase with the payload execution. So the samples are always at 1.0, 2.0, 3.0, … μs after the payload executes (plus or minus small variation on the order of 10 nanoseconds), so if the true time until halt is anywhere between 9.00 and 9.99 μs, we will always read 9 μs (because time shown for the sample is the &lt;em&gt;end&lt;/em&gt; of the sample and covers the preceding 1 us). For the halt time, the scenario is reversed: the start of the interval is uncertain, but the end should be exact modulo the delay in taking the sample. &lt;a href=&quot;#fnref:resnote&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:halted&quot;&gt;
      &lt;p&gt;Generally you can detect halted periods when &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;rdtsc&lt;/code&gt; jumps forward, but performance counters like “non-halted cycles” do not, and there are not indications of a larger interruption such as a context switch. You can find a similar case of halted periods &lt;a href=&quot;https://stackoverflow.com/q/45472147/149138&quot;&gt;in this question&lt;/a&gt; which was also caused by frequency transitions (in this case, to obey the different active core count turbo ratios). &lt;a href=&quot;#fnref:halted&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:thistle&quot;&gt;
      &lt;p&gt;More precisely, the color is &lt;a href=&quot;https://encycolorpedia.com/d8bfd8&quot;&gt;thistle&lt;/a&gt;. &lt;a href=&quot;#fnref:thistle&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:confirm&quot;&gt;
      &lt;p&gt;In particular, I have confirmed that these three samples all occur after the payload has executed and retired using the &lt;em&gt;period&lt;/em&gt; column in the output, which indicates clearly which sames are before or after a given execution of the payload. The payload itself is followed by an &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;lfence&lt;/code&gt; to ensure it has retired before taking further samples (and in any case the number of instructions per sample is too large be accommodated by the &lt;abbr title=&quot;Out-of-order execution allows CPUs to execute instructions out of order with respect to the source.&quot;&gt;OoO&lt;/abbr&gt; window). &lt;a href=&quot;#fnref:confirm&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:whynot&quot;&gt;
      &lt;p&gt;Those who are still awake at this point might be wondering why introduce this new variant of the test now: why not just execute the payload instructions during the wait period in the original tests too? One reason is that by hot spinning on &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;rdtsc&lt;/code&gt; we get somewhat more consistent results when we care only about measuring the frequency in that we almost always sample at exactly the specified period (plus or minus the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;rdtsc&lt;/code&gt; latency, more or less). When we execute the longish payload function during the wait, the sample point diverges a bit more from the ideal, and the number of spins per sample suffers more quantization effects (i.e., the pattern of spin counts might be 3,3,3,2,3,3,3,2… rather than 450,450,451,450…), which can sometimes lead to a slight sawtooth effect in the samples. &lt;a href=&quot;#fnref:whynot&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:ideal&quot;&gt;
      &lt;p&gt;In practice, we don’t exactly reach this ideal: we execute the payload function 2 or 3 times, for 2,000 or 3,000 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;vpor&lt;/code&gt; instructions, but there are about 600 additional instructions of overhead associated with taking a sample, so the overhead instructions are a significant portion. Probably 600 instructions is too many, I haven’t optimized that and it could likely be lowered significantly. However, we can also improve the ratio simply by decreasing the sampling resolution (i.e., increasing the sampling time). I selected 1 μs as a tradeoff between these competing factors. Note: Since I wrote this footnote I optimized several things in the sampling loop, so measured IPCs are now very close to their theoretical values, but I guess this footnote still has value?. &lt;a href=&quot;#fnref:ideal&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:give&quot;&gt;
      &lt;p&gt;The main uncertainty in the timing of the function itself concerns the boundary conditions: if we run this function 10 times &lt;em&gt;without&lt;/em&gt; touching &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;zmm0&lt;/code&gt; in between, the dependency on &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;zmm0&lt;/code&gt; will be carried between functions and the total time will be very close to 10 x 1000 = 10,000 cycles. However, if some compiler generated code in between calls to the payload function happens to write to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;zmm0&lt;/code&gt;, breaking the dependency, the individual chains for each function no longer depend on each other, so some overlap is possible. The amount of this overlap is limited by the size of the RS, so the effect won’t be &lt;em&gt;huge&lt;/em&gt; but it could be noticeable (with say 100 payload instructions rather than 1,000 it could be very significant). We basically sidestep this whole issue by putting an &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;lfence&lt;/code&gt; between each call to the payload function, which forms an &lt;em&gt;execution barrier&lt;/em&gt; between calls. &lt;a href=&quot;#fnref:give&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:catchup&quot;&gt;

      &lt;p&gt;The way the sampling works in this test could be described &lt;em&gt;locked interval without skipping&lt;/em&gt;. Here, &lt;em&gt;locked interval&lt;/em&gt; means we calculate the next target sample time based on the previous &lt;em&gt;target&lt;/em&gt; sample time (rather than the &lt;em&gt;actual&lt;/em&gt; sample time), plus the period. So if we are sampling at 10 μs, the target sample times will always be 10 μs, 20 μs, etc. In particular, the series of target sample time doesn’t depend on what happens during the test, e.g., it doesn’t depend on the actual sample time: even if we actually end up sampling at time = 12 μs rather than 10, we target time = 20 μs as the next sample, not 12 + 10 = 22 μs. This raises the question about what happens when some delay (e.g., an interrupt, a frequency transition) causes us to miss more than 1 entire sample period.&lt;/p&gt;

      &lt;p&gt;E.g., with 10 μs resolution we just sampled at 90 μs, so the next sample target is 100 μs, but a delay causes the next sample to be taken at 125 μs. We are now behind! The next sample should occur at 110 μs, but of course that is in the past. The current test design still takes all the required samples, as quickly as possible (but with a minimum of one payload call if we are in the &lt;em&gt;extra payload period&lt;/em&gt;) – that’s the &lt;em&gt;no skipping&lt;/em&gt; part. In the current example, it means we would take subsequent samples quickly until we are caught up, at say 125 μs (target 110 μs), 126 μs (target 120 μs), 130 μs (target 130 μs), with that last sample being “on target”.&lt;/p&gt;

      &lt;p&gt;These samples occur quickly: very quickly in the case of normal samples which take less than 0.2 μs each, or more slowly in the case of the &lt;em&gt;extra payload&lt;/em&gt; region, where the payload call bumps that to about 0.5 μs. So that’s what’s happening in the green region: we just had a frequency transition halt of ~10 μs, so we are ~10 samples beyind and so the following samples are taken more quickly (as you can see because the data point markers are spaced more closely), with only a single payload call each (versus 2 or 3 usually). This changes the ratio of payload instructions to overhead instructions, which tends to bump the &lt;abbr title=&quot;Instructions per cycle: calculated over an interval by measuring the number of instructions executed and the duration in cycles.&quot;&gt;IPC&lt;/abbr&gt; a bit (for the same reason the caught-up blue region almost shows &lt;abbr title=&quot;Instructions per cycle: calculated over an interval by measuring the number of instructions executed and the duration in cycles.&quot;&gt;IPC&lt;/abbr&gt; &amp;gt; 1). This effect can be reduced or eliminated by increasing the sampling period, since that reduces the number of catchup samples.&lt;/p&gt;

      &lt;p&gt;Incidentally, this also explains the oscillating pattern you see in the blue region: the ideal number of payload calls to get a 1 μs sample rate is ~2.5, so the sampling strategy tends to alternative between two and three calls: more calls means &lt;abbr title=&quot;Instructions per cycle: calculated over an interval by measuring the number of instructions executed and the duration in cycles.&quot;&gt;IPC&lt;/abbr&gt; closer to 1: so the peaks of the oscillating are two calls and the valleys, three. &lt;a href=&quot;#fnref:catchup&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:tput&quot;&gt;
      &lt;p&gt;I’m not going to fully analyze the throughput case, but the thorough among us can find the chart is &lt;a href=&quot;/assets/avxfreq1/fig-ipc-zoomed-zmm-tput.svg&quot;&gt;here&lt;/a&gt;. Note that the overhead here cuts the other way: pushing the &lt;abbr title=&quot;Instructions per cycle: calculated over an interval by measuring the number of instructions executed and the duration in cycles.&quot;&gt;IPC&lt;/abbr&gt; below the expected value of 2.0 (there are 2 512-bit vector units capable of executing &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;vpord&lt;/code&gt;) because here the throughput limited instructions compete for ports with the overhead instructions. &lt;a href=&quot;#fnref:tput&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:orisit&quot;&gt;
      &lt;p&gt;Of course, another possibility is that something in the rest of the test loop uses &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;xmm&lt;/code&gt; registers so they remain “hot”: they are “baseline” for x86-64 after all, so the compiler is free to use them without any special flags. We could test this theory with a more compact test loop audited to be free of any &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;xmm&lt;/code&gt; use… but I’m not going to bother. I’m pretty sure these guys are powered up all the time as their use is pervasive in most x86-64 code. &lt;a href=&quot;#fnref:orisit&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:upper&quot;&gt;
      &lt;p&gt;Specifically, the the part handing the second (bits 128:255) for the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ymm&lt;/code&gt; case, and the upper 3 lanes (bits 128:511) in the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;zmm&lt;/code&gt; case. &lt;a href=&quot;#fnref:upper&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:amd&quot;&gt;
      &lt;p&gt;This isn’t far fetched - that’s exactly how AMD Zen always executes 256-bit AVX instructions with its 128 bit units, and similar to how SVB and &lt;abbr title=&quot;Intel's Ivy Bridge architecture, aka 3rd Generation Intel Core i3,i5,i7&quot;&gt;IVB&lt;/abbr&gt; did the same for 256-bit load and store instructions. So using narrower vector units to implement wider instructions is definitely a thing. &lt;a href=&quot;#fnref:amd&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:ewise&quot;&gt;
      &lt;p&gt;These are usually called &lt;em&gt;cross lane&lt;/em&gt; instructions (where a lane is 128 bits on Intel). For example, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;vpor&lt;/code&gt; is &lt;em&gt;not&lt;/em&gt; like this: it is an &lt;em&gt;element-wise&lt;/em&gt; operation where each output element (down to each bit, in this case) depends only on the element in the same position in the input vectors. On other the hand, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;vpermd&lt;/code&gt; is: each 32-bit element in the output can come from &lt;em&gt;any&lt;/em&gt; position in the input vector (but it still behaves as element-wise wrt the mask register&lt;sup id=&quot;fnref:bonus&quot;&gt;&lt;a href=&quot;#fn:bonus&quot; class=&quot;footnote&quot;&gt;42&lt;/a&gt;&lt;/sup&gt;). &lt;a href=&quot;#fnref:ewise&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:lt&quot;&gt;
      &lt;p&gt;The notation 4L4T means: “4 cycles of latency and 4 cycles of inverse throughput”. That is, an given instance of this instruction takes 4 cycles to finish (latency), and a new instruction can start every 4 cycles (inverse throughput, hence the throughput is 0.25). When the CPU is running normally, most single-&lt;abbr title=&quot;Micro-operation: instructions are translated into one or more uops, which are simple operations executed by the CPU's execution units.&quot;&gt;uop&lt;/abbr&gt; &lt;abbr title=&quot;Single Instruction Multiple Data: an ISA type or ISA extension like Intel's AVX or ARM's NEON that can perform multiple identical operations on elements packed into a SIMD register.&quot;&gt;SIMD&lt;/abbr&gt; instructions have latency of 1 (in lane), 3 (most cross lane integer or shuffle ops) or 4 (most FP arithmetic). &lt;a href=&quot;#fnref:lt&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:alu&quot;&gt;
      &lt;p&gt;I say &lt;em&gt;ALU instructions&lt;/em&gt; here, but I strongly suspect it might be all instructions: you can certainly test that at home using the same test (the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;vporxymm250_*&lt;/code&gt; group of tests) but with other types of instructions such as loads replacing the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;add&lt;/code&gt;. I didn’t really test &lt;em&gt;all&lt;/em&gt; ALU instructions either: just a few - but it is fair to assume that if &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;add&lt;/code&gt; is slowed down, it is something generic, probably affecting at least all ALU stuff, not something instruction specific. &lt;a href=&quot;#fnref:alu&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:ratentries&quot;&gt;
      &lt;p&gt;Documentation claims 97 entries, but my testing seems to indicate they are not unified in Skylake: apparently only 64 can be used for ALU ops, and 33 for memory ops. &lt;a href=&quot;#fnref:ratentries&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:vcc&quot;&gt;
      &lt;p&gt;Try as I might, I can’t determine if this refers to &lt;em&gt;measured&lt;/em&gt; core voltage, i.e., true voltage as determine at some sensor within the core, or &lt;em&gt;demanded&lt;/em&gt; voltage, i.e., the voltage the processor wants right now based on the various power-relevant parameters, sometimes called the VID. In any case, we expect those values to track each other fairly closely, perhaps with some offset and since we are looking for voltage &lt;em&gt;changes&lt;/em&gt; either one works. That said, I am very interested if you know the answer to this question. &lt;a href=&quot;#fnref:vcc&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:whyp&quot;&gt;
      &lt;p&gt;This payload time series is meant to show the exact same thing as the &lt;abbr title=&quot;Instructions per cycle: calculated over an interval by measuring the number of instructions executed and the duration in cycles.&quot;&gt;IPC&lt;/abbr&gt; series in earlier charts: we just want an indication of when the Type 1 throttling starts and stops. I used &lt;abbr title=&quot;Instructions per cycle: calculated over an interval by measuring the number of instructions executed and the duration in cycles.&quot;&gt;IPC&lt;/abbr&gt; initially because it was easy (I didn’t have to instrument the payload section specially) - but it doesn’t work when reading volts because that measurement involves a ton of additional instructions and a user-kernel transition, so it throws the &lt;abbr title=&quot;Instructions per cycle: calculated over an interval by measuring the number of instructions executed and the duration in cycles.&quot;&gt;IPC&lt;/abbr&gt; off completely. So I went ahead and instrumented the payload section directly, so we can still see the throttling, but no way I want to go back and change the other plots that use &lt;abbr title=&quot;Instructions per cycle: calculated over an interval by measuring the number of instructions executed and the duration in cycles.&quot;&gt;IPC&lt;/abbr&gt;. &lt;a href=&quot;#fnref:whyp&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:cthrottle&quot;&gt;
      &lt;p&gt;The throttling here is quite conservative I think: this is only a very small voltage change (less than 1%), so it is hard to believe that 4x throttling is &lt;em&gt;necessary.&lt;/em&gt; It seems likely, for example, that cutting the dispatch rate in half would be enough to compensate for the missing 6 mV – but it is easy to imagine that just having a big conservative throttling number for all these voltage-too-low throttling scenarios is easy and safe, and these periods don’t occur often enough for it to really matter. &lt;a href=&quot;#fnref:cthrottle&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:acc&quot;&gt;
      &lt;p&gt;Essentially all modern Intel CPUs have varying maximum turbo ratios depending on the number of active (not halted or in a sleep state). E.g., my Skylake CPU can run at at a max speed of 3.5, 3.3, 3.2 or 3.1 GHz if 1, 2, 3 or 4 cores are active, respectively. If only one core is currently running, at 3.5 GHz, and another core becomes active (e.g., because the scheduler found something to run) – the first core has to immediately transition down to 3.3 GHz and as we’ve seen above, it takes an ~8-10 μs halt to do so. When the other core stops running, it can return to 3.3 GHz. If cores are flipping between inactive and active quickly enough, those halts add up and cut into your effective frequency. At some point, you might get less work done by trying to run at max turbo, versus just running at 3.3 GHz all the time since in this case no halts need to be taken when the second core starts. Further explored &lt;a href=&quot;https://stackoverflow.com/a/45592838/149138&quot;&gt;over here&lt;/a&gt;. &lt;a href=&quot;#fnref:acc&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:hiddenbo&quot;&gt;
      &lt;p&gt;Avoiding these transitions are a hidden bonus of making the 1 and 2-core turbos the same, and more generally “grouping” the turbo ratios in a more coarse grained way across core counts: you don’t need any transition when the “from” and “to” core counts have the same max turbo ratio. &lt;a href=&quot;#fnref:hiddenbo&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:recovery&quot;&gt;
      &lt;p&gt;Earlier we mentioned a low frequency duration of 650 μs, but that was the test that ran only a single payload instruction. The recovery period is measured from the &lt;em&gt;last&lt;/em&gt; wide instruction and in this test (that shows the &lt;abbr title=&quot;Instructions per cycle: calculated over an interval by measuring the number of instructions executed and the duration in cycles.&quot;&gt;IPC&lt;/abbr&gt;) we execute 100 μs of payload, so the recovery time will be 100 μs + 650 μs = ~750 μs. &lt;a href=&quot;#fnref:recovery&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:heavy&quot;&gt;
      &lt;p&gt;Unlike the transitions discussed here, transitions related to heavy instructions are &lt;em&gt;soft&lt;/em&gt; transitions: they do not occur unconditionally after a single instruction of the relevant type is executed, but rather only after some &lt;em&gt;density&lt;/em&gt; threshold is reached for those instructions. Exploring this threshold would be interesting. There is another effect mentioned in the Intel optimization manual: heavy instructions may cause a license transition even in cases where it wouldn’t normally occur, when light instructions of one license level are mixed with &lt;em&gt;half-width&lt;/em&gt; heavy instructions. That is, 128-bit heavy instructions can use the fastest L0 license, as can 256-bit light instructions. However, apparently, mixing these can cause a request for the L1 license. Similarly for 256-bit heavy instructions and 512-bit light instructions, where a downgrade from L1 to L2 could occur. &lt;a href=&quot;#fnref:heavy&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:t1deets&quot;&gt;
      &lt;p&gt;I give a range of 8 to 20 μs because that’s what I measured in my testing, but the highest frequency I measured for a voltage-only transition at was 3.5 GHz, with a 15 mV delta. It is entirely possible that at higher frequencies and voltages, times are much longer. It could also depend on the hardware, e.g., the presence or absence of a &lt;abbr title=&quot;Fully Integrated Voltage Regulator&quot;&gt;FIVR&lt;/abbr&gt;. &lt;a href=&quot;#fnref:t1deets&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:fdeets&quot;&gt;
      &lt;p&gt;This transition time seems to be required by &lt;em&gt;any&lt;/em&gt; frequency transition, whether up or down in frequency and regardless of the cause. This includes transition causes not tested here: for example, when the max turbo ratio changes because the active core count changes, or when the ideal frequency changes for any other reason. &lt;a href=&quot;#fnref:fdeets&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:lldeets&quot;&gt;
      &lt;p&gt;This same relaxation period appears to apply to both of the transitions types discussed in this post, e.g., both voltage and frequency. The relaxation timer is reset any type an instruction that needs the current license is executed. In this case, the 680 μs period is measured from the instruction that causes the transition (which is also the last relevant instruction since only a single payload instruction is used), until the time that the CPU resumes executing again at the higher frequency. This period includes one dispatch throttling period and two frequency transitions, so only about 650 μs of the 680 μs is spent executing instructions at full speed. &lt;a href=&quot;#fnref:lldeets&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:noenjoy&quot;&gt;
      &lt;p&gt;On the other hand, if you didn’t enjoy it, I have to wonder how you made it this far! Maybe you started off liking it, then felt invested and compelled to finish it? Beware the sunk cost fallacy! &lt;a href=&quot;#fnref:noenjoy&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:bonus&quot;&gt;
      &lt;p&gt;Bonus question: are there any single-&lt;abbr title=&quot;Micro-operation: instructions are translated into one or more uops, which are simple operations executed by the CPU's execution units.&quot;&gt;uop&lt;/abbr&gt; AVX/AVX-512 instructions which are cross-lane in both of their inputs? There are 3-input shuffles, like &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;VPERMI2B&lt;/code&gt; which have 2 of their inputs 3 cross-lane (the two input tables), but they need 2 uops. &lt;a href=&quot;#fnref:bonus&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><author><name>Travis Downs</name><email>travis.downs@gmail.com</email></author><category term="blog" /><category term="performance" /><category term="c++" /><category term="Intel" /><category term="uarch" /><summary type="html">Note: For the really short version, you can skip to the summary, but then what will do you for the rest of the day?</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/assets/avxfreq1/twitter-card.png" /><media:content medium="image" url="http://localhost:4000/assets/avxfreq1/twitter-card.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">A note on mask registers</title><link href="http://localhost:4000/blog/2019/12/05/kreg-facts.html" rel="alternate" type="text/html" title="A note on mask registers" /><published>2019-12-05T18:30:00+02:00</published><updated>2019-12-05T18:30:00+02:00</updated><id>http://localhost:4000/blog/2019/12/05/kreg-facts</id><content type="html" xml:base="http://localhost:4000/blog/2019/12/05/kreg-facts.html">&lt;p class=&quot;warning&quot;&gt;If you are in a rush, you can &lt;a href=&quot;#summary&quot;&gt;skip to the summary&lt;/a&gt;, but you’ll miss out on the journey.&lt;/p&gt;

&lt;p&gt;AVX-512 introduced eight so-called &lt;em&gt;mask registers&lt;/em&gt;&lt;sup id=&quot;fnref:naming&quot;&gt;&lt;a href=&quot;#fn:naming&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;k0&lt;/code&gt;&lt;sup id=&quot;fnref:k0note&quot;&gt;&lt;a href=&quot;#fn:k0note&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt; through &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;k7&lt;/code&gt;, which apply to most ALU operations and allow you to apply a zero-masking or merging&lt;sup id=&quot;fnref:maskmerge&quot;&gt;&lt;a href=&quot;#fn:maskmerge&quot; class=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt; operation on a per-element basis, speeding up code that would otherwise require extra blending operations in AVX2 and earlier.&lt;/p&gt;

&lt;p&gt;If that single sentence doesn’t immediately indoctrinate you into the mask register religion, here’s a copy and paste from &lt;a href=&quot;https://en.wikipedia.org/wiki/AVX-512#Opmask_registers&quot;&gt;Wikipedia&lt;/a&gt; that should fill in the gaps and close the deal:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Most AVX-512 instructions may indicate one of 8 opmask registers (k0–k7). For instructions which use a mask register as an opmask, register &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;k0&lt;/code&gt; is special: a hardcoded constant used to indicate unmasked operations. For other operations, such as those that write to an opmask register or perform arithmetic or logical operations, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;k0&lt;/code&gt; is a functioning, valid register. In most instructions, the opmask is used to control which values are written to the destination. A flag controls the opmask behavior, which can either be “zero”, which zeros everything not selected by the mask, or “merge”, which leaves everything not selected untouched. The merge behavior is identical to the blend instructions.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;So mask registers&lt;sup id=&quot;fnref:kreg&quot;&gt;&lt;a href=&quot;#fn:kreg&quot; class=&quot;footnote&quot;&gt;4&lt;/a&gt;&lt;/sup&gt; are important, but are not household names unlike say general purpose registers (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;eax&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;rsi&lt;/code&gt; and friends) or &lt;abbr title=&quot;Single Instruction Multiple Data: an ISA type or ISA extension like Intel's AVX or ARM's NEON that can perform multiple identical operations on elements packed into a SIMD register.&quot;&gt;SIMD&lt;/abbr&gt; registers (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;xmm0&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ymm5&lt;/code&gt;, etc). They certainly aren’t going to show up on Intel slides disclosing the size of &lt;abbr title=&quot;Microarchitecture: a specific implementation of an ISA, e.g., &amp;quot;Haswell microarchitecture&amp;quot;.&quot;&gt;uarch&lt;/abbr&gt; resources, like these:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/kreg/min/intel-skx-slide.png&quot; alt=&quot;Intel Slide&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;In particular, I don’t think the size of the mask register physical register file (&lt;abbr title=&quot;Physical register file: The hardware registers used for renaming architectural (source visible) registers, usually much larger in number than the architectural register count.&quot;&gt;PRF&lt;/abbr&gt;) has ever been reported. Let’s fix that today.&lt;/p&gt;

&lt;p&gt;We use an updated version of the &lt;abbr title=&quot;Re-order buffer: n ordered buffer which stores in-progress instructions on an out-of-order processor.&quot;&gt;ROB&lt;/abbr&gt; size &lt;a href=&quot;https://github.com/travisdowns/robsize&quot;&gt;probing tool&lt;/a&gt; originally authored and &lt;a href=&quot;http://blog.stuffedcow.net/2013/05/measuring-rob-capacity/&quot;&gt;described by Henry Wong&lt;/a&gt;&lt;sup id=&quot;fnref:hcite&quot;&gt;&lt;a href=&quot;#fn:hcite&quot; class=&quot;footnote&quot;&gt;5&lt;/a&gt;&lt;/sup&gt; (hereafter simply &lt;em&gt;Henry&lt;/em&gt;), who used it to probe the size of various documented and undocumented &lt;abbr title=&quot;Out-of-order execution allows CPUs to execute instructions out of order with respect to the source.&quot;&gt;out-of-order&lt;/abbr&gt; structures on earlier architecture. If you haven’t already read that post, stop now and do it. This post will be here when you get back.&lt;/p&gt;

&lt;p&gt;You’ve already read Henry’s blog for a full description (right?), but for the naughty among you here’s the fast food version:&lt;/p&gt;

&lt;h4 id=&quot;fast-food-method-of-operation&quot;&gt;Fast Food Method of Operation&lt;/h4&gt;

&lt;p&gt;We separate two cache miss load instructions&lt;sup id=&quot;fnref:misstime&quot;&gt;&lt;a href=&quot;#fn:misstime&quot; class=&quot;footnote&quot;&gt;6&lt;/a&gt;&lt;/sup&gt; by a variable number of &lt;em&gt;filler instructions&lt;/em&gt; which vary based on the CPU resource we are probing. When the number of filler instructions is small enough, the two cache misses execute in parallel and their latencies are overlapped so the total execution time is roughly&lt;sup id=&quot;fnref:roughly&quot;&gt;&lt;a href=&quot;#fn:roughly&quot; class=&quot;footnote&quot;&gt;7&lt;/a&gt;&lt;/sup&gt; as long as a single miss.&lt;/p&gt;

&lt;p&gt;However, once the number of filler instructions reaches a critical threshold, all of the targeted resource are consumed and instruction allocation stalls before the second miss is issued and so the cache misses can no longer run in parallel. This causes the runtime to spike to about twice the baseline cache miss latency.&lt;/p&gt;

&lt;p&gt;Finally, we ensure that each filler instruction consumes exactly one of the resource we are interested in, so that the location of the spike indicates the size of the underlying resource. For example, regular &lt;abbr title=&quot;General purpose: as opposed to SIMD or FP. On x86 often refers to instructions such as integer addition, or registers such as eax.&quot;&gt;GP&lt;/abbr&gt; instructions usually consume one physical register from the &lt;abbr title=&quot;General purpose: as opposed to SIMD or FP. On x86 often refers to instructions such as integer addition, or registers such as eax.&quot;&gt;GP&lt;/abbr&gt; &lt;abbr title=&quot;Physical register file: The hardware registers used for renaming architectural (source visible) registers, usually much larger in number than the architectural register count.&quot;&gt;PRF&lt;/abbr&gt; so are a good choice to measure the size of that resource.&lt;/p&gt;

&lt;h4 id=&quot;mask-register-prf-size&quot;&gt;Mask Register &lt;abbr title=&quot;Physical register file: The hardware registers used for renaming architectural (source visible) registers, usually much larger in number than the architectural register count.&quot;&gt;PRF&lt;/abbr&gt; Size&lt;/h4&gt;

&lt;p&gt;Here, we use instructions that write a mask register, so can measure the size of the mask register &lt;abbr title=&quot;Physical register file: The hardware registers used for renaming architectural (source visible) registers, usually much larger in number than the architectural register count.&quot;&gt;PRF&lt;/abbr&gt;.&lt;/p&gt;

&lt;p&gt;To start, we use a series of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;kaddd k1, k2, k3&lt;/code&gt; instructions, as such (shown for 16 filler instructions):&lt;/p&gt;

&lt;div class=&quot;language-nasm highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;mov&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;rcx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;QWORD&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;PTR&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[rcx]&lt;/span&gt;  &lt;span class=&quot;c&quot;&gt;; first cache miss load&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;kaddd&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;k1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;k2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;k3&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;kaddd&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;k1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;k2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;k3&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;kaddd&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;k1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;k2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;k3&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;kaddd&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;k1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;k2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;k3&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;kaddd&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;k1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;k2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;k3&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;kaddd&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;k1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;k2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;k3&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;kaddd&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;k1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;k2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;k3&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;kaddd&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;k1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;k2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;k3&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;kaddd&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;k1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;k2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;k3&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;kaddd&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;k1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;k2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;k3&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;kaddd&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;k1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;k2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;k3&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;kaddd&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;k1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;k2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;k3&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;kaddd&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;k1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;k2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;k3&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;kaddd&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;k1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;k2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;k3&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;kaddd&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;k1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;k2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;k3&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;kaddd&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;k1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;k2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;k3&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;mov&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;rdx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;QWORD&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;PTR&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[rdx]&lt;/span&gt;  &lt;span class=&quot;c&quot;&gt;; second cache miss load&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;lfence&lt;/span&gt;                      &lt;span class=&quot;c&quot;&gt;; stop issue until the above block completes&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;; this block is repeated 16 more times&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Each &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;kaddd&lt;/code&gt; instruction consumes one physical mask register. If number of filler instructions is equal to or less than the number of mask registers, we expect the misses to happen in parallel, otherwise the misses will be resolved serially. So we expect at that point to see a large spike in the running time.&lt;/p&gt;

&lt;p&gt;That’s exactly what we see:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/kreg/min/skx-27.svg&quot; alt=&quot;Test 27 kaddd instructions&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Let’s zoom in on the critical region, where the spike occurs:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/kreg/min/skx-27-zoomed.svg&quot; alt=&quot;Test 27 zoomed&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Here we clearly see that the transition isn’t &lt;em&gt;sharp&lt;/em&gt; – when the filler instruction count is between 130 and 134, we the runtime is intermediate: falling between the low and high levels. Henry calls this &lt;em&gt;non ideal&lt;/em&gt; behavior and I have seen it repeatedly across many but not all of these resource size tests. The idea is that the hardware implementation doesn’t always allow all of the resources to be used as you approach the limit&lt;sup id=&quot;fnref:nonideal&quot;&gt;&lt;a href=&quot;#fn:nonideal&quot; class=&quot;footnote&quot;&gt;8&lt;/a&gt;&lt;/sup&gt; - sometimes you get to use every last resource, but in other cases you may hit the limit a few filler instructions before the theoretical limit.&lt;/p&gt;

&lt;p&gt;Under this assumption, we want to look at the last (rightmost) point which is still faster than the slow performance level, since it indicates that &lt;em&gt;sometimes&lt;/em&gt; that many resources are available, implying that at least that many are physically present. Here, we see that final point occurs at 134 filler instructions.&lt;/p&gt;

&lt;p&gt;So we conclude that &lt;em&gt;&lt;abbr title=&quot;Intel's Skylake (server) architecture including Skylake-SP, Skylake-X and Skylake-W&quot;&gt;SKX&lt;/abbr&gt; has 134 physical registers available to hold speculative mask register values&lt;/em&gt;. As Henry indicates on the original post, it is likely that there are 8 physical registers dedicated to holding the non-speculative architectural state of the 8 mask registers, so our best guess at the total size of the mask register &lt;abbr title=&quot;Physical register file: The hardware registers used for renaming architectural (source visible) registers, usually much larger in number than the architectural register count.&quot;&gt;PRF&lt;/abbr&gt; is 142. That’s somewhat smaller than the &lt;abbr title=&quot;General purpose: as opposed to SIMD or FP. On x86 often refers to instructions such as integer addition, or registers such as eax.&quot;&gt;GP&lt;/abbr&gt; &lt;abbr title=&quot;Physical register file: The hardware registers used for renaming architectural (source visible) registers, usually much larger in number than the architectural register count.&quot;&gt;PRF&lt;/abbr&gt; (180 entires) or the &lt;abbr title=&quot;Single Instruction Multiple Data: an ISA type or ISA extension like Intel's AVX or ARM's NEON that can perform multiple identical operations on elements packed into a SIMD register.&quot;&gt;SIMD&lt;/abbr&gt; &lt;abbr title=&quot;Physical register file: The hardware registers used for renaming architectural (source visible) registers, usually much larger in number than the architectural register count.&quot;&gt;PRF&lt;/abbr&gt; (168 entries), but still quite large (see &lt;a href=&quot;//blog/2019/06/11/speed-limits.html#ooo-table&quot;&gt;this table of out of order resource sizes&lt;/a&gt; for sizes on other platforms).&lt;/p&gt;

&lt;p&gt;In particular, it is definitely large enough that you aren’t likely to run into this limit in practical code: it’s hard to imagine non-contrived code where almost 60%&lt;sup id=&quot;fnref:twothirds&quot;&gt;&lt;a href=&quot;#fn:twothirds&quot; class=&quot;footnote&quot;&gt;9&lt;/a&gt;&lt;/sup&gt; of the instructions &lt;em&gt;write&lt;/em&gt;&lt;sup id=&quot;fnref:write&quot;&gt;&lt;a href=&quot;#fn:write&quot; class=&quot;footnote&quot;&gt;10&lt;/a&gt;&lt;/sup&gt; to mask registers, because that’s what you’d need to hit this limit.&lt;/p&gt;

&lt;h4 id=&quot;are-they-distinct-prfs&quot;&gt;Are They Distinct PRFs?&lt;/h4&gt;

&lt;p&gt;You may have noticed that so far I’m simply &lt;em&gt;assuming&lt;/em&gt; that the mask register &lt;abbr title=&quot;Physical register file: The hardware registers used for renaming architectural (source visible) registers, usually much larger in number than the architectural register count.&quot;&gt;PRF&lt;/abbr&gt; is distinct from the others. I think this is highly likely, given the way mask registers are used and since they are part of a disjoint renaming domain&lt;sup id=&quot;fnref:rename&quot;&gt;&lt;a href=&quot;#fn:rename&quot; class=&quot;footnote&quot;&gt;11&lt;/a&gt;&lt;/sup&gt;. It is also supported by the fact that that apparent mask register PFR size doesn’t match either the &lt;abbr title=&quot;General purpose: as opposed to SIMD or FP. On x86 often refers to instructions such as integer addition, or registers such as eax.&quot;&gt;GP&lt;/abbr&gt; or &lt;abbr title=&quot;Single Instruction Multiple Data: an ISA type or ISA extension like Intel's AVX or ARM's NEON that can perform multiple identical operations on elements packed into a SIMD register.&quot;&gt;SIMD&lt;/abbr&gt; &lt;abbr title=&quot;Physical register file: The hardware registers used for renaming architectural (source visible) registers, usually much larger in number than the architectural register count.&quot;&gt;PRF&lt;/abbr&gt; sizes, but we can go further and actually test it!&lt;/p&gt;

&lt;p&gt;To do that, we use a similar test to the above, but with the filler instructions alternating between the same &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;kaddd&lt;/code&gt; instruction as the original test and an instruction that uses either a &lt;abbr title=&quot;General purpose: as opposed to SIMD or FP. On x86 often refers to instructions such as integer addition, or registers such as eax.&quot;&gt;GP&lt;/abbr&gt; or &lt;abbr title=&quot;Single Instruction Multiple Data: an ISA type or ISA extension like Intel's AVX or ARM's NEON that can perform multiple identical operations on elements packed into a SIMD register.&quot;&gt;SIMD&lt;/abbr&gt; register. If the register file is shared, we expect to hit a limit at size of the &lt;abbr title=&quot;Physical register file: The hardware registers used for renaming architectural (source visible) registers, usually much larger in number than the architectural register count.&quot;&gt;PRF&lt;/abbr&gt;. If the PRFs are not shared, we expect that neither &lt;abbr title=&quot;Physical register file: The hardware registers used for renaming architectural (source visible) registers, usually much larger in number than the architectural register count.&quot;&gt;PRF&lt;/abbr&gt; limit will be hit, and we will instead hit a different limit such as the &lt;abbr title=&quot;Re-order buffer: n ordered buffer which stores in-progress instructions on an out-of-order processor.&quot;&gt;ROB&lt;/abbr&gt; size.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/travisdowns/robsize/blob/fb039f212f1364e2e65b8cb2a0c3f8023c85777f/asm-gold/asm-29.asm&quot;&gt;Test 29&lt;/a&gt; alternates &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;kaddd&lt;/code&gt; and scalar &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;add&lt;/code&gt; instructions, like this:&lt;/p&gt;

&lt;div class=&quot;language-nasm highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;mov&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;rcx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;QWORD&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;PTR&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[rcx]&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;add&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;ebx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ebx&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;kaddd&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;k1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;k2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;k3&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;add&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;esi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;esi&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;kaddd&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;k1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;k2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;k3&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;add&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;ebx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ebx&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;kaddd&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;k1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;k2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;k3&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;add&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;esi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;esi&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;kaddd&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;k1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;k2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;k3&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;add&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;ebx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ebx&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;kaddd&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;k1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;k2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;k3&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;add&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;esi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;esi&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;kaddd&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;k1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;k2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;k3&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;add&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;ebx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ebx&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;kaddd&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;k1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;k2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;k3&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;mov&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;rdx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;QWORD&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;PTR&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[rdx]&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;lfence&lt;/span&gt; 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Here’s the chart:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/kreg/min/skx-29.svg&quot; alt=&quot;Test 29: alternating kaddd and scalar add&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We see that the spike is at a filler count larger than the &lt;abbr title=&quot;General purpose: as opposed to SIMD or FP. On x86 often refers to instructions such as integer addition, or registers such as eax.&quot;&gt;GP&lt;/abbr&gt; and &lt;abbr title=&quot;Physical register file: The hardware registers used for renaming architectural (source visible) registers, usually much larger in number than the architectural register count.&quot;&gt;PRF&lt;/abbr&gt; sizes. So we can conclude that the mask and &lt;abbr title=&quot;General purpose: as opposed to SIMD or FP. On x86 often refers to instructions such as integer addition, or registers such as eax.&quot;&gt;GP&lt;/abbr&gt; PRFs are not shared.&lt;/p&gt;

&lt;p&gt;Maybe the mask register is shared with the &lt;abbr title=&quot;Single Instruction Multiple Data: an ISA type or ISA extension like Intel's AVX or ARM's NEON that can perform multiple identical operations on elements packed into a SIMD register.&quot;&gt;SIMD&lt;/abbr&gt; &lt;abbr title=&quot;Physical register file: The hardware registers used for renaming architectural (source visible) registers, usually much larger in number than the architectural register count.&quot;&gt;PRF&lt;/abbr&gt;? After all, mask registers are more closely associated with &lt;abbr title=&quot;Single Instruction Multiple Data: an ISA type or ISA extension like Intel's AVX or ARM's NEON that can perform multiple identical operations on elements packed into a SIMD register.&quot;&gt;SIMD&lt;/abbr&gt; instructions than general purpose ones, so maybe there is some synergy there.&lt;/p&gt;

&lt;p&gt;To check, here’s &lt;a href=&quot;https://github.com/travisdowns/robsize/blob/fb039f212f1364e2e65b8cb2a0c3f8023c85777f/asm-gold/asm-35.asm&quot;&gt;Test 35&lt;/a&gt;, which is similar to 29 except that it alternates between &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;kaddd&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;vxorps&lt;/code&gt;, like so:&lt;/p&gt;

&lt;div class=&quot;language-nasm highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;mov&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;rcx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;QWORD&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;PTR&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[rcx]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;vxorps&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ymm0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ymm0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ymm1&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;kaddd&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;k1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;k2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;k3&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;vxorps&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ymm2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ymm2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ymm3&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;kaddd&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;k1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;k2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;k3&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;vxorps&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ymm4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ymm4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ymm5&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;kaddd&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;k1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;k2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;k3&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;vxorps&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ymm6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ymm6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ymm7&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;kaddd&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;k1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;k2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;k3&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;vxorps&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ymm0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ymm0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ymm1&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;kaddd&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;k1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;k2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;k3&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;vxorps&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ymm2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ymm2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ymm3&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;kaddd&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;k1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;k2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;k3&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;vxorps&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ymm4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ymm4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ymm5&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;kaddd&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;k1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;k2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;k3&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;mov&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;rdx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;QWORD&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;PTR&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[rdx]&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;lfence&lt;/span&gt; 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Here’s the corresponding chart:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/kreg/min/skx-35.svg&quot; alt=&quot;Test 35: alternating kaddd and SIMD xor&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The behavior is basically identical to the prior test, so we conclude that there is no direct sharing between the mask register and &lt;abbr title=&quot;Single Instruction Multiple Data: an ISA type or ISA extension like Intel's AVX or ARM's NEON that can perform multiple identical operations on elements packed into a SIMD register.&quot;&gt;SIMD&lt;/abbr&gt; PRFs either.&lt;/p&gt;

&lt;h4 id=&quot;an-unresolved-puzzle&quot;&gt;An Unresolved Puzzle&lt;/h4&gt;

&lt;p&gt;Something we notice in both of the above tests, however, is that the spike seems to finish around 212 filler instructions. However, the &lt;abbr title=&quot;Re-order buffer: n ordered buffer which stores in-progress instructions on an out-of-order processor.&quot;&gt;ROB&lt;/abbr&gt; size for this microarchtiecture is 224. Is this just &lt;em&gt;non ideal behavior&lt;/em&gt; as we saw earlier? Well we can test this by comparing against Test 4, which just uses &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;nop&lt;/code&gt; instructions as the filler: these shouldn’t consume almost any resources beyond &lt;abbr title=&quot;Re-order buffer: n ordered buffer which stores in-progress instructions on an out-of-order processor.&quot;&gt;ROB&lt;/abbr&gt; entries. Here’s Test 4 (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;nop&lt;/code&gt; filer) versus Test 29 (alternating &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;kaddd&lt;/code&gt; and scalar &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;add&lt;/code&gt;):&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/kreg/min/skx-4-29.svg&quot; alt=&quot;Test 4 vs 29&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;nop&lt;/code&gt;-using &lt;a href=&quot;https://github.com/travisdowns/robsize/blob/fb039f212f1364e2e65b8cb2a0c3f8023c85777f/asm-gold/asm-4.asm&quot;&gt;Test 4&lt;/a&gt; &lt;em&gt;nails&lt;/em&gt; the &lt;abbr title=&quot;Re-order buffer: n ordered buffer which stores in-progress instructions on an out-of-order processor.&quot;&gt;ROB&lt;/abbr&gt; size at exactly 224 (these charts are SVG so feel free to “View Image” and zoom in confirm). So it seems that we hit some other limit around 212 when we mix mask and &lt;abbr title=&quot;General purpose: as opposed to SIMD or FP. On x86 often refers to instructions such as integer addition, or registers such as eax.&quot;&gt;GP&lt;/abbr&gt; registers, or when we mix mask and &lt;abbr title=&quot;Single Instruction Multiple Data: an ISA type or ISA extension like Intel's AVX or ARM's NEON that can perform multiple identical operations on elements packed into a SIMD register.&quot;&gt;SIMD&lt;/abbr&gt; registers. In fact the same limit applies even between &lt;abbr title=&quot;General purpose: as opposed to SIMD or FP. On x86 often refers to instructions such as integer addition, or registers such as eax.&quot;&gt;GP&lt;/abbr&gt; and &lt;abbr title=&quot;Single Instruction Multiple Data: an ISA type or ISA extension like Intel's AVX or ARM's NEON that can perform multiple identical operations on elements packed into a SIMD register.&quot;&gt;SIMD&lt;/abbr&gt; registers, if we compare Test 4 and &lt;a href=&quot;https://github.com/travisdowns/robsize/blob/fb039f212f1364e2e65b8cb2a0c3f8023c85777f/asm-gold/asm-21.asm&quot;&gt;Test 21&lt;/a&gt; (which mixes &lt;abbr title=&quot;General purpose: as opposed to SIMD or FP. On x86 often refers to instructions such as integer addition, or registers such as eax.&quot;&gt;GP&lt;/abbr&gt; adds with &lt;abbr title=&quot;Single Instruction Multiple Data: an ISA type or ISA extension like Intel's AVX or ARM's NEON that can perform multiple identical operations on elements packed into a SIMD register.&quot;&gt;SIMD&lt;/abbr&gt; &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;vxorps&lt;/code&gt;):&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/kreg/min/skx-4-21.svg&quot; alt=&quot;Test 4 vs 21&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Henry mentions a more extreme version of the same thing in the original &lt;a href=&quot;http://blog.stuffedcow.net/2013/05/measuring-rob-capacity/&quot;&gt;blog entry&lt;/a&gt;, in the section also headed &lt;strong&gt;Unresolved Puzzle&lt;/strong&gt;:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Sandy Bridge AVX or SSE interleaved with integer instructions seems to be limited to looking ahead ~147 instructions by something other than the &lt;abbr title=&quot;Re-order buffer: n ordered buffer which stores in-progress instructions on an out-of-order processor.&quot;&gt;ROB&lt;/abbr&gt;. Having tried other combinations (e.g., varying the ordering and proportion of AVX vs. integer instructions, inserting some NOPs into the mix), it seems as though both SSE/AVX and integer instructions consume registers from some form of shared pool, as the instruction window is always limited to around 147 regardless of how many of each type of instruction are used, as long as neither type exhausts its own &lt;abbr title=&quot;Physical register file: The hardware registers used for renaming architectural (source visible) registers, usually much larger in number than the architectural register count.&quot;&gt;PRF&lt;/abbr&gt; supply on its own.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Read the full section for all the details. The effect is similar here but smaller: we at least get 95% of the way to the &lt;abbr title=&quot;Re-order buffer: n ordered buffer which stores in-progress instructions on an out-of-order processor.&quot;&gt;ROB&lt;/abbr&gt; size, but still stop before it.  It is possible the shared resource is related to register reclamation, e.g., the PRRT&lt;sup id=&quot;fnref:prrt&quot;&gt;&lt;a href=&quot;#fn:prrt&quot; class=&quot;footnote&quot;&gt;12&lt;/a&gt;&lt;/sup&gt; - a table which keeps track of which registers can be reclaimed when a given instruction retires.&lt;/p&gt;

&lt;p&gt;Finally, we finish this party off with a few miscellaneous notes on mask registers, checking for parity with some features available to &lt;abbr title=&quot;General purpose: as opposed to SIMD or FP. On x86 often refers to instructions such as integer addition, or registers such as eax.&quot;&gt;GP&lt;/abbr&gt; and &lt;abbr title=&quot;Single Instruction Multiple Data: an ISA type or ISA extension like Intel's AVX or ARM's NEON that can perform multiple identical operations on elements packed into a SIMD register.&quot;&gt;SIMD&lt;/abbr&gt; registers.&lt;/p&gt;

&lt;h3 id=&quot;move-elimination&quot;&gt;Move Elimination&lt;/h3&gt;

&lt;p&gt;Both &lt;abbr title=&quot;General purpose: as opposed to SIMD or FP. On x86 often refers to instructions such as integer addition, or registers such as eax.&quot;&gt;GP&lt;/abbr&gt; and &lt;abbr title=&quot;Single Instruction Multiple Data: an ISA type or ISA extension like Intel's AVX or ARM's NEON that can perform multiple identical operations on elements packed into a SIMD register.&quot;&gt;SIMD&lt;/abbr&gt; registers are eligible for so-called &lt;em&gt;move elimination&lt;/em&gt;. This means that a register to register move like &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mov eax, edx&lt;/code&gt; or &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;vmovdqu ymm1, ymm2&lt;/code&gt; can be eliminated at rename by “simply”&lt;sup id=&quot;fnref:simply&quot;&gt;&lt;a href=&quot;#fn:simply&quot; class=&quot;footnote&quot;&gt;13&lt;/a&gt;&lt;/sup&gt; pointing the destination register entry in the &lt;abbr title=&quot;Register alias table: a table which maps an architectural register identifier to a physical register.&quot;&gt;RAT&lt;/abbr&gt; to the same physical register as the source, without involving the ALU.&lt;/p&gt;

&lt;p&gt;Let’s check if something like &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;kmov k1, k2&lt;/code&gt; also qualifies for move elimination. First, we check the chart for &lt;a href=&quot;https://github.com/travisdowns/robsize/blob/fb039f212f1364e2e65b8cb2a0c3f8023c85777f/asm-gold/asm-28.asm&quot;&gt;Test 28&lt;/a&gt;, where the filler instruction is &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;kmovd k1, k2&lt;/code&gt;:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/kreg/min/skx-28.svg&quot; alt=&quot;Test 28&quot; /&gt;&lt;/p&gt;

&lt;p&gt;It looks exactly like Test 27 we saw earlier with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;kaddd&lt;/code&gt;. So we would suspect that physical registers are being consumed, unless we have happened to hit a different move-elimination related limit with exactly the same size and limiting behavior&lt;sup id=&quot;fnref:moves&quot;&gt;&lt;a href=&quot;#fn:moves&quot; class=&quot;footnote&quot;&gt;14&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;p&gt;Additional confirmation comes from uops.info which &lt;a href=&quot;https://uops.info/table.html?search=kmov%20(K%2C%20K)&amp;amp;cb_lat=on&amp;amp;cb_tp=on&amp;amp;cb_uops=on&amp;amp;cb_ports=on&amp;amp;cb_SKX=on&amp;amp;cb_measurements=on&amp;amp;cb_avx512=on&quot;&gt;shows that&lt;/a&gt; all variants of mask to mask register &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;kmov&lt;/code&gt; take one &lt;abbr title=&quot;Micro-operation: instructions are translated into one or more uops, which are simple operations executed by the CPU's execution units.&quot;&gt;uop&lt;/abbr&gt; dispatched to &lt;abbr title=&quot;port 0 (GP and SIMD ALU, not-taken branches)&quot;&gt;p0&lt;/abbr&gt;. If the move is eliminated, we wouldn’t see any dispatched uops.&lt;/p&gt;

&lt;p&gt;Therefore I conclude that register to register&lt;sup id=&quot;fnref:regreg&quot;&gt;&lt;a href=&quot;#fn:regreg&quot; class=&quot;footnote&quot;&gt;15&lt;/a&gt;&lt;/sup&gt; moves involving mask registers are not eliminated.&lt;/p&gt;

&lt;h3 id=&quot;dependency-breaking-idioms&quot;&gt;Dependency Breaking Idioms&lt;/h3&gt;

&lt;p&gt;The &lt;a href=&quot;https://stackoverflow.com/a/33668295/149138&quot;&gt;best way&lt;/a&gt; to set a &lt;abbr title=&quot;General purpose: as opposed to SIMD or FP. On x86 often refers to instructions such as integer addition, or registers such as eax.&quot;&gt;GP&lt;/abbr&gt; register to zero in x86 is via the xor zeroing idiom: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;xor reg, reg&lt;/code&gt;. This works because any value xored with itself is zero. This is smaller (fewer instruction bytes) than the more obvious &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mov eax, 0&lt;/code&gt;, and also faster since the processor recognizes it as a &lt;em&gt;zeroing idiom&lt;/em&gt; and performs the necessary work at rename&lt;sup id=&quot;fnref:zero&quot;&gt;&lt;a href=&quot;#fn:zero&quot; class=&quot;footnote&quot;&gt;16&lt;/a&gt;&lt;/sup&gt;, so no ALU is involved and no &lt;abbr title=&quot;Micro-operation: instructions are translated into one or more uops, which are simple operations executed by the CPU's execution units.&quot;&gt;uop&lt;/abbr&gt; is dispatched.&lt;/p&gt;

&lt;p&gt;Furthermore, the idiom is &lt;em&gt;dependency breaking:&lt;/em&gt; although &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;xor reg1, reg2&lt;/code&gt; in general depends on the value of both &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;reg1&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;reg2&lt;/code&gt;, in the special case that &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;reg1&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;reg2&lt;/code&gt; are the same, there is no dependency as the result is zero regardless of the inputs. All modern x86 CPUs recognize this&lt;sup id=&quot;fnref:otherzero&quot;&gt;&lt;a href=&quot;#fn:otherzero&quot; class=&quot;footnote&quot;&gt;17&lt;/a&gt;&lt;/sup&gt; special case for &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;xor&lt;/code&gt;. The same applies to &lt;abbr title=&quot;Single Instruction Multiple Data: an ISA type or ISA extension like Intel's AVX or ARM's NEON that can perform multiple identical operations on elements packed into a SIMD register.&quot;&gt;SIMD&lt;/abbr&gt; versions of xor such as integer &lt;a href=&quot;https://www.felixcloutier.com/x86/pxor&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;vpxor&lt;/code&gt;&lt;/a&gt; and floating point &lt;a href=&quot;https://www.felixcloutier.com/x86/xorps&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;vxorps&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;https://www.felixcloutier.com/x86/xorpd&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;vxorpd&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;That background out of the way, a curious person might wonder if the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;kxor&lt;/code&gt; &lt;a href=&quot;https://www.felixcloutier.com/x86/kxorw:kxorb:kxorq:kxord&quot;&gt;variants&lt;/a&gt; are treated the same way. Is &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;kxorb k1, k1, k1&lt;/code&gt;&lt;sup id=&quot;fnref:notall&quot;&gt;&lt;a href=&quot;#fn:notall&quot; class=&quot;footnote&quot;&gt;18&lt;/a&gt;&lt;/sup&gt; treated as a zeroing idiom?&lt;/p&gt;

&lt;p&gt;This is actually two separate questions, since there are two aspects to zeroing idioms:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Zero latency execution with no execution unit (elimination)&lt;/li&gt;
  &lt;li&gt;Dependency breaking&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Let’s look at each in turn.&lt;/p&gt;

&lt;h4 id=&quot;execution-elimination&quot;&gt;Execution Elimination&lt;/h4&gt;

&lt;p&gt;So are zeroing xors like &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;kxorb k1, k1, k1&lt;/code&gt; executed at rename without latency and without needing an execution unit?&lt;/p&gt;

&lt;p&gt;No.&lt;/p&gt;

&lt;p&gt;Here, I don’t even have to do any work: uops.info has our back because they’ve performed &lt;a href=&quot;https://uops.info/html-tp/SKX/KXORD_K_K_K-Measurements.html#sameReg&quot;&gt;this exact test&lt;/a&gt; and report a latency of 1 cycle and one &lt;abbr title=&quot;port 0 (GP and SIMD ALU, not-taken branches)&quot;&gt;p0&lt;/abbr&gt; &lt;abbr title=&quot;Micro-operation: instructions are translated into one or more uops, which are simple operations executed by the CPU's execution units.&quot;&gt;uop&lt;/abbr&gt; used. So we can conclude that zeroing xors of mask registers are not eliminated.&lt;/p&gt;

&lt;h4 id=&quot;dependency-breaking&quot;&gt;Dependency Breaking&lt;/h4&gt;

&lt;p&gt;Well maybe zeroing kxors are dependency breaking, even though they require an execution unit?&lt;/p&gt;

&lt;p&gt;In this case, we can’t simply check uops.info. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;kxor&lt;/code&gt; is a one cycle latency instruction that runs only on a single execution port (&lt;abbr title=&quot;port 0 (GP and SIMD ALU, not-taken branches)&quot;&gt;p0&lt;/abbr&gt;), so we hit the interesting (?) case where a chain of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;kxor&lt;/code&gt; runs at the same speed regardless of whether the are dependent or independent: the throughput bottleneck of 1/cycle is the same as the latency bottleneck of 1/cycle!&lt;/p&gt;

&lt;p&gt;Don’t worry, we’ve got other tricks up our sleeve. We can test this by constructing a tests which involve a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;kxor&lt;/code&gt; in a carried dependency chain with enough total latency so that the chain latency is the bottleneck. If the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;kxor&lt;/code&gt; carries a dependency, the runtime will be equal to the sum of the latencies in the chain. If the instruction is dependency breaking, the chain is broken and the different disconnected chains can overlap and performance will likely be limited by some throughput restriction (e.g., &lt;a href=&quot;//blog/2019/06/11/speed-limits.html#portexecution-unit-limits&quot;&gt;port contention&lt;/a&gt;). This could use a good diagram, but I’m not good at diagrams.&lt;/p&gt;

&lt;p&gt;All the tests are in &lt;a href=&quot;https://github.com/travisdowns/uarch-bench/blob/ccbebbec39ab02d6460a1837857d052e120c0946/x86_avx512.asm#L20&quot;&gt;&lt;abbr title=&quot;Microarchitecture: a specific implementation of an ISA, e.g., &amp;quot;Haswell microarchitecture&amp;quot;.&quot;&gt;uarch&lt;/abbr&gt; bench&lt;/a&gt;, but I’ll show the key parts here.&lt;/p&gt;

&lt;p&gt;First we get a baseline measurement for the latency of moving from a mask register to a &lt;abbr title=&quot;General purpose: as opposed to SIMD or FP. On x86 often refers to instructions such as integer addition, or registers such as eax.&quot;&gt;GP&lt;/abbr&gt; register and back:&lt;/p&gt;

&lt;div class=&quot;language-nasm highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;kmovb&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;k0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;eax&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;kmovb&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;eax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;k0&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;; repeated 127 more times&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This pair clocks in&lt;sup id=&quot;fnref:runit&quot;&gt;&lt;a href=&quot;#fn:runit&quot; class=&quot;footnote&quot;&gt;19&lt;/a&gt;&lt;/sup&gt; at 4 cycles. It’s hard to know how to partition the latency between the two instructions: are they both 2 cycles or is there a 3-1 split one way or the other&lt;sup id=&quot;fnref:fyiuops&quot;&gt;&lt;a href=&quot;#fn:fyiuops&quot; class=&quot;footnote&quot;&gt;20&lt;/a&gt;&lt;/sup&gt;, but for our purposes it doesn’t matter because we just care about the latency of the round-trip. Importantly, the post-based throughput limit of this sequence is 1/cycle, 4x faster than the latency limit, because each instruction goes to a different port (&lt;abbr title=&quot;port 5 (GP and SIMD ALU, vector shuffles)&quot;&gt;p5&lt;/abbr&gt; and &lt;abbr title=&quot;port 0 (GP and SIMD ALU, not-taken branches)&quot;&gt;p0&lt;/abbr&gt;, respectively). This means we will be able to tease out latency effects independent of throughput.&lt;/p&gt;

&lt;p&gt;Next, we throw a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;kxor&lt;/code&gt; into the chain that we know is &lt;em&gt;not&lt;/em&gt; zeroing:&lt;/p&gt;

&lt;div class=&quot;language-nasm highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;kmovb&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;k0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;eax&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;kxorb&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;k0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;k0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;k1&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;kmovb&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;eax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;k0&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;; repeated 127 more times&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Since &lt;a href=&quot;https://uops.info/table.html?search=kxorb&amp;amp;cb_lat=on&amp;amp;cb_tp=on&amp;amp;cb_uops=on&amp;amp;cb_ports=on&amp;amp;cb_SKX=on&amp;amp;cb_measurements=on&amp;amp;cb_avx512=on&quot;&gt;we know&lt;/a&gt; &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;kxorb&lt;/code&gt; has 1 cycle of latency, we expect to increase the latency to 5 cycles and that’s exactly what we measure (the first two tests shown):&lt;/p&gt;

&lt;pre&gt;
** Running group avx512 : AVX512 stuff **
                               Benchmark    Cycles     Nanos
                kreg-&lt;abbr title=&quot;General purpose: as opposed to SIMD or FP. On x86 often refers to instructions such as integer addition, or registers such as eax.&quot;&gt;GP&lt;/abbr&gt; rountrip latency      4.00      1.25
    kreg-&lt;abbr title=&quot;General purpose: as opposed to SIMD or FP. On x86 often refers to instructions such as integer addition, or registers such as eax.&quot;&gt;GP&lt;/abbr&gt; roundtrip + nonzeroing kxorb      5.00      1.57
&lt;/pre&gt;

&lt;p&gt;Finally, the key test:&lt;/p&gt;

&lt;div class=&quot;language-nasm highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;kmovb&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;k0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;eax&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;kxorb&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;k0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;k0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;k0&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;kmovb&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;eax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;k0&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;; repeated 127 more times&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This has a zeroing &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;kxorb k0, k0, k0&lt;/code&gt;. If it breaks the dependency on k0, it would mean that the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;kmovb eax, k0&lt;/code&gt; no longer depends on the earlier &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;kmovb k0, eax&lt;/code&gt;, and the carried chain is broken and we’d see a lower cycle time.&lt;/p&gt;

&lt;p&gt;Drumroll…&lt;/p&gt;

&lt;p&gt;We measure this at the exact same 5.0 cycles as the prior example:&lt;/p&gt;

&lt;pre&gt;
** Running group avx512 : AVX512 stuff **
                               Benchmark    Cycles     Nanos
                kreg-&lt;abbr title=&quot;General purpose: as opposed to SIMD or FP. On x86 often refers to instructions such as integer addition, or registers such as eax.&quot;&gt;GP&lt;/abbr&gt; rountrip latency      4.00      1.25
    kreg-&lt;abbr title=&quot;General purpose: as opposed to SIMD or FP. On x86 often refers to instructions such as integer addition, or registers such as eax.&quot;&gt;GP&lt;/abbr&gt; roundtrip + nonzeroing kxorb      5.00      1.57
&lt;span style=&quot;background: green;&quot;&gt;       kreg-&lt;abbr title=&quot;General purpose: as opposed to SIMD or FP. On x86 often refers to instructions such as integer addition, or registers such as eax.&quot;&gt;GP&lt;/abbr&gt; roundtrip + zeroing kxorb      5.00      1.57&lt;/span&gt;
&lt;/pre&gt;

&lt;p&gt;So we tentatively conclude that zeroing idioms aren’t recognized at all when they involve mask registers.&lt;/p&gt;

&lt;p&gt;Finally, as a check on our logic, we use the following test which replaces the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;kxor&lt;/code&gt; with a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;kmov&lt;/code&gt; which we know is &lt;em&gt;always&lt;/em&gt; dependency breaking:&lt;/p&gt;

&lt;div class=&quot;language-nasm highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;kmovb&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;k0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;eax&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;kmovb&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;k0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ecx&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;kmovb&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;eax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;k0&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;; repeated 127 more times&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This is the final result shown in the output above, and it runs much more quickly at 2 cycles, bottlenecked on &lt;abbr title=&quot;port 5 (GP and SIMD ALU, vector shuffles)&quot;&gt;p5&lt;/abbr&gt; (the two &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;kmov k, r32&lt;/code&gt; instructions both go only to &lt;abbr title=&quot;port 5 (GP and SIMD ALU, vector shuffles)&quot;&gt;p5&lt;/abbr&gt;):&lt;/p&gt;

&lt;pre&gt;
** Running group avx512 : AVX512 stuff **
                               Benchmark    Cycles     Nanos
                kreg-&lt;abbr title=&quot;General purpose: as opposed to SIMD or FP. On x86 often refers to instructions such as integer addition, or registers such as eax.&quot;&gt;GP&lt;/abbr&gt; rountrip latency      4.00      1.25
    kreg-&lt;abbr title=&quot;General purpose: as opposed to SIMD or FP. On x86 often refers to instructions such as integer addition, or registers such as eax.&quot;&gt;GP&lt;/abbr&gt; roundtrip + nonzeroing kxorb      5.00      1.57
       kreg-&lt;abbr title=&quot;General purpose: as opposed to SIMD or FP. On x86 often refers to instructions such as integer addition, or registers such as eax.&quot;&gt;GP&lt;/abbr&gt; roundtrip + zeroing kxorb      5.00      1.57
&lt;span style=&quot;background: green;&quot;&gt;         kreg-&lt;abbr title=&quot;General purpose: as opposed to SIMD or FP. On x86 often refers to instructions such as integer addition, or registers such as eax.&quot;&gt;GP&lt;/abbr&gt; roundtrip + mov from &lt;abbr title=&quot;General purpose: as opposed to SIMD or FP. On x86 often refers to instructions such as integer addition, or registers such as eax.&quot;&gt;GP&lt;/abbr&gt;      2.00      0.63&lt;/span&gt;
&lt;/pre&gt;

&lt;p&gt;So our experiment seems to check out.&lt;/p&gt;

&lt;h3 id=&quot;reproduction&quot;&gt;Reproduction&lt;/h3&gt;

&lt;p&gt;You can reproduce these results yourself with the &lt;a href=&quot;https://github.com/travisdowns/robsize&quot;&gt;robsize&lt;/a&gt; binary on Linux or Windows (using WSL). The specific results for this article are &lt;a href=&quot;https://github.com/travisdowns/robsize/tree/master/scripts/kreg/results&quot;&gt;also available&lt;/a&gt; as are the &lt;a href=&quot;https://github.com/travisdowns/robsize/tree/master/scripts/kreg/&quot;&gt;scripts&lt;/a&gt; used to collect them and generate the plots.&lt;/p&gt;

&lt;h3 id=&quot;summary&quot;&gt;Summary&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;abbr title=&quot;Intel's Skylake (server) architecture including Skylake-SP, Skylake-X and Skylake-W&quot;&gt;SKX&lt;/abbr&gt; has a separate &lt;abbr title=&quot;Physical register file: The hardware registers used for renaming architectural (source visible) registers, usually much larger in number than the architectural register count.&quot;&gt;PRF&lt;/abbr&gt; for mask registers with a speculative size of 134 and an estimated total size of 142&lt;/li&gt;
  &lt;li&gt;This is large enough compared to the other &lt;abbr title=&quot;Physical register file: The hardware registers used for renaming architectural (source visible) registers, usually much larger in number than the architectural register count.&quot;&gt;PRF&lt;/abbr&gt; size and the &lt;abbr title=&quot;Re-order buffer: n ordered buffer which stores in-progress instructions on an out-of-order processor.&quot;&gt;ROB&lt;/abbr&gt; to make it unlikely to be a bottleneck&lt;/li&gt;
  &lt;li&gt;Mask registers are not eligible for move elimination&lt;/li&gt;
  &lt;li&gt;Zeroing idioms&lt;sup id=&quot;fnref:tech&quot;&gt;&lt;a href=&quot;#fn:tech&quot; class=&quot;footnote&quot;&gt;21&lt;/a&gt;&lt;/sup&gt; in mask registers are not recognized for execution elimination or dependency breaking&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;comments&quot;&gt;Comments&lt;/h3&gt;

&lt;p&gt;Discussion on &lt;a href=&quot;https://news.ycombinator.com/item?id=21714390&quot;&gt;Hacker News&lt;/a&gt;, Reddit (&lt;a href=&quot;https://www.reddit.com/r/asm/comments/e6kokb/x86_avx512_a_note_on_mask_registers/&quot;&gt;r/asm&lt;/a&gt; and &lt;a href=&quot;https://www.reddit.com/r/programming/comments/e6ko7i/a_note_on_mask_registers_avx512/&quot;&gt;r/programming&lt;/a&gt;) or &lt;a href=&quot;https://twitter.com/trav_downs/status/1202637229606264833&quot;&gt;Twitter&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Direct feedback also welcomed by &lt;a href=&quot;mailto:travis.downs@gmail.com&quot;&gt;email&lt;/a&gt; or as &lt;a href=&quot;https://github.com/travisdowns/travisdowns.github.io/issues&quot;&gt;a GitHub issue&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;thanks&quot;&gt;Thanks&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;https://lemire.me&quot;&gt;Daniel Lemire&lt;/a&gt; who provided access to the AVX-512 system I used for testing.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://www.stuffedcow.net/&quot;&gt;Henry Wong&lt;/a&gt; who wrote the &lt;a href=&quot;http://blog.stuffedcow.net/2013/05/measuring-rob-capacity/&quot;&gt;original article&lt;/a&gt; which introduced me to this technique and graciously shared the code for his tool, which I now &lt;a href=&quot;https://github.com/travisdowns/robsize&quot;&gt;host on github&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://twitter.com/Jeffinatorator/status/1202642436406669314&quot;&gt;Jeff Baker&lt;/a&gt;, &lt;a href=&quot;http://0x80.pl&quot;&gt;Wojciech Muła&lt;/a&gt; for reporting typos.&lt;/p&gt;

&lt;p&gt;Image credit: &lt;a href=&quot;https://www.flickr.com/photos/like_the_grand_canyon/31064064387&quot;&gt;Kellogg’s Special K&lt;/a&gt; by &lt;a href=&quot;https://www.flickr.com/photos/like_the_grand_canyon/&quot;&gt;Like_the_Grand_Canyon&lt;/a&gt; is licensed under &lt;a href=&quot;https://creativecommons.org/licenses/by/2.0/&quot;&gt;CC BY 2.0&lt;/a&gt;.&lt;/p&gt;

&lt;p class=&quot;info&quot;&gt;If you liked this post, check out the &lt;a href=&quot;/&quot;&gt;homepage&lt;/a&gt; for others you might enjoy&lt;sup id=&quot;fnref:noenjoy&quot;&gt;&lt;a href=&quot;#fn:noenjoy&quot; class=&quot;footnote&quot;&gt;22&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;hr /&gt;
&lt;hr /&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;div class=&quot;footnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:naming&quot;&gt;
      &lt;p&gt;These &lt;em&gt;mask registers&lt;/em&gt; are often called &lt;em&gt;k&lt;/em&gt; registers or simply &lt;em&gt;kregs&lt;/em&gt; based on their naming scheme. &lt;a href=&quot;https://twitter.com/tom_forsyth/status/1202666300591337472&quot;&gt;Rumor has it&lt;/a&gt; that this letter was chosen randomly only after a long and bloody naming battle between  MFs. &lt;a href=&quot;#fnref:naming&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:k0note&quot;&gt;
      &lt;p&gt;There is sometimes a misconception (until recently even on the AVX-512 wikipedia article) that &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;k0&lt;/code&gt; is not a normal mask register, but just a hardcoded indicator that no masking should be used. That’s not true: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;k0&lt;/code&gt; is a valid mask register and you can read and write to it with the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;k&lt;/code&gt;-prefixed instructions and &lt;abbr title=&quot;Single Instruction Multiple Data: an ISA type or ISA extension like Intel's AVX or ARM's NEON that can perform multiple identical operations on elements packed into a SIMD register.&quot;&gt;SIMD&lt;/abbr&gt; instructions that write mask registers (e.g., any AVX-512 &lt;a href=&quot;https://www.felixcloutier.com/x86/pcmpeqb:pcmpeqw:pcmpeqd&quot;&gt;comparison&lt;/a&gt;. However, the encoding that would normally be used for &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;k0&lt;/code&gt; as a writemask register in a &lt;abbr title=&quot;Single Instruction Multiple Data: an ISA type or ISA extension like Intel's AVX or ARM's NEON that can perform multiple identical operations on elements packed into a SIMD register.&quot;&gt;SIMD&lt;/abbr&gt; operation indicates instead “no masking”, so the contents of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;k0&lt;/code&gt; cannot be used for that purpose. &lt;a href=&quot;#fnref:k0note&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:maskmerge&quot;&gt;
      &lt;p&gt;The distinction being that a zero-masking operation results in zeroed destination elements at positions not selected by the mask, while merging leaves the existing elements in the destination register unchanged at those positions. As as side-effect this means that with merging, the destination register becomes a type of destructive source-destination register and there is an input dependency on this register. &lt;a href=&quot;#fnref:maskmerge&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:kreg&quot;&gt;
      &lt;p&gt;I’ll try to use the full term &lt;em&gt;mask register&lt;/em&gt; here, but I may also use &lt;em&gt;kreg&lt;/em&gt; a common nickname based on the labels &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;k0&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;k1&lt;/code&gt;, etc. So just mentally swap &lt;em&gt;kreg&lt;/em&gt; for &lt;em&gt;mask register&lt;/em&gt; if and when you see it (or vice-versa). &lt;a href=&quot;#fnref:kreg&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:hcite&quot;&gt;
      &lt;p&gt;H. Wong, &lt;em&gt;Measuring Reorder Buffer Capacity&lt;/em&gt;, May, 2013. [Online]. Available: http://blog.stuffedcow.net/2013/05/measuring-rob-capacity/ &lt;a href=&quot;#fnref:hcite&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:misstime&quot;&gt;
      &lt;p&gt;Generally taking 100 to 300 cycles each (latency-wise). The wide range is because the cache miss wall clock time varies by a factor of about 2x, generally between 50 and 100 naneseconds, depending on platform and &lt;abbr title=&quot;Microarchitecture: a specific implementation of an ISA, e.g., &amp;quot;Haswell microarchitecture&amp;quot;.&quot;&gt;uarch&lt;/abbr&gt; details, and the CPU frequency varies by a factor of about 2.5x (say from 2 GHz to 5 GHz). However, on a given host, with equivalent TLB miss/hit behavior, we expect the time to be roughly constant. &lt;a href=&quot;#fnref:misstime&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:roughly&quot;&gt;
      &lt;p&gt;The reason I have to add &lt;em&gt;roughly&lt;/em&gt; as a weasel word here is itself interesting. A glance at the charts shows that they are certainly not totally flat in either the fast or slow regions surrounding the spike. Rather there are various noticeable regions with distinct behavior and other artifacts: e.g., in Test 29 a very flat region up to about 104 filler instructions, followed by a bump and then a linearly ramping region up to the spike somewhat after 200 instructions. Some of those features are explicable by mentally (or &lt;a href=&quot;https://godbolt.org/z/eAGxhH&quot;&gt;actually&lt;/a&gt;) simulating the pipeline, which reveals that at some point the filler instructions will contribute (although only a cycle or so) to the runtime, but some features are still unexplained (for now). &lt;a href=&quot;#fnref:roughly&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:nonideal&quot;&gt;
      &lt;p&gt;For example, a given rename slot may only be able to write a subset of all the &lt;abbr title=&quot;Register alias table: a table which maps an architectural register identifier to a physical register.&quot;&gt;RAT&lt;/abbr&gt; entries, and uses the first available. When the &lt;abbr title=&quot;Register alias table: a table which maps an architectural register identifier to a physical register.&quot;&gt;RAT&lt;/abbr&gt; is almost full, it is possible that none of the allowed entries are empty, so it is as if the structure is full even though some free entries remain, but accessible only to other uops. Since the allowed entries may be essentially random across iterations, this ends up with a more-or-less linear ramp between the low and high performance levels in the non-ideal region. &lt;a href=&quot;#fnref:nonideal&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:twothirds&quot;&gt;
      &lt;p&gt;The “60 percent” comes from 134 / 224, i.e., the speculative mask register &lt;abbr title=&quot;Physical register file: The hardware registers used for renaming architectural (source visible) registers, usually much larger in number than the architectural register count.&quot;&gt;PRF&lt;/abbr&gt; size, divided by the &lt;abbr title=&quot;Re-order buffer: n ordered buffer which stores in-progress instructions on an out-of-order processor.&quot;&gt;ROB&lt;/abbr&gt; size. The idea is that if you’ll hit the &lt;abbr title=&quot;Re-order buffer: n ordered buffer which stores in-progress instructions on an out-of-order processor.&quot;&gt;ROB&lt;/abbr&gt; size limit no matter what once you have 224 instructions in flight, so you’d need to have 60% of those instructions be mask register writes&lt;sup id=&quot;fnref:write:1&quot;&gt;&lt;a href=&quot;#fn:write&quot; class=&quot;footnote&quot;&gt;10&lt;/a&gt;&lt;/sup&gt; in order to hit the 134 limit first. Of course, you might also hit some &lt;em&gt;other&lt;/em&gt; limit first, so even 60% might not be enough, but the &lt;abbr title=&quot;Re-order buffer: n ordered buffer which stores in-progress instructions on an out-of-order processor.&quot;&gt;ROB&lt;/abbr&gt; size puts a lower bound on this figure since it &lt;em&gt;always&lt;/em&gt; applies. &lt;a href=&quot;#fnref:twothirds&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:write&quot;&gt;
      &lt;p&gt;Importantly, only instructions which write a mask register consume a physical register. Instructions that simply read a mask register (e.g,. &lt;abbr title=&quot;Single Instruction Multiple Data: an ISA type or ISA extension like Intel's AVX or ARM's NEON that can perform multiple identical operations on elements packed into a SIMD register.&quot;&gt;SIMD&lt;/abbr&gt; instructions using a writemask) do not consume a new physical mask register. &lt;a href=&quot;#fnref:write&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt; &lt;a href=&quot;#fnref:write:1&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:rename&quot;&gt;
      &lt;p&gt;More renaming domains makes things easier on the renamer for a given number of input registers. That is, it is easier to rename 2 &lt;abbr title=&quot;General purpose: as opposed to SIMD or FP. On x86 often refers to instructions such as integer addition, or registers such as eax.&quot;&gt;GP&lt;/abbr&gt; and 2 &lt;abbr title=&quot;Single Instruction Multiple Data: an ISA type or ISA extension like Intel's AVX or ARM's NEON that can perform multiple identical operations on elements packed into a SIMD register.&quot;&gt;SIMD&lt;/abbr&gt; input registers (separate domains) than 4 &lt;abbr title=&quot;General purpose: as opposed to SIMD or FP. On x86 often refers to instructions such as integer addition, or registers such as eax.&quot;&gt;GP&lt;/abbr&gt; registers. &lt;a href=&quot;#fnref:rename&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:prrt&quot;&gt;
      &lt;p&gt;This is either the &lt;em&gt;Physical Register Reclaim Table&lt;/em&gt; or &lt;em&gt;Post Retirement Reclaim Table&lt;/em&gt; depending on who you ask. &lt;a href=&quot;#fnref:prrt&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:simply&quot;&gt;
      &lt;p&gt;Of course, it is not actually so simple. For one, you now need to track these “move elimination sets” (sets of registers all pointing to the same physical register) in order to know when the physical register can be released (once the set is empty), and these sets are themselves a limited resource which must be tracked. Flags introduce another complication since flags are apparently stored along with the destination register, so the presence and liveness of the flags must be tracked as well. &lt;a href=&quot;#fnref:simply&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:moves&quot;&gt;
      &lt;p&gt;In particular, in the corresponding test for &lt;abbr title=&quot;General purpose: as opposed to SIMD or FP. On x86 often refers to instructions such as integer addition, or registers such as eax.&quot;&gt;GP&lt;/abbr&gt; registers (Test 7), the chart looks very different as move elimination reduce the &lt;abbr title=&quot;Physical register file: The hardware registers used for renaming architectural (source visible) registers, usually much larger in number than the architectural register count.&quot;&gt;PRF&lt;/abbr&gt; demand down to almost zero and we get to the &lt;abbr title=&quot;Re-order buffer: n ordered buffer which stores in-progress instructions on an out-of-order processor.&quot;&gt;ROB&lt;/abbr&gt; limit. &lt;a href=&quot;#fnref:moves&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:regreg&quot;&gt;
      &lt;p&gt;Note that I am not restricting my statement to moves between two mask registers only, but any registers. That is, moves between a &lt;abbr title=&quot;General purpose: as opposed to SIMD or FP. On x86 often refers to instructions such as integer addition, or registers such as eax.&quot;&gt;GP&lt;/abbr&gt; registers and a mask registers are also not eliminated (the latter fact is obvious if consider than they use distinct register files, so move elimination seems impossible). &lt;a href=&quot;#fnref:regreg&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:zero&quot;&gt;
      &lt;p&gt;Probably by pointing the entry in the &lt;abbr title=&quot;Register alias table: a table which maps an architectural register identifier to a physical register.&quot;&gt;RAT&lt;/abbr&gt; to a fixed, shared zero register, or setting a flag in the &lt;abbr title=&quot;Register alias table: a table which maps an architectural register identifier to a physical register.&quot;&gt;RAT&lt;/abbr&gt; that indicates it is zero. &lt;a href=&quot;#fnref:zero&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:otherzero&quot;&gt;
      &lt;p&gt;Although &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;xor&lt;/code&gt; is the most reliable, other idioms may be recognized as zeroing or dependency breaking idioms by some CPUs as well, e.g., &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sub reg,reg&lt;/code&gt; and even &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sbb reg, reg&lt;/code&gt; which is not a zeroing idiom, but rather sets the value of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;reg&lt;/code&gt; to zero or -1 (all bits set) depending on the value of the carry flag. This doesn’t depend on the value of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;reg&lt;/code&gt; but only the carry flag, and some CPUs recognize that and break the dependency. Agner’s &lt;a href=&quot;https://www.agner.org/optimize/#manual_microarch&quot;&gt;microarchitecture guide&lt;/a&gt; covers the &lt;abbr title=&quot;Microarchitecture: a specific implementation of an ISA, e.g., &amp;quot;Haswell microarchitecture&amp;quot;.&quot;&gt;uarch&lt;/abbr&gt;-dependent support for these idioms very well. &lt;a href=&quot;#fnref:otherzero&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:notall&quot;&gt;
      &lt;p&gt;Note that only the two source registers really need to be the same: if &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;kxorb k1, k1, k1&lt;/code&gt; is treated as zeroing, I would expect the same for &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;kxorb k1, k2, k2&lt;/code&gt;. &lt;a href=&quot;#fnref:notall&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:runit&quot;&gt;
      &lt;p&gt;Run all the tests in this section using &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;./uarch-bench.sh --test-name=avx512/*&lt;/code&gt;. &lt;a href=&quot;#fnref:runit&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:fyiuops&quot;&gt;
      &lt;p&gt;This is why uops.info reports the latency for both &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;kmov r32, k&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;kmov k, 32&lt;/code&gt; as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&amp;lt;= 3&lt;/code&gt;. They know the pair takes 4 cycles in total and under the assumption that each instruction must take &lt;em&gt;at least&lt;/em&gt; one cycle the only thing you can really say is that each instruction takes at most 3 cycles. &lt;a href=&quot;#fnref:fyiuops&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:tech&quot;&gt;
      &lt;p&gt;Technically, I only tested the xor zeroing idiom, but since that’s the groud-zero, most basic idiom we can pretty sure nothing else will be recognized as zeroing. I’m open to being proven wrong: the code is public and easy to modify to test whatever idiom you want. &lt;a href=&quot;#fnref:tech&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:noenjoy&quot;&gt;
      &lt;p&gt;On the other hand, if you didn’t enjoy it, I have to wonder how you made it this far! Maybe you started off liking it, then felt invested and compelled to finish it? Beware the sunk cost fallacy! &lt;a href=&quot;#fnref:noenjoy&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><author><name>Travis Downs</name><email>travis.downs@gmail.com</email></author><category term="blog" /><category term="performance" /><category term="c++" /><category term="Intel" /><category term="uarch" /><summary type="html">If you are in a rush, you can skip to the summary, but you’ll miss out on the journey.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/assets/kreg/specialk.jpg" /><media:content medium="image" url="http://localhost:4000/assets/kreg/specialk.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Clang-format tanks performance</title><link href="http://localhost:4000/blog/2019/11/19/toupper.html" rel="alternate" type="text/html" title="Clang-format tanks performance" /><published>2019-11-19T00:00:00+02:00</published><updated>2019-11-19T00:00:00+02:00</updated><id>http://localhost:4000/blog/2019/11/19/toupper</id><content type="html" xml:base="http://localhost:4000/blog/2019/11/19/toupper.html">&lt;p class=&quot;info&quot;&gt;This article is also &lt;a href=&quot;https://habr.com/ru/company/pvs-studio/blog/480012/&quot;&gt;available in Russian&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Let’s benchmark &lt;a href=&quot;http://man7.org/linux/man-pages/man3/toupper.3.html&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;toupper&lt;/code&gt;&lt;/a&gt; implementations, because that’s a thing you do on Tuesdays.&lt;/p&gt;

&lt;p&gt;Actually, I don’t really care about &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;toupper&lt;/code&gt; much at all, but I was writing a different post and needed a peg to hang my narrative hat on, and hey &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;toupper&lt;/code&gt; seems like a nice harmless benchmark. Despite my effort to choose something which should be totally straightforward and not sidetrack me, this weird thing popped out.&lt;/p&gt;

&lt;p&gt;This will be a pretty short one - the longer, original post on the original, maybe far more interesting, topic is coming soon. The source is &lt;a href=&quot;https://github.com/travisdowns/toupper-bench&quot;&gt;available on github&lt;/a&gt; if you want to follow along.&lt;/p&gt;

&lt;p&gt;So let’s take a look at three implementations which perform &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;toupper&lt;/code&gt; over an array of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;char&lt;/code&gt;: that is, which take an input array and modifies it in-place so any lowercase characters are converted to uppercase.&lt;/p&gt;

&lt;p&gt;The first simply calls the C standard library &lt;a href=&quot;https://linux.die.net/man/3/toupper&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;toupper&lt;/code&gt; function&lt;/a&gt;&lt;sup id=&quot;fnref:tubroken&quot;&gt;&lt;a href=&quot;#fn:tubroken&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt; in a C-style loop:&lt;/p&gt;

&lt;div class=&quot;language-c++ highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;toupper_rawloop&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;char&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;buf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;size_t&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;size_t&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;++&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;buf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;toupper&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;buf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]);&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The second uses the &lt;a href=&quot;https://www.youtube.com/watch?v=2olsGf6JIkU&quot;&gt;more modern&lt;/a&gt; approach of using to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;std::transform&lt;/code&gt; to replace the raw loop:&lt;/p&gt;

&lt;div class=&quot;language-c++ highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;toupper_transform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;char&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;buf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;size_t&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;transform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;buf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;buf&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;buf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;toupper&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Finally, the third one is our bespoke ASCII-specific version that checks if the character lies in the range &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;a&lt;/code&gt; - &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;z&lt;/code&gt; and remaps it by subtracting 32 if so&lt;sup id=&quot;fnref:subnote&quot;&gt;&lt;a href=&quot;#fn:subnote&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;:&lt;/p&gt;

&lt;div class=&quot;language-c++ highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;toupper_branch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;char&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;buf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;size_t&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;size_t&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;++&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;kt&quot;&gt;char&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;buf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;];&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;c&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;=&lt;/span&gt; &lt;span class=&quot;sc&quot;&gt;'a'&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;=&lt;/span&gt; &lt;span class=&quot;sc&quot;&gt;'z'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;buf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Seems straightforward enough, right?&lt;/p&gt;

&lt;p&gt;Let’s benchmark these on my Skylake i7-6700HQ laptop, with the default gcc 5.5. Here’s a JPSP&lt;sup id=&quot;fnref:jpsp&quot;&gt;&lt;a href=&quot;#fn:jpsp&quot; class=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt; :&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/toupper/toupper-skl.svg&quot; alt=&quot;Skylake-S toupper performance&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Let’s get three observations that aren’t really part of the story out of the way.&lt;/p&gt;

&lt;p&gt;First, the pattern for the branchy algorithm (green). It’s the only one that varies much at all with input size - the other two are basically flat. This turns out to be just a benchmarking artifact. I use random ASCII input&lt;sup id=&quot;fnref:randnote&quot;&gt;&lt;a href=&quot;#fn:randnote&quot; class=&quot;footnote&quot;&gt;4&lt;/a&gt;&lt;/sup&gt;, so primary determinant of performance our branchy algorithm is branch prediction. For small input sizes, the branch predictor &lt;em&gt;learns the entire input sequence&lt;/em&gt; across iterations of the benchmark and so mispredictions are low and performance is high, &lt;a href=&quot;https://lemire.me/blog/2019/10/16/benchmarking-is-hard-processors-learn-to-predict-branches/&quot;&gt;just like this&lt;/a&gt;. As sequence size grows the predictor memorizes less and less of the sequence until it flatlines when it mispredicts every time there is an uppercase character (0.27 mispredicts per character).&lt;/p&gt;

&lt;p&gt;The second thing is this green blob of much slower samples from the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;toupper_branch&lt;/code&gt; in the upper left:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/toupper/green-blob.png&quot; alt=&quot;The green blob&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This wasn’t a one time artifact, I saw those guys hanging out up there across several runs. They don’t reproduce if you run that particular size alone however, only when the script runs to collect input across all size values. They don’t always show up. I didn’t look into it further but my best guess is some of unfortunate collision or aliasing effect perhaps in the branch predictor or in the 4k physical to virtual page mapping (VA space randomization was off, however).&lt;/p&gt;

&lt;p&gt;The third not interesting thing is the bimodal behavior of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;toupper_rawloop&lt;/code&gt; – the blue dots form two distinct lines, at just above 2 cycles per char and a faster line at 1.5 cycles per char. All performance counters that I checked were the same between the two “modes”. The fast mode, which runs at 1.57 chars/cycle is basically bottlenecked on the load ports: there are 1.54 uops/cycle going to both port 2 and port 3, so those ports are 98% occupied. The slower mode I can’t explain.&lt;/p&gt;

&lt;p&gt;While I was investigating it, the fast mode suddenly stopped appearing and I was stuck in slow mode. Maybe my CPU saw what I was up to and downloaded a microcode update in the background to remove the inconsistency, but I still have the SVG to prove it (for now).&lt;/p&gt;

&lt;p&gt;So what’s the interesting thing?&lt;/p&gt;

&lt;p&gt;The interesting thing is that the raw loop version runs 3x to 4x faster than the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;std::transform&lt;/code&gt; version: 1.5 to 2 cycles per character versus just above 7 cycles per character.&lt;/p&gt;

&lt;p&gt;What’s up with that? Are my standard algorithms letting me down? Does &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;std::transform&lt;/code&gt; have some fatal flaw?&lt;/p&gt;

&lt;p&gt;Not really. Well, not at all.&lt;/p&gt;

&lt;p&gt;It turns out these results occur when the functions are compiled in &lt;a href=&quot;https://github.com/travisdowns/toupper-bench/blob/256bb8318444faa8411ca6a9b11dcf4396f9ee81/impls-noalgo.cpp&quot;&gt;separate&lt;/a&gt; &lt;a href=&quot;https://github.com/travisdowns/toupper-bench/blob/256bb8318444faa8411ca6a9b11dcf4396f9ee81/impls-algo.cpp&quot;&gt;files&lt;/a&gt;. If you put them into the same file, suddenly the performance is is the same: they both run slowly.&lt;/p&gt;

&lt;p&gt;No, it’s not an alignment thing.&lt;/p&gt;

&lt;p&gt;It gets weirder too: the fast raw loop version, compiled in a separate file, &lt;em&gt;slows down if you simply include the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&amp;lt;algorithm&amp;gt;&lt;/code&gt; header&lt;/em&gt;. That’s right - including that header, which is never used and generates no code in the object file, slows down the raw loop by 3 to 4 times. Conversely, the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;std::transform&lt;/code&gt; version speeds up to full speed if you copy and paste the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;std::transform&lt;/code&gt; implementation out of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&amp;lt;algorithm&amp;gt;&lt;/code&gt; and stop including that file.&lt;/p&gt;

&lt;p&gt;It gets &lt;em&gt;even&lt;/em&gt; weirder (this is the last “it gets weirder”, I promise): including &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&amp;lt;algorithm&amp;gt;&lt;/code&gt; doesn’t always do this. The slowdown happens if &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&amp;lt;algorithm&amp;gt;&lt;/code&gt; is included before &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&amp;lt;ctype.h&amp;gt;&lt;/code&gt;, but not if you swap them around:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Slow:&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&quot;language-c++ highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;cp&quot;&gt;#include &amp;lt;algorithm&amp;gt;
#include &amp;lt;ctype.h&amp;gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Fast:&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&quot;language-c++ highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;cp&quot;&gt;#include &amp;lt;ctype.h&amp;gt;
#include &amp;lt;algorithm&amp;gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;In fact, in my case, this performance anomaly was triggered (in a different project) when clang-format sorted my headers, moving &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&amp;lt;algorithm&amp;gt;&lt;/code&gt; to the top where it belonged (hence the clickbait title).&lt;/p&gt;

&lt;p&gt;Of course, we were going to end up mired in assembly at some point. Let’s not postpone the pain any longer.&lt;/p&gt;

&lt;p&gt;Here are are the &lt;a href=&quot;https://godbolt.org/z/DwZBJM&quot;&gt;fast and slow&lt;/a&gt; versions of the functions&lt;sup id=&quot;fnref:fsnote&quot;&gt;&lt;a href=&quot;#fn:fsnote&quot; class=&quot;footnote&quot;&gt;5&lt;/a&gt;&lt;/sup&gt;, with the core loops annotated:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&amp;lt;algorithm&amp;gt;&lt;/code&gt; included first:&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;language-nasm highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;toupper_rawloop&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;char&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;unsigned&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;long&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;push&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;rbp&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;push&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;rbx&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;lea&lt;/span&gt;     &lt;span class=&quot;n&quot;&gt;rbp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rdi&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rsi&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;]&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;sub&lt;/span&gt;     &lt;span class=&quot;n&quot;&gt;rsp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;8&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;test&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;rsi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rsi&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;je&lt;/span&gt;      &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;L1&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;mov&lt;/span&gt;     &lt;span class=&quot;n&quot;&gt;rbx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rdi&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;L5&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;movsx&lt;/span&gt;   &lt;span class=&quot;n&quot;&gt;edi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;BYTE&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;PTR&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[rbx]&lt;/span&gt;  &lt;span class=&quot;c&quot;&gt;; load a char from *buf&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;add&lt;/span&gt;     &lt;span class=&quot;n&quot;&gt;rbx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;               &lt;span class=&quot;c&quot;&gt;; buf++&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;call&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;toupper&lt;/span&gt;              &lt;span class=&quot;c&quot;&gt;; call toupper(c)&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;mov&lt;/span&gt;     &lt;span class=&quot;n&quot;&gt;BYTE&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;PTR&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rbx&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;al&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;; save the result to buf[-1]&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;cmp&lt;/span&gt;     &lt;span class=&quot;n&quot;&gt;rbp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rbx&lt;/span&gt;             &lt;span class=&quot;c&quot;&gt;; check for buf == buf_end&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;jne&lt;/span&gt;     &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;L5&lt;/span&gt;                  &lt;span class=&quot;c&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;L1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;add&lt;/span&gt;     &lt;span class=&quot;n&quot;&gt;rsp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;8&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;pop&lt;/span&gt;     &lt;span class=&quot;n&quot;&gt;rbx&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;pop&lt;/span&gt;     &lt;span class=&quot;n&quot;&gt;rbp&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;ret&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;With &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&amp;lt;algorithm&amp;gt;&lt;/code&gt; second:&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;language-nasm highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;toupper_rawloop&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;char&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;unsigned&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;long&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;test&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;rsi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rsi&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;je&lt;/span&gt;      &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;L7&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;push&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;rbp&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;push&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;rbx&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;mov&lt;/span&gt;     &lt;span class=&quot;n&quot;&gt;rbp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rsi&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;mov&lt;/span&gt;     &lt;span class=&quot;n&quot;&gt;rbx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rdi&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;sub&lt;/span&gt;     &lt;span class=&quot;n&quot;&gt;rsp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;8&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;call&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;__ctype_toupper_loc&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;lea&lt;/span&gt;     &lt;span class=&quot;n&quot;&gt;rsi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rbx&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rbp&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;]&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;mov&lt;/span&gt;     &lt;span class=&quot;n&quot;&gt;rdi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rbx&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;L4&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;movsx&lt;/span&gt;   &lt;span class=&quot;n&quot;&gt;rcx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;BYTE&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;PTR&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[rdi]&lt;/span&gt;        &lt;span class=&quot;c&quot;&gt;; load a char from buf&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;mov&lt;/span&gt;     &lt;span class=&quot;n&quot;&gt;rdx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;QWORD&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;PTR&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[rax]&lt;/span&gt;       &lt;span class=&quot;c&quot;&gt;; load the toupper table address (pointed to by __ctype_toupper_loc)&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;add&lt;/span&gt;     &lt;span class=&quot;n&quot;&gt;rdi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;                     &lt;span class=&quot;c&quot;&gt;; buf++&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;mov&lt;/span&gt;     &lt;span class=&quot;n&quot;&gt;edx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;DWORD&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;PTR&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rdx&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rcx&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;; look up the toupper result by indexing into table with the char&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;mov&lt;/span&gt;     &lt;span class=&quot;n&quot;&gt;BYTE&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;PTR&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rdi&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dl&lt;/span&gt;       &lt;span class=&quot;c&quot;&gt;; store the result&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;cmp&lt;/span&gt;     &lt;span class=&quot;n&quot;&gt;rsi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rdi&lt;/span&gt;                   &lt;span class=&quot;c&quot;&gt;; check buf == end_buf&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;jne&lt;/span&gt;     &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;L4&lt;/span&gt;                        &lt;span class=&quot;c&quot;&gt;;&lt;/span&gt;

        &lt;span class=&quot;k&quot;&gt;add&lt;/span&gt;     &lt;span class=&quot;n&quot;&gt;rsp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;8&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;pop&lt;/span&gt;     &lt;span class=&quot;n&quot;&gt;rbx&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;pop&lt;/span&gt;     &lt;span class=&quot;n&quot;&gt;rbp&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;L7&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;kr&quot;&gt;rep&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;ret&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The key difference is the slow version simply calls &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;toupper&lt;/code&gt; in the loop, while the fast version has no function calls at all, just a table lookup&lt;sup id=&quot;fnref:aliasing&quot;&gt;&lt;a href=&quot;#fn:aliasing&quot; class=&quot;footnote&quot;&gt;6&lt;/a&gt;&lt;/sup&gt; - the body of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;std::toupper&lt;/code&gt; has been inlined.&lt;/p&gt;

&lt;p&gt;Examining the glibc &lt;a href=&quot;https://sourceware.org/git/?p=glibc.git;a=blob;f=ctype/ctype.h;h=d17f727cf0dc2a0f6c62fa50aff799b175dcb426;hb=2a764c6ee848dfe92cb2921ed3b14085f15d9e79#l205&quot;&gt;source&lt;/a&gt;, we find the implementation of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;toupper&lt;/code&gt;:&lt;/p&gt;

&lt;div class=&quot;language-c++ highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;__extern_inline&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;__NTH&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;toupper&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;__c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;// __NTH is a macro that indicates the function doesn't throw&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;__c&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;128&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;__c&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;256&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;?&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;__ctype_toupper_loc&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;())[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;__c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;__c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We see that &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;toupper&lt;/code&gt; is implemented as an &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;extern inline&lt;/code&gt; function that first checks that the range of the char fits within a byte&lt;sup id=&quot;fnref:bcheck&quot;&gt;&lt;a href=&quot;#fn:bcheck&quot; class=&quot;footnote&quot;&gt;7&lt;/a&gt;&lt;/sup&gt; and then looks up the character in the table returned by &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;__ctype_toupper_loc()&lt;/code&gt;. That function returns a thread-local pointer (a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;const int **&lt;/code&gt;), which in turn points to a lookup table which given a character returns the uppercase version&lt;sup id=&quot;fnref:nobran&quot;&gt;&lt;a href=&quot;#fn:nobran&quot; class=&quot;footnote&quot;&gt;8&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;p&gt;So now the assembly makes sense: the fast version of the algorithm inlines the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;toupper&lt;/code&gt; body, but it can’t inline the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;__ctype_toupper_loc()&lt;/code&gt; call&lt;sup id=&quot;fnref:inline2&quot;&gt;&lt;a href=&quot;#fn:inline2&quot; class=&quot;footnote&quot;&gt;9&lt;/a&gt;&lt;/sup&gt;; however, this call is declared &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;__attribute__((const))&lt;/code&gt; which means that its return value depends only on the arguments (and here there are no arguments) and so the compiler knows it returns the same value every time and so can be hoisted out of the loop, so the loop body has only a few loads associated with the lookup table, the store of the new value to the buffer, and loop control.&lt;/p&gt;

&lt;p&gt;The slow version, on the other hand, leaves the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;toupper()&lt;/code&gt; inside the loop. The loop itself is one instruction shorted, but of course you need to run all the code inside &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;toupper&lt;/code&gt; as well. Here’s what that looks like on my system:&lt;/p&gt;

&lt;div class=&quot;language-nasm highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  &lt;span class=&quot;k&quot;&gt;lea&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;edx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rdi&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mh&quot;&gt;0x80&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;]&lt;/span&gt;                   &lt;span class=&quot;c&quot;&gt;; edx = rdi + 0x80&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;movsxd&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;edi&lt;/span&gt;                          &lt;span class=&quot;c&quot;&gt;; zero extend c&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;cmp&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;edx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mh&quot;&gt;0x17f&lt;/span&gt;                        &lt;span class=&quot;c&quot;&gt;; check that c is in -128 to 255&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;ja&lt;/span&gt;     &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;                               &lt;span class=&quot;c&quot;&gt;; if not, we're done&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;mov&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;rdx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;QWORD&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;PTR&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rip&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mh&quot;&gt;0x395f30&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;]&lt;/span&gt;     &lt;span class=&quot;c&quot;&gt;; lookup TLS index&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;mov&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;rdx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;QWORD&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;PTR&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[rdx]&lt;/span&gt;           &lt;span class=&quot;c&quot;&gt;; access TLS at index&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;mov&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;rdx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;QWORD&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;PTR&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[rdx]&lt;/span&gt;              &lt;span class=&quot;c&quot;&gt;; dereference TLS pointer&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;mov&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;rdx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;QWORD&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;PTR&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rdx&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mh&quot;&gt;0x48&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;]&lt;/span&gt;         &lt;span class=&quot;c&quot;&gt;; load current toupper lookup table&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;mov&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;eax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;DWORD&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;PTR&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rdx&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rax&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mh&quot;&gt;0x200&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;]&lt;/span&gt;  &lt;span class=&quot;c&quot;&gt;; lookup c in LUT&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;ret&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Since it’s a standalone function call, it has to do more work. There are no less than &lt;em&gt;five&lt;/em&gt; chained (pointer-chasing) memory accesses. Only two of those accesses remained in the fast loop, because the rest were hoisted up and out of the loop. The input to output latency of this function is probably close to 25 cycles, so out measured throughput of ~7 cycles means that the CPU was able to run several copies in parallel, not too terrible all things considered.&lt;/p&gt;

&lt;p&gt;Why does this happen?&lt;/p&gt;

&lt;p&gt;Through a long series of includes, C++ files like &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&amp;lt;algorithm&amp;gt;&lt;/code&gt; include &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&amp;lt;bits/os_defines.h&amp;gt;&lt;/code&gt; which has this line:&lt;/p&gt;

&lt;div class=&quot;language-c++ highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;// This keeps isanum, et al from being propagated as macros.&lt;/span&gt;
&lt;span class=&quot;cp&quot;&gt;#define __NO_CTYPE 1
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;When &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&amp;lt;ctype.h&amp;gt;&lt;/code&gt; is ultimately included, this prevents the block containing the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;extern inline&lt;/code&gt; definition of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;toupper&lt;/code&gt; from being included:&lt;/p&gt;

&lt;div class=&quot;language-c++ highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;cp&quot;&gt;#if !defined __NO_CTYPE
# ifdef __isctype_f
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;__isctype_f&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;alnum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;// lots more like this&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;__isctype_f&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xdigit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;cp&quot;&gt;# elif defined __isctype
# define isalnum(c)	__isctype((c), _ISalnum)
# define isalpha(c)	__isctype((c), _ISalpha)
&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;// more like this&lt;/span&gt;
&lt;span class=&quot;cp&quot;&gt;# endif
&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;// the stuff we care about&lt;/span&gt;
&lt;span class=&quot;cp&quot;&gt;# ifdef __USE_EXTERN_INLINES
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;__extern_inline&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;__NTH&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tolower&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;__c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;__c&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;128&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;__c&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;256&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;?&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;__ctype_tolower_loc&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;())[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;__c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;__c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;__extern_inline&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;__NTH&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;toupper&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;__c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;__c&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;128&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;__c&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;256&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;?&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;__ctype_toupper_loc&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;())[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;__c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;__c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;cp&quot;&gt;# endif
&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;// here's where tolower is defined as a macro&lt;/span&gt;
&lt;span class=&quot;cp&quot;&gt;# if __GNUC__ &amp;gt;= 2 &amp;amp;&amp;amp; defined __OPTIMIZE__ &amp;amp;&amp;amp; !defined __cplusplus
#  define tolower(c)	__tobody (c, tolower, *__ctype_tolower_loc (), (c))
#  define toupper(c)	__tobody (c, toupper, *__ctype_toupper_loc (), (c))
# endif &lt;/span&gt;&lt;span class=&quot;cm&quot;&gt;/* Optimizing gcc */&lt;/span&gt;&lt;span class=&quot;cp&quot;&gt;
&lt;/span&gt;
&lt;span class=&quot;cp&quot;&gt;#endif &lt;/span&gt;&lt;span class=&quot;cm&quot;&gt;/* Not __NO_CTYPE.  */&lt;/span&gt;&lt;span class=&quot;cp&quot;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Note when including &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&amp;lt;ctype.h&amp;gt;&lt;/code&gt; C++ &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;toupper&lt;/code&gt; is &lt;em&gt;never&lt;/em&gt; defined as a macro - at most it is &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;extern inline&lt;/code&gt; - the macro definitions below that are guarded by &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;!defined __cplusplus&lt;/code&gt; so they’ll never take effect.&lt;/p&gt;

&lt;p&gt;So I’m not sure if the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;extern inline&lt;/code&gt; bodies of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;tolower&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;toupper&lt;/code&gt; are intended to be excluded by &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;__NO_CTYPE&lt;/code&gt; in this case, but that’s what happens and this has a significant performance impact in this toy loop. As a corollary, if you include &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&amp;lt;cctype&amp;gt;&lt;/code&gt; rather than &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&amp;lt;ctype.h&amp;gt;&lt;/code&gt; (the C++ version of the C header which puts functions in the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;std::&lt;/code&gt; namespace) you also get the slow behavior because &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&amp;lt;cctype&amp;gt;&lt;/code&gt; ultimately includes &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&amp;lt;bits/os_defines.h&amp;gt;&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Does this matter?&lt;/em&gt; Nah.&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;toupper&lt;/code&gt; is broken for serious multilingual use and, if you only care about ASCII you can write your own faster function. If you care about proper text handling, you are probably using UTF-8 and you’ll have to use something like ICU to do locale-aware text handling, or wait for C++ to get Unicode support (you might be waiting a while). It’s only interesting in clickbait sense of “clang-format can cause a 4x performance regression”.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Does this happen on all libc versions?&lt;/em&gt; Mostly yes, but it gets complicated.&lt;/p&gt;

&lt;p&gt;The above results apply directly to gcc 5.5 and glibc 2.23, since that’s what I used, but things get weirder on newer versions (starting around glibc 2.27). On newer versions, the issue as described above occurs between &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&amp;lt;algorithm&amp;gt;&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&amp;lt;ctype.h&amp;gt;&lt;/code&gt;, but additionally &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&amp;lt;stdlib.h&amp;gt;&lt;/code&gt; enters the picture&lt;sup id=&quot;fnref:anwho&quot;&gt;&lt;a href=&quot;#fn:anwho&quot; class=&quot;footnote&quot;&gt;10&lt;/a&gt;&lt;/sup&gt;: if you include &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&amp;lt;stdlib.h&amp;gt;&lt;/code&gt; before &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&amp;lt;ctype.h&amp;gt;&lt;/code&gt; you’ll enter slow mode, which doesn’t happen on earlier versions. So apparently &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&amp;lt;stdlib.h&amp;gt;&lt;/code&gt; also ends up defining &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;__NO_CTYPE&lt;/code&gt; at some point on these newer versions. At least here we can’t blame clang-format sorting – it might &lt;em&gt;fix&lt;/em&gt; the issue for by sorting headers (in files where you aren’t including any other problematic header).&lt;/p&gt;

&lt;p&gt;I have filed a &lt;a href=&quot;https://sourceware.org/bugzilla/show_bug.cgi?id=25214&quot;&gt;bug against libc&lt;/a&gt;, so this specific issue might be fixed in the future, but no doubt the scourge of include order will be with us more or less forever.&lt;/p&gt;

&lt;h3 id=&quot;comments&quot;&gt;Comments&lt;/h3&gt;

&lt;p&gt;I don’t have a comments system, but I’m working on it (read: occasionally complaining out loud that comments on a static site are hard).&lt;/p&gt;

&lt;p&gt;In the meantime you can discuss this on &lt;a href=&quot;https://news.ycombinator.com/item?id=21579333&quot;&gt;Hacker News&lt;/a&gt; or &lt;a href=&quot;https://lobste.rs/s/tjxzck/clang_format_tanks_performance&quot;&gt;lobste.rs&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;thanks&quot;&gt;Thanks&lt;/h3&gt;

&lt;p&gt;Thanks to &lt;abbr title=&quot;HackerNews&quot;&gt;HN&lt;/abbr&gt; user ufo who &lt;a href=&quot;https://news.ycombinator.com/item?id=21579483&quot;&gt;pointed out&lt;/a&gt; you don’t need a lambda to adapt &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;std::toupper&lt;/code&gt; for use in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;std::transform&lt;/code&gt;, and Jonathan Müller who subsequently &lt;a href=&quot;https://twitter.com/foonathan/status/1197051249822195712&quot;&gt;pointed out&lt;/a&gt; that in fact you do want a lambda.&lt;/p&gt;

&lt;p class=&quot;info&quot;&gt;If you liked this post, check out the &lt;a href=&quot;/&quot;&gt;homepage&lt;/a&gt; for others you might enjoy&lt;sup id=&quot;fnref:noenjoy&quot;&gt;&lt;a href=&quot;#fn:noenjoy&quot; class=&quot;footnote&quot;&gt;11&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;hr /&gt;
&lt;hr /&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;div class=&quot;footnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:tubroken&quot;&gt;
      &lt;p&gt;Yes, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;toupper(3)&lt;/code&gt; from &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&amp;lt;ctype.h&amp;gt;&lt;/code&gt; is basically irreparably broken for most non-ASCII use, because it cannot handle multibyte characters, but it is good enough for our purposes. We only feed it ASCII strings. &lt;a href=&quot;#fnref:tubroken&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:subnote&quot;&gt;
      &lt;p&gt;ASCII conveniently locates lowercase and uppercase characters 32 positions apart, meaning that converting between then is a simple matter of adding or subtracting 32. In fact, if we were sure that all our input were ASCII letters, we could just unconditionally clear the 5th bit, e.g. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;c &amp;amp; 0b11011111&lt;/code&gt;, which would lower any uppercase and leave lowercase unchanged. Of course, we can’t rely on inputs to be letters, so we need the range check to avoid clobbering non-letter &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;char&lt;/code&gt; values. &lt;a href=&quot;#fnref:subnote&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:jpsp&quot;&gt;
      &lt;p&gt;Err, that would be a &lt;em&gt;Jittered Performance Scatter Plot&lt;/em&gt;. This is basically a scatter plot with some interesting parameter on the x-axis (in this case, the size of the input) and performance (in this case, cycles per character, &lt;em&gt;lower is better&lt;/em&gt;) on the y-axis. The main feature is that each x parameter value is sampled multiple times: here the benchmark is repeated 10 times for each size &lt;a href=&quot;#fnref:jpsp&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:randnote&quot;&gt;
      &lt;p&gt;In particular, characters are chosen uniformly at random in the range [32, 127], so the if statement in the function is true ~27% of the time. &lt;a href=&quot;#fnref:randnote&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:fsnote&quot;&gt;
      &lt;p&gt;Specifically, I’m showing the generated code in both cases for the raw loop version, the only difference being the order of include of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&amp;lt;algorithm&amp;gt;&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&amp;lt;ctype.h&amp;gt;&lt;/code&gt;. The source generated is basically the same for all the fast and slow variants: e.g., the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;std::transform&lt;/code&gt; version generates basically the same slow and fast assembly as shown if you use it through &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&amp;lt;algorithm&amp;gt;&lt;/code&gt; or “copy and paste”, respectively. &lt;a href=&quot;#fnref:fsnote&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:aliasing&quot;&gt;
      &lt;p&gt;Even the fast loop isn’t as fast as I could be, as the lookup table pointer is redundantly reloaded (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mov rdx, QWORD PTR [rax]&lt;/code&gt;) inside the loop. That table pointer would change when the locale changes, but it is not updated during the loop so it could be hoisted. Perhaps the compiler can’t prove that because we are writing a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;char&lt;/code&gt; array (which could in principle alias &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;[rax]&lt;/code&gt;, the table pointer), but even &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;__restrict__&lt;/code&gt; doesn’t help. A different version of the loop which just sums the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;toupper&lt;/code&gt; values and doesn’t write to a char array &lt;a href=&quot;https://godbolt.org/z/Kb6pc8&quot;&gt;does receive this optimization&lt;/a&gt; - the load of the pointer is moved outside the loop. &lt;a href=&quot;#fnref:aliasing&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:bcheck&quot;&gt;
      &lt;p&gt;This range check doesn’t leave in the trace in the inlined assembly, because the compiler already knows that &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;char&lt;/code&gt; values will always fall in the range &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;[-128, 255]&lt;/code&gt; - it is needed only because the API to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;toupper(c)&lt;/code&gt; takes an &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;int&lt;/code&gt; value rather than &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;char&lt;/code&gt; so people could pass any old &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;int&lt;/code&gt; value but the lookup tables are only sized for &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;char&lt;/code&gt;, so this check is needed to avoid out-of-bounds accesses. &lt;a href=&quot;#fnref:bcheck&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:nobran&quot;&gt;
      &lt;p&gt;Incidentally, this shows why the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;std::toupper&lt;/code&gt; routines don’t show any dependence on input size: they don’t use branches (except for the range-check branches which will predict perfectly), but rather a branch-free lookup table. &lt;a href=&quot;#fnref:nobran&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:inline2&quot;&gt;
      &lt;p&gt;This call could not be inlined even if you wanted to: its body is not available in the header. &lt;a href=&quot;#fnref:inline2&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:anwho&quot;&gt;
      &lt;p&gt;I’m not really picking on &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;stdlib.h&lt;/code&gt; (or &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&amp;lt;algorithm&amp;gt;&lt;/code&gt; for that matter) - it is entirely likely that all C++ headers, and possibly many more C headers also trigger the behavior, but I didn’t test for that. I was including &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;stdlib.h&lt;/code&gt; just to get the definition for &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;size_t&lt;/code&gt;. &lt;a href=&quot;#fnref:anwho&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:noenjoy&quot;&gt;
      &lt;p&gt;On the other hand, if you didn’t enjoy it, I have to wonder how you made it this far! Maybe you started off liking it, then felt invested and compelled to finish it? Beware the sunk cost fallacy! &lt;a href=&quot;#fnref:noenjoy&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><author><name>Travis Downs</name><email>travis.downs@gmail.com</email></author><category term="blog" /><category term="performance" /><category term="c++" /><category term="Intel" /><category term="uarch" /><summary type="html">This article is also available in Russian.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/assets/toupper/twitter-card.png" /><media:content medium="image" url="http://localhost:4000/assets/toupper/twitter-card.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Incrementing vectors</title><link href="http://localhost:4000/blog/2019/08/26/vector-inc.html" rel="alternate" type="text/html" title="Incrementing vectors" /><published>2019-08-26T00:00:00+02:00</published><updated>2019-08-26T00:00:00+02:00</updated><id>http://localhost:4000/blog/2019/08/26/vector-inc</id><content type="html" xml:base="http://localhost:4000/blog/2019/08/26/vector-inc.html">&lt;p class=&quot;info&quot;&gt;This article is also &lt;a href=&quot;https://habr.com/ru/company/pvs-studio/blog/475636/&quot;&gt;available in Russian&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;What’s faster, incrementing a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;std::vector&lt;/code&gt; of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;uint8_t&lt;/code&gt; or of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;uint32_t&lt;/code&gt;?&lt;/p&gt;

&lt;p&gt;To make it concrete, let’s consider these two implementations:&lt;/p&gt;

&lt;div class=&quot;language-c++ highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;vector8_inc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vector&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;uint8_t&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;amp;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;v&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;size_t&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;v&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;++&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;v&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;++&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;vector32_inc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vector&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;uint32_t&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;amp;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;v&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;size_t&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;v&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;++&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;v&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;++&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;lets-guess&quot;&gt;Let’s Guess&lt;/h3&gt;

&lt;p&gt;It’s easy to answer this question with a benchmark, and we will get to that soon, but for now let’s take some guesses (also known as “reasoning from first principles” if you want to pretend there is a lot of science in it).&lt;/p&gt;

&lt;p&gt;First, we might reasonably ask: &lt;em&gt;How big are these vectors&lt;/em&gt;?&lt;/p&gt;

&lt;p&gt;Let’s pick a number then. I’m going with 20,000 elements in each.&lt;/p&gt;

&lt;p&gt;Then knowing the hardware we’re testing on, Intel Skylake, we can check the characteristics of &lt;a href=&quot;https://uops.info/html-instr/ADD_M8_I8.html&quot;&gt;8-bit&lt;/a&gt; and &lt;a href=&quot;https://uops.info/html-instr/ADD_M32_I32.html&quot;&gt;32-bit&lt;/a&gt; add &lt;abbr title=&quot;When discussing assembly instructions an immediate is a value embedded in the instruction itself, e.g., the 1 in add eax, 1.&quot;&gt;immediate&lt;/abbr&gt; instructions. It turns out that their primary performance characteristics are identical: 1 per cycle throughput and 4 cycles latency through memory&lt;sup id=&quot;fnref:thrumem&quot;&gt;&lt;a href=&quot;#fn:thrumem&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;. In this case, we don’t expect latency to matter since each add is independent, so we might hope this runs at 1 cycle per element, if the rest of the loop overhead can run in parallel.&lt;/p&gt;

&lt;p&gt;We might also note that 20,000 elements means a working set of 20 KB for the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;uint8_t&lt;/code&gt; vector, but 80 KB for the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;uint32_t&lt;/code&gt; vector. The former fits cleanly in the L1 cache of modern x86 machines, while the second doesn’t. So maybe the 8-bit version will have a leg up due to caching effects?&lt;/p&gt;

&lt;p&gt;Finally, we might note that this problem seems like a textbook case for &lt;a href=&quot;https://en.wikipedia.org/wiki/Automatic_vectorization&quot;&gt;auto-vectorization&lt;/a&gt;: a loop with a known number of iterations doing an arithmetic operation on adjacent elements in memory. In that case, we might expect the 8-bit version to have a huge advantage over the 32-bit version, since every vector operation will process 4 times as many elements, and in general, on Intel, the byte element vector operations have the same performance as their 32-bit element counterparts.&lt;/p&gt;

&lt;p&gt;Enough prologue. Let’s benchmark this.&lt;/p&gt;

&lt;h3 id=&quot;the-benchmark&quot;&gt;The Benchmark&lt;/h3&gt;

&lt;p&gt;For 20,000 elements, I get the following timings with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;gcc 8&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;clang 8&lt;/code&gt; at various optimization levels:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Compiler&lt;/th&gt;
      &lt;th&gt;Element&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;-O1&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;-O2&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;-O3&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;gcc 8&lt;/td&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;uint8_t&lt;/code&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;2.0&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;2.0&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;2.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;gcc 8&lt;/td&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;uint32_t&lt;/code&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;2.3&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;1.3&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;clang 8&lt;/td&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;uint8_t&lt;/code&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;9.2&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;2.0&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;2.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;clang 8&lt;/td&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;uint32_t&lt;/code&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;9.2&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.2&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.2&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;The conclusion is that except at &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-O1&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;uint32_t&lt;/code&gt; is faster than &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;uint8_t&lt;/code&gt; and sometimes dramatically so: for gcc at &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-O3&lt;/code&gt; the speedup was 5.4x and for clang at either &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-O2&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-O3&lt;/code&gt; the speedup was 8.0x. Yes, incrementing 32-bit integers in a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;std::vector&lt;/code&gt; is up to &lt;em&gt;eight times faster&lt;/em&gt; than incrementing 8-bits on a popular compiler with common optimization settings.&lt;/p&gt;

&lt;p&gt;As usual, we hope the assembly will tell us what is going on.&lt;/p&gt;

&lt;p&gt;Here’s gcc 8 at &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-O2&lt;/code&gt; where the 32-bit version was “only” 1.5x faster than the 8-bit one&lt;sup id=&quot;fnref:looponly&quot;&gt;&lt;a href=&quot;#fn:looponly&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;8-bit:&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&quot;language-nasm highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;L3&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;inc&lt;/span&gt;     &lt;span class=&quot;n&quot;&gt;BYTE&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;PTR&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rdx&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rax&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;mov&lt;/span&gt;     &lt;span class=&quot;n&quot;&gt;rdx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;QWORD&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;PTR&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[rdi]&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;inc&lt;/span&gt;     &lt;span class=&quot;n&quot;&gt;rax&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;mov&lt;/span&gt;     &lt;span class=&quot;n&quot;&gt;rcx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;QWORD&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;PTR&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rdi&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;8&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;sub&lt;/span&gt;     &lt;span class=&quot;n&quot;&gt;rcx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rdx&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;cmp&lt;/span&gt;     &lt;span class=&quot;n&quot;&gt;rax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rcx&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;jb&lt;/span&gt;      &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;L3&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;32-bit:&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&quot;language-nasm highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;L9&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;inc&lt;/span&gt;     &lt;span class=&quot;n&quot;&gt;DWORD&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;PTR&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[rax]&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;add&lt;/span&gt;     &lt;span class=&quot;n&quot;&gt;rax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;cmp&lt;/span&gt;     &lt;span class=&quot;n&quot;&gt;rax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rdx&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;jne&lt;/span&gt;     &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;L9&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The 32-bit code looks like we’d expect from a not-unrolled&lt;sup id=&quot;fnref:unrolled&quot;&gt;&lt;a href=&quot;#fn:unrolled&quot; class=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt; loop: an increment&lt;sup id=&quot;fnref:incbad&quot;&gt;&lt;a href=&quot;#fn:incbad&quot; class=&quot;footnote&quot;&gt;4&lt;/a&gt;&lt;/sup&gt; with a memory destination, and then three loop control instructions: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;add rax, 4&lt;/code&gt; to increment the induction variable&lt;sup id=&quot;fnref:induc&quot;&gt;&lt;a href=&quot;#fn:induc&quot; class=&quot;footnote&quot;&gt;5&lt;/a&gt;&lt;/sup&gt;, and a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;cmp&lt;/code&gt; &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;jne&lt;/code&gt; pair to check and branch on the loop termination condition. It looks fine – an unroll would help to amortize the loop overhead, getting us closer to 1 cycle/element store limit&lt;sup id=&quot;fnref:gccunroll&quot;&gt;&lt;a href=&quot;#fn:gccunroll&quot; class=&quot;footnote&quot;&gt;6&lt;/a&gt;&lt;/sup&gt;, but good enough for open source work.&lt;/p&gt;

&lt;p&gt;So What’s going on with the 8-bit version? In addition to the memory-destination &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;inc&lt;/code&gt;, you have two other loads from memory and some stuff like a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sub&lt;/code&gt; that appeared out of nowhere.&lt;/p&gt;

&lt;p&gt;Here’s an annotated version:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;8-bit:&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&quot;language-nasm highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;L3&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;inc&lt;/span&gt;     &lt;span class=&quot;n&quot;&gt;BYTE&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;PTR&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rdx&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rax&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;]&lt;/span&gt;     &lt;span class=&quot;c&quot;&gt;; increment memory at v[i]&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;mov&lt;/span&gt;     &lt;span class=&quot;n&quot;&gt;rdx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;QWORD&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;PTR&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[rdi]&lt;/span&gt;   &lt;span class=&quot;c&quot;&gt;; load v.begin&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;inc&lt;/span&gt;     &lt;span class=&quot;n&quot;&gt;rax&lt;/span&gt;                    &lt;span class=&quot;c&quot;&gt;; i++&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;mov&lt;/span&gt;     &lt;span class=&quot;n&quot;&gt;rcx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;QWORD&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;PTR&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rdi&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;8&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;; load v.end&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;sub&lt;/span&gt;     &lt;span class=&quot;n&quot;&gt;rcx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rdx&lt;/span&gt;               &lt;span class=&quot;c&quot;&gt;; end - start (i.e., vector.size())&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;cmp&lt;/span&gt;     &lt;span class=&quot;n&quot;&gt;rax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rcx&lt;/span&gt;               &lt;span class=&quot;c&quot;&gt;; i &amp;lt; size()&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;jb&lt;/span&gt;      &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;L3&lt;/span&gt;                    &lt;span class=&quot;c&quot;&gt;; next itr if i &amp;lt; size()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Here &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;vector::begin&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;vector::end&lt;/code&gt; are the internal pointers that &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;std::vector&lt;/code&gt; keeps to indicate the beginning and of the elements stored in its internal storage&lt;sup id=&quot;fnref:realnames&quot;&gt;&lt;a href=&quot;#fn:realnames&quot; class=&quot;footnote&quot;&gt;7&lt;/a&gt;&lt;/sup&gt; - basically the same values used to implement &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;vector::begin()&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;vector::end()&lt;/code&gt; (although the type differs). So all the extra instructions appear as a consequence of calculating &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;vector.size()&lt;/code&gt;. Maybe doesn’t seem too weird? Well maybe it does, because of course the 32-bit version uses &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;size()&lt;/code&gt; also, but the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;size()&lt;/code&gt; related instructions don’t appear that loop. Instead, the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;size()&lt;/code&gt; calculation happens once, outside the loop.&lt;/p&gt;

&lt;p&gt;What’s the difference? The short answer is &lt;a href=&quot;https://en.wikipedia.org/wiki/Pointer_aliasing&quot;&gt;&lt;em&gt;pointer aliasing&lt;/em&gt;&lt;/a&gt;. The long answer is up next.&lt;/p&gt;

&lt;h3 id=&quot;the-long-answer&quot;&gt;The Long Answer&lt;/h3&gt;

&lt;p&gt;The vector &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;v&lt;/code&gt; is passed to the function as a reference, which boils down to a pointer under the covers. The compiler needs to use the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;v::begin&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;v::end&lt;/code&gt; members of the vector to calculate &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;size()&lt;/code&gt;, and in the source we’ve written &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;size()&lt;/code&gt; is evaluated &lt;em&gt;every iteration&lt;/em&gt;. The compiler isn’t necessarily a slave to the source however: it can hoist the result of the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;size()&lt;/code&gt; function above the loop, but only if it can prove that this &lt;a href=&quot;https://en.wikipedia.org/wiki/As-if_rule&quot;&gt;doesn’t change&lt;/a&gt; the semantics of the program. In that regard, the only problematic part of the loop is the increment &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;v[i]++&lt;/code&gt;. This writes to some unknown memory location. The question is: &lt;em&gt;can that write change the value of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;size()&lt;/code&gt;&lt;/em&gt;?&lt;/p&gt;

&lt;p&gt;In the case of writes to a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;std::vector&amp;lt;uint32_t&amp;gt;&lt;/code&gt; (which boil down to writes to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;uint32_t *&lt;/code&gt;) the answer is &lt;strong&gt;no, it cannot change size()&lt;/strong&gt;. Writes to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;uint32_t&lt;/code&gt; objects can only change &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;uint32_t&lt;/code&gt; objects, and the pointers involved in the calculation of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;size()&lt;/code&gt; are not &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;uint32_t&lt;/code&gt; objects&lt;sup id=&quot;fnref:ptype&quot;&gt;&lt;a href=&quot;#fn:ptype&quot; class=&quot;footnote&quot;&gt;8&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;p&gt;On the other hand, for &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;uint8_t&lt;/code&gt;, at least for common compilers&lt;sup id=&quot;fnref:commoncom&quot;&gt;&lt;a href=&quot;#fn:commoncom&quot; class=&quot;footnote&quot;&gt;9&lt;/a&gt;&lt;/sup&gt;, the answer is &lt;strong&gt;yes, it could in theory change the value of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;size()&lt;/code&gt;&lt;/strong&gt;, because &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;uint8_t&lt;/code&gt; is a typedef for &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;unsigned char&lt;/code&gt;, and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;unsigned char&lt;/code&gt; (and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;char&lt;/code&gt;) arrays are allowed to &lt;em&gt;alias any type&lt;/em&gt;. That means that writes through &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;uint8_t&lt;/code&gt; pointers are treated as potentially updating any memory of unknown provenance&lt;sup id=&quot;fnref:unp&quot;&gt;&lt;a href=&quot;#fn:unp&quot; class=&quot;footnote&quot;&gt;10&lt;/a&gt;&lt;/sup&gt;. So the compiler assumes that every increment &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;v[i]++&lt;/code&gt; potentially modifies &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;size()&lt;/code&gt; and hence must recalculate it every iteration.&lt;/p&gt;

&lt;p&gt;Now, you and I know writes to the memory pointed to by a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;std::vector&lt;/code&gt; are never going modify its own &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;size()&lt;/code&gt; - that would mean that the vector object itself has somehow been allocated &lt;em&gt;inside its own heap storage&lt;/em&gt;, an almost impossible chicken-and-egg scenario&lt;sup id=&quot;fnref:inside&quot;&gt;&lt;a href=&quot;#fn:inside&quot; class=&quot;footnote&quot;&gt;11&lt;/a&gt;&lt;/sup&gt;. Unfortunately, the compiler doesn’t know that!&lt;/p&gt;

&lt;h3 id=&quot;what-about-the-rest&quot;&gt;What about the rest&lt;/h3&gt;

&lt;p&gt;So that explains the small performance difference between the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;uint8_t&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;uint32_t&lt;/code&gt; for &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;gcc -O2&lt;/code&gt;. How about the huge, up to 8x difference, we see for clang or gcc at &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-O3&lt;/code&gt;?&lt;/p&gt;

&lt;p&gt;That’s easy - clang can autovectorize this loop in the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;uint32_t&lt;/code&gt; case:&lt;/p&gt;

&lt;div class=&quot;language-nasm highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;LBB1_6&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;                                &lt;span class=&quot;c&quot;&gt;; =&amp;gt;This Inner Loop Header: Depth=1&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;vmovdqu&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ymm1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ymmword&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ptr&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rax&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rdi&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;vmovdqu&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ymm2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ymmword&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ptr&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rax&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rdi&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;32&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;vmovdqu&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ymm3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ymmword&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ptr&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rax&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rdi&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;64&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;vmovdqu&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ymm4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ymmword&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ptr&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rax&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rdi&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;96&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;vpsubd&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;ymm1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ymm1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ymm0&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;vpsubd&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;ymm2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ymm2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ymm0&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;vpsubd&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;ymm3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ymm3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ymm0&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;vpsubd&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;ymm4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ymm4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ymm0&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;vmovdqu&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ymmword&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ptr&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rax&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rdi&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ymm1&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;vmovdqu&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ymmword&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ptr&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rax&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rdi&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;32&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ymm2&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;vmovdqu&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ymmword&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ptr&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rax&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rdi&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;64&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ymm3&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;vmovdqu&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ymmword&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ptr&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rax&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rdi&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;96&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ymm4&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;vmovdqu&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ymm1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ymmword&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ptr&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rax&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rdi&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;128&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;vmovdqu&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ymm2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ymmword&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ptr&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rax&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rdi&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;160&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;vmovdqu&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ymm3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ymmword&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ptr&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rax&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rdi&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;192&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;vmovdqu&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ymm4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ymmword&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ptr&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rax&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rdi&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;224&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;vpsubd&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;ymm1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ymm1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ymm0&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;vpsubd&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;ymm2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ymm2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ymm0&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;vpsubd&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;ymm3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ymm3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ymm0&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;vpsubd&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;ymm4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ymm4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ymm0&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;vmovdqu&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ymmword&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ptr&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rax&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rdi&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;128&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ymm1&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;vmovdqu&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ymmword&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ptr&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rax&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rdi&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;160&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ymm2&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;vmovdqu&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ymmword&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ptr&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rax&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rdi&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;192&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ymm3&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;vmovdqu&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ymmword&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ptr&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rax&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rdi&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;224&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ymm4&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;add&lt;/span&gt;     &lt;span class=&quot;n&quot;&gt;rdi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;64&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;add&lt;/span&gt;     &lt;span class=&quot;n&quot;&gt;rsi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;jne&lt;/span&gt;     &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;LBB1_6&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The loop as been unrolled 8x, and this is basically as fast as you’ll get, approaching one vector (8 elements) per cycle in the L1 cache (limited by one store per cycle&lt;sup id=&quot;fnref:align&quot;&gt;&lt;a href=&quot;#fn:align&quot; class=&quot;footnote&quot;&gt;12&lt;/a&gt;&lt;/sup&gt;).&lt;/p&gt;

&lt;p&gt;No vectorization happens for the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;uint8_t&lt;/code&gt; case, because need to recalculate &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;size()&lt;/code&gt; check for loop termination in between every element completely inhibits it. So the ultimate cause is the same as the first case we looked at, but the impact is much bigger.&lt;/p&gt;

&lt;p&gt;Auto-vectorization explains all the very fast results: gcc only autovectorizes at &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-O3&lt;/code&gt; but clang does it at &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-O2&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-O3&lt;/code&gt; by default. The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-O3&lt;/code&gt; result for gcc is somewhat slower than clang because it does not unroll the autovectorized loop.&lt;/p&gt;

&lt;h3 id=&quot;fixing-it&quot;&gt;Fixing it&lt;/h3&gt;

&lt;p&gt;So now that we know what the problem is, how do we fix it?&lt;/p&gt;

&lt;p&gt;First, we’ll try one that doesn’t work – using a more idiomatic iterator-based loop, like this:&lt;/p&gt;

&lt;div class=&quot;language-c++ highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;auto&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;v&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;begin&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;!=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;v&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;end&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;++&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;++&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;It generates somewhat better code, compared to the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;size()&lt;/code&gt; version, at &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;gcc -O2&lt;/code&gt;:&lt;/p&gt;

&lt;div class=&quot;language-nasm highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;L17&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;add&lt;/span&gt;     &lt;span class=&quot;n&quot;&gt;BYTE&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;PTR&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[rax],&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;add&lt;/span&gt;     &lt;span class=&quot;n&quot;&gt;rax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;cmp&lt;/span&gt;     &lt;span class=&quot;n&quot;&gt;QWORD&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;PTR&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rdi&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;8&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rax&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;jne&lt;/span&gt;     &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;L17&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Two extra reads have turned into one, because you just compare to the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;end&lt;/code&gt; pointer in the vector, rather than recalculate &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;size()&lt;/code&gt; from both the begin and end pointers. It actually ties the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;uint32_t&lt;/code&gt; version in instruction count since the extra load was folded into the comparison. However, the underlying problem is still there and auto-vectorization is inhibited, so there is still a very large gap between &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;uint8_t&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;uint32_t&lt;/code&gt; - more than 5x for both gcc and clang for build where auto-vectorization is allowed.&lt;/p&gt;

&lt;p&gt;Let’s try again. Again, we will fail or rather perhaps &lt;em&gt;succeed&lt;/em&gt; in finding &lt;em&gt;another&lt;/em&gt; way that doesn’t work.&lt;/p&gt;

&lt;p&gt;This is the one where we calculate &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;size()&lt;/code&gt; only once, before the loop, and stash it in a local variable:&lt;/p&gt;

&lt;div class=&quot;language-c++ highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;size_t&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;s&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;v&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;++&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;v&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;++&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;It seems like this should work, right? The problem was &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;size()&lt;/code&gt; and now we are saying “freeze &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;size()&lt;/code&gt; into a local &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;s&lt;/code&gt; at the start of the loop, and the compiler knows locals cannot overlap with anything”. Basically we manually hoist the thing the compiler couldn’t. It actually does generate better code (compared to the original version):&lt;/p&gt;

&lt;div class=&quot;language-c++ highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;L9&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;mov&lt;/span&gt;     &lt;span class=&quot;n&quot;&gt;rdx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;QWORD&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;PTR&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rdi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;add&lt;/span&gt;     &lt;span class=&quot;n&quot;&gt;BYTE&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;PTR&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rdx&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;add&lt;/span&gt;     &lt;span class=&quot;n&quot;&gt;rax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;cmp&lt;/span&gt;     &lt;span class=&quot;n&quot;&gt;rax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rcx&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;jne&lt;/span&gt;     &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;L9&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;There is only one extra read and no &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sub&lt;/code&gt;. So what is that extra read (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;rdx, QWORD PTR [rdi]&lt;/code&gt;) doing if it isn’t part of calculating the size? It’s loading the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;data()&lt;/code&gt; pointer from &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;v&lt;/code&gt;!&lt;/p&gt;

&lt;p&gt;Effectively, the implementation of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;v[i]&lt;/code&gt; is &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;*(v.data() + i)&lt;/code&gt; and the member underlying &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;data()&lt;/code&gt; (which is fact just the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;begin&lt;/code&gt; pointer) has the same potential problem as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;size()&lt;/code&gt;. I just didn’t really notice it the original example because we got that read “for free” since we were loading it anyways to get the size.&lt;/p&gt;

&lt;p&gt;We’re almost there, I promise. We just need to remove the dependence on &lt;em&gt;anything&lt;/em&gt; stored inside the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;std::vector&lt;/code&gt; in the loop. This is easiest if we modify the idiomatic iterator based approach like this:&lt;/p&gt;

&lt;div class=&quot;language-c++ highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;auto&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;v&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;begin&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;e&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;v&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;end&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;!=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;e&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;++&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;++&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The results are dramatic (here we compare only &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;uint8_t&lt;/code&gt; loops, with and without saving the end iterator before the loop):&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Compiler&lt;/th&gt;
      &lt;th&gt;Loop Check&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;-O1&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;-O2&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;-O3&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;gcc 8&lt;/td&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;i != v.end()&lt;/code&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;1.3&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;1.3&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;1.3&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;gcc 8&lt;/td&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;i != e&lt;/code&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;1.3&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;1.3&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.06&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;gcc speedup&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;1x&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;1x&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;&lt;strong&gt;20x&lt;/strong&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;clang 8&lt;/td&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;i != v.end()&lt;/code&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;29.9&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;1.3&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;1.3&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;clang 8&lt;/td&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;i != e&lt;/code&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;20.3&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.06&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.06&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;clang speedup&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;1.5x&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;&lt;strong&gt;20x&lt;/strong&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;&lt;strong&gt;20x&lt;/strong&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;After this small change, the speedup is a factor of &lt;em&gt;twenty&lt;/em&gt; for the cases where vectorization is able to kick in. We didn’t just tie the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;uint32_t&lt;/code&gt; case either: for gcc &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-O3&lt;/code&gt; and clang &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-O2&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-O3&lt;/code&gt; the speedup of the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;uint8_t&lt;/code&gt; code over &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;uint32_t&lt;/code&gt; is almost exactly 4x, as we originally expected after vectorization: after all, 4 times as many elements are handled per vector, and we need 4 times less bandwidth to whatever level of cache is involved&lt;sup id=&quot;fnref:evenbetter&quot;&gt;&lt;a href=&quot;#fn:evenbetter&quot; class=&quot;footnote&quot;&gt;13&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;p&gt;Some of you who’ve gotten this far have probably screaming, nearly since the start:&lt;/p&gt;

&lt;p&gt;&lt;em&gt;What about the &lt;a href=&quot;https://en.cppreference.com/w/cpp/language/range-for&quot;&gt;range-based for loop&lt;/a&gt; introduced in C++11&lt;/em&gt;?&lt;/p&gt;

&lt;p&gt;Good news! It works fine. It is syntactic sugar for almost exactly the iterator-based version above, with the end pointer captured once before the loop, and so it performs equivalently.&lt;/p&gt;

&lt;p&gt;At the opposite end of the scale from modern C++, if I had just summoned my inner caveman and written a C-like function, we would have also been fine:&lt;/p&gt;

&lt;div class=&quot;language-c++ highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;array_inc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;uint8_t&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;size_t&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;size_t&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;++&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;++&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Here, the array pointer &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;a&lt;/code&gt; and the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;size&lt;/code&gt; are by-value function parameters, so they cannot be be modified by any writes through &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;a&lt;/code&gt;&lt;sup id=&quot;fnref:arrayin&quot;&gt;&lt;a href=&quot;#fn:arrayin&quot; class=&quot;footnote&quot;&gt;14&lt;/a&gt;&lt;/sup&gt;, just like local variables. It performs equivalently to the other successful options.&lt;/p&gt;

&lt;p&gt;Finally, for compilers that offer it, you can use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;__restrict&lt;/code&gt; on the vector declaration, like this&lt;sup id=&quot;fnref:residx&quot;&gt;&lt;a href=&quot;#fn:residx&quot; class=&quot;footnote&quot;&gt;15&lt;/a&gt;&lt;/sup&gt;:&lt;/p&gt;

&lt;div class=&quot;language-c++ highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;vector8_inc_restrict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vector&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;uint8_t&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;amp;&lt;/span&gt; &lt;span class=&quot;kr&quot;&gt;__restrict&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;v&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;size_t&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;v&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;++&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;v&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;++&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;__restrict&lt;/code&gt; keyword is not standard C++, but it is in &lt;a href=&quot;https://en.cppreference.com/w/c/language/restrict&quot;&gt;C since C99&lt;/a&gt; (as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;restrict&lt;/code&gt;). For those compilers that implement it as an extension in C++, you can reasonably expect that it follows the C semantics. Of course, C doesn’t have references at all - so you can mentally substitute the vector reference with a pointer to a vector.&lt;/p&gt;

&lt;p&gt;Note that restrict is not &lt;em&gt;transitive:&lt;/em&gt; the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;__restrict&lt;/code&gt; qualifier on the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;std::vector&lt;/code&gt; reference doesn’t apply restrict to the heap storage backing &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;v.data()&lt;/code&gt;, it only applies to the members of vector itself. In this case, that is enough because (as we saw above with local variables), proving to the compiler that the vector start/finish members themselves don’t overlap anything is enough. The distinction is still relevant: it means that writes to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;v.data()&lt;/code&gt; may still cause aliasing problems for &lt;em&gt;other&lt;/em&gt; objects in your function.&lt;/p&gt;

&lt;h3 id=&quot;disappointment&quot;&gt;Disappointment&lt;/h3&gt;

&lt;p&gt;That brings us to our last, and perhaps most disappointing note: all of the ways we’ve fixed the function above really only apply directly to this specific case of a vector interfering with itself. We’ve fixed it by hoisting or isolating the state backing the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;size()&lt;/code&gt; or &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;end()&lt;/code&gt; calls in the vector, and &lt;em&gt;not&lt;/em&gt; by teaching the compiler that writes to the vector data don’t modify anything. That doesn’t scale well as our function grows.&lt;/p&gt;

&lt;p&gt;The aliasing loophole is still there, and those writes still alias “anything” - it’s just that there isn’t anything else interesting to alias in this function … yet. As soon as you add more to the function, it &lt;em&gt;bytes&lt;/em&gt; us again. A &lt;a href=&quot;https://godbolt.org/z/iQHDY5&quot;&gt;random example&lt;/a&gt;. You’ll be fighting the compiler to the end as long as you have writes to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;uint8_t&lt;/code&gt; arrays in your core loops&lt;sup id=&quot;fnref:extreme&quot;&gt;&lt;a href=&quot;#fn:extreme&quot; class=&quot;footnote&quot;&gt;16&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;h3 id=&quot;comments&quot;&gt;Comments&lt;/h3&gt;

&lt;p&gt;Feedback of any type is welcome. I don’t have a comments system&lt;sup id=&quot;fnref:comments&quot;&gt;&lt;a href=&quot;#fn:comments&quot; class=&quot;footnote&quot;&gt;17&lt;/a&gt;&lt;/sup&gt; yet, so as usual I’ll outsource discussion to &lt;a href=&quot;https://news.ycombinator.com/item?id=20800076&quot;&gt;this HackerNews thread&lt;/a&gt;.&lt;/p&gt;

&lt;p class=&quot;info&quot;&gt;If you liked this post, check out the &lt;a href=&quot;/&quot;&gt;homepage&lt;/a&gt; for others you might enjoy&lt;sup id=&quot;fnref:noenjoy&quot;&gt;&lt;a href=&quot;#fn:noenjoy&quot; class=&quot;footnote&quot;&gt;18&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;hr /&gt;
&lt;hr /&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;div class=&quot;footnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:thrumem&quot;&gt;
      &lt;p&gt;&lt;em&gt;Through memory&lt;/em&gt; here means that the dependency chain is through memory: writes to the same location need to read the last value written and hence are dependent (in practice, store-to-load forwarding will be involved if the writes are close in time). There are other ways that &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;add&lt;/code&gt; to memory might be dependent, such as through the addressing calculation, but these don’t apply here. &lt;a href=&quot;#fnref:thrumem&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:looponly&quot;&gt;
      &lt;p&gt;Only the core loop is shown, the setup code is simple and fast. &lt;a href=&quot;https://godbolt.org/z/pZ5FKz&quot;&gt;Plug it into godbolt&lt;/a&gt; if you want to see everything. &lt;a href=&quot;#fnref:looponly&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:unrolled&quot;&gt;
      &lt;p&gt;Is &lt;em&gt;not-unrolled&lt;/em&gt; a double negative? Should it simply be a &lt;em&gt;rolled&lt;/em&gt; loop? Anyway, gcc generally doesn’t unroll loops even at &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-O2&lt;/code&gt; or &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-O3&lt;/code&gt; except in some special cases like small &lt;em&gt;compile-time known&lt;/em&gt; iteration counts. This often hurts it benchmarks versus clang, but helps a lot for code size. gcc will unroll loops if you use profile guided optimization, or you can request it on the command using &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-funroll-loops&lt;/code&gt;. &lt;a href=&quot;#fnref:unrolled&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:incbad&quot;&gt;
      &lt;p&gt;Actually gcc using &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;inc DWORD PTR [rax]&lt;/code&gt; is a missed optimization: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;add [rax], 1&lt;/code&gt; is almost always better since it is only 2 uops in the fused domain, compared to 3 for &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;inc&lt;/code&gt;. In this case it makes only a difference of about 6%, but with a slightly unrolled loop where the store was the only repeated element it could make a bigger difference (for a loop unrolled even more it would probably stop mattering as you hit the 1 store/cycle limit, not any total &lt;abbr title=&quot;Micro-operation: instructions are translated into one or more uops, which are simple operations executed by the CPU's execution units.&quot;&gt;uop&lt;/abbr&gt; limit). &lt;a href=&quot;#fnref:incbad&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:induc&quot;&gt;
      &lt;p&gt;I call it the &lt;em&gt;induction&lt;/em&gt; variable rather than simply &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;i&lt;/code&gt; as in the source, because the compiler has transformed single increments of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;i&lt;/code&gt; in the original code into 4-byte increments of a pointer stored in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;rax&lt;/code&gt; and adjusted the loop condition appropriately. Basically our vector-indexing loop has been re-written as a pointer/iterator incrementing one, a form of &lt;em&gt;strength reduction&lt;/em&gt;. &lt;a href=&quot;#fnref:induc&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:gccunroll&quot;&gt;
      &lt;p&gt;In fact, if you add &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-funroll-loops&lt;/code&gt; you get to 1.08 cycles per element with gcc’s 8x unroll. Even with this flag, gcc &lt;em&gt;doesn’t&lt;/em&gt; unroll the 8-bit version, so the gap between the two grows a lot! &lt;a href=&quot;#fnref:gccunroll&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:realnames&quot;&gt;
      &lt;p&gt;These are private members and their names will be implementation specific, but they aren’t actually called &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;start&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;finish&lt;/code&gt; in stdlibc++ as used by gcc, rather they are &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;_Vector_base::_Vector_impl::_M_start&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;_Vector_base::_Vector_impl::_M_finish&lt;/code&gt;, i.e., they live in a structure &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;_Vector_impl&lt;/code&gt; which is a member &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;_M_impl&lt;/code&gt; (in fact the only member) of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;_Vector_base&lt;/code&gt;, which is the base class of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;std::vector&lt;/code&gt;. Whew! Luckily the compiler removes all this abstraction just fine. &lt;a href=&quot;#fnref:realnames&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:ptype&quot;&gt;
      &lt;p&gt;The standard makes no guarantees about what the internal types of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;std::vector&lt;/code&gt; members will be, but in libstdc++’s implementation they are simply &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Alloc::pointer&lt;/code&gt; where &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Alloc&lt;/code&gt; is the allocator for the vector, which for the default &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;std::allocated&lt;/code&gt; is simply &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;T*&lt;/code&gt;, i.e., a raw pointer to object, in this case &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;uint32_t *&lt;/code&gt;. &lt;a href=&quot;#fnref:ptype&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:commoncom&quot;&gt;
      &lt;p&gt;The caveat &lt;em&gt;at least for common compilers&lt;/em&gt; appears because it seems to be the case that &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;uint8_t&lt;/code&gt; could be treated as a type distinct from &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;char&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;signed char&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;unsigned char&lt;/code&gt;. Because the aliasing loophole only applies to &lt;em&gt;character types&lt;/em&gt;, this would presumably mean that the rule wouldn’t apply to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;uint8_t&lt;/code&gt; and so they would behave like any other non-char type. No compiler I’m aware of actually implement this: they all &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;typedef&lt;/code&gt; &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;uint8_t&lt;/code&gt; to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;unsigned char&lt;/code&gt;, so the distinction is invisible to the compiler even if it wanted to act on it. &lt;a href=&quot;#fnref:commoncom&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:unp&quot;&gt;
      &lt;p&gt;&lt;em&gt;Unknown provenance&lt;/em&gt; here just means that the compiler doesn’t know where the memory points to or how it was created. Arbitrary pointers passed in to a function qualify, global and static member variables qualify. Local variables, on the other hand, have a known storage location that doesn’t overlap with anything else, so those are usually safe from aliasing issues (assuming you don’t allow the local to escape somehow). Pointers created from &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;malloc&lt;/code&gt; or &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;new&lt;/code&gt; calls that the compiler can &lt;em&gt;see&lt;/em&gt; at the point of use behave a lot like local variables, at least for some compilers: the compiler knows these calls return regions of memory that don’t overlap with anything else. Usually the compiler cannot see the originating &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;malloc&lt;/code&gt; or &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;new&lt;/code&gt; call, however. &lt;a href=&quot;#fnref:unp&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:inside&quot;&gt;
      &lt;p&gt;Maybe it is &lt;em&gt;possible&lt;/em&gt; for &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;std::vector&lt;/code&gt;? E.g., create a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;std::vector&amp;lt;uint8_t&amp;gt; a&lt;/code&gt; of a suitable size, then placement-new a new vector &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;b&lt;/code&gt; &lt;em&gt;inside&lt;/em&gt; the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;a.data()&lt;/code&gt; array, at a suitably aligned location. Then &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;std::swap(a, b)&lt;/code&gt; which will just swap the underlying storage, so now &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;b&lt;/code&gt; lives inside its own storage? Or skip the swap and use the move constructor directly when placement-new constructing &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;b&lt;/code&gt;. Such a vector has no practical purpose and will break when you do pretty much anything (e.g., adding an element may expand the vector), destroying the original backing storage and hence the vector itself. &lt;a href=&quot;#fnref:inside&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:align&quot;&gt;
      &lt;p&gt;It only gets 8 element per cycle suitably aligned: i.e., aligned by 32. In this test my &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;std::vector&lt;/code&gt; instances were lucky enough to get that alignment by default. &lt;a href=&quot;#fnref:align&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:evenbetter&quot;&gt;
      &lt;p&gt;In fact, you could get a better than 4x speedup in the case that the smaller element case fit in a inner cache level but the larger one didn’t. That is actually the case here, with the 8-bit case fitting in the L1 cache, but the 32-bit case being more than 2x as large as the L1 - but there is no additional speedup because the L2 cache is apparently able to sustain the required bandwidth. &lt;a href=&quot;#fnref:evenbetter&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:arrayin&quot;&gt;
      &lt;p&gt;Although a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;vector&lt;/code&gt; can conceptually live inside its own heap storage, the same is not true of a pointer: a write through a pointer cannot modify the pointer. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;vector&lt;/code&gt; has an extra level of indirection which makes this inception-style stuff possible. &lt;a href=&quot;#fnref:arrayin&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:residx&quot;&gt;
      &lt;p&gt;It is show here with the indexing based approach &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;v[i]&lt;/code&gt; but it works with the iterator-based approach as well. &lt;a href=&quot;#fnref:residx&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:extreme&quot;&gt;
      &lt;p&gt;There are more extreme solutions out there, e.g., you can create an &lt;em&gt;opaque typedef&lt;/em&gt;, which is basically just a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;struct&lt;/code&gt; wrapping a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;uint8_t&lt;/code&gt; value, combined with operator overloading and perhaps implicit conversions to make it act like a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;uint8_t&lt;/code&gt; while being a totally different type as far as the compiler is concerned. Surprisingly, while this &lt;a href=&quot;https://godbolt.org/z/IhyTTM&quot;&gt;works great in clang, it doesn’t work at all gcc&lt;/a&gt;, producing the same code as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;uint8_t&lt;/code&gt;. Somewhere in the guts of its optimizer, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;gcc&lt;/code&gt; ends up treating this type as a character type along with all the aliasing implications. Another extreme solution would be to create your own &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;vector&lt;/code&gt;-alike type, but whose pointer member(s) are declared &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;__restrict&lt;/code&gt;. &lt;a href=&quot;#fnref:extreme&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:comments&quot;&gt;
      &lt;p&gt;If anyone has a recommendation or a if anyone knows of a comments system that works with static sites, and which is not Disqus, has no ads, is free and fast, and lets me own the comment data (or at least export it in a reasonable format), I am all ears. &lt;a href=&quot;#fnref:comments&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:noenjoy&quot;&gt;
      &lt;p&gt;On the other hand, if you didn’t enjoy it, I have to wonder how you made it this far! Maybe you started off liking it, then felt invested and compelled to finish it? Beware the sunk cost fallacy! &lt;a href=&quot;#fnref:noenjoy&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><author><name>Travis Downs</name><email>travis.downs@gmail.com</email></author><category term="blog" /><category term="performance" /><category term="c++" /><summary type="html">This article is also available in Russian.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/assets/vector-inc/cpp_logo.png" /><media:content medium="image" url="http://localhost:4000/assets/vector-inc/cpp_logo.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Where do interrupts happen?</title><link href="http://localhost:4000/blog/2019/08/20/interrupts.html" rel="alternate" type="text/html" title="Where do interrupts happen?" /><published>2019-08-20T00:00:00+02:00</published><updated>2019-08-20T00:00:00+02:00</updated><id>http://localhost:4000/blog/2019/08/20/interrupts</id><content type="html" xml:base="http://localhost:4000/blog/2019/08/20/interrupts.html">&lt;style&gt;
.perf-annotate {
  white-space: pre-wrap;
  word-wrap: break-word;
  font-family: monospace;
  background-color: Gainsboro;
  margin-bottom: 20px;
}
&lt;/style&gt;

&lt;p&gt;On Twitter, Paul Khuong &lt;a href=&quot;https://twitter.com/pkhuong/status/1162832557030948864&quot;&gt;asks&lt;/a&gt;: &lt;em&gt;Has anyone done a study on the distribution of interrupt points in &lt;abbr title=&quot;Out-of-order execution allows CPUs to execute instructions out of order with respect to the source.&quot;&gt;OOO&lt;/abbr&gt; processors?&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Personally, I’m not aware of any such study for modern x86, and I have also wondered the same thing. In particular, when a CPU receives an externally triggered&lt;sup id=&quot;fnref:triggered&quot;&gt;&lt;a href=&quot;#fn:triggered&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt; interrupt, at what point in the instruction stream is the CPU interrupted?&lt;/p&gt;

&lt;p&gt;For a simple 1-wide in-order, non-pipelined CPU the answer might be as simple as: the CPU is interrupted either before or after instruction that is currently running&lt;sup id=&quot;fnref:twoc&quot;&gt;&lt;a href=&quot;#fn:twoc&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;. For anything more complicated it’s not going to be easy. On a modern &lt;abbr title=&quot;Out-of-order execution allows CPUs to execute instructions out of order with respect to the source.&quot;&gt;out-of-order&lt;/abbr&gt; processor there may be hundreds of instructions in-flight at any time, some waiting to execute, a dozen or more currently executing, and others waiting to retire. From all these choices, which instruction will be chosen as the victim?&lt;/p&gt;

&lt;p&gt;Among other reasons, the answer is interesting because it helps us understand how useful the exact interrupt position is when profiling via interrupt: can we extract useful information from the instruction position, or should we only trust it at a higher level (e.g., over regions of say 100s of instructions).&lt;/p&gt;

&lt;p&gt;So let’s go figure out how interruption works, at least on my Skylake i7-6700HQ, by compiling a bunch of small pure-asm programs and running them. The source for all the tests is available in the associated &lt;a href=&quot;https://github.com/travisdowns/interrupt-test&quot;&gt;git repo&lt;/a&gt; so you can follow along or write your own tests. All the tests are written in assembly because we want full control over the instructions and because they are all short and simple. In any case, we can’t avoid assembly-level analysis when talking about what instructions get interrupted.&lt;/p&gt;

&lt;p&gt;First, let’s take a look at some asm that doesn’t have any instruction that sticks out in any way at all, just a bunch of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mov&lt;/code&gt; instructions&lt;sup id=&quot;fnref:whymov&quot;&gt;&lt;a href=&quot;#fn:whymov&quot; class=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt;. The key part of the &lt;a href=&quot;https://github.com/travisdowns/interrupt-test/blob/master/indep-mov.asm&quot;&gt;source&lt;/a&gt; looks like this&lt;sup id=&quot;fnref:srcnotes&quot;&gt;&lt;a href=&quot;#fn:srcnotes&quot; class=&quot;footnote&quot;&gt;4&lt;/a&gt;&lt;/sup&gt;:&lt;/p&gt;

&lt;div class=&quot;language-nasm highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;loop&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;

&lt;span class=&quot;cp&quot;&gt;%rep&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;mov&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;eax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;mov&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;ebx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;mov&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;edi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;mov&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;edx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;mov&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;r8d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;mov&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;r9d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;6&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;mov&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;r10d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;7&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;mov&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;r11d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;8&lt;/span&gt;
&lt;span class=&quot;cp&quot;&gt;%endrep&lt;/span&gt;

	&lt;span class=&quot;k&quot;&gt;dec&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rcx&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;jne&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;loop&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Just constant moves into registers, 8 of them repeated 10 times. This code executes with an expected and measured&lt;sup id=&quot;fnref:ipc&quot;&gt;&lt;a href=&quot;#fn:ipc&quot; class=&quot;footnote&quot;&gt;5&lt;/a&gt;&lt;/sup&gt; &lt;abbr title=&quot;Instructions per cycle: calculated over an interval by measuring the number of instructions executed and the duration in cycles.&quot;&gt;IPC&lt;/abbr&gt; of 4.&lt;/p&gt;

&lt;p&gt;Next, we get to the meat of the investigation. We run the binary using &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;perf record -e task-clock ./indep-mov&lt;/code&gt;, which will periodically interrupt the process and record the &lt;abbr title=&quot;Instruction pointer&quot;&gt;IP&lt;/abbr&gt;. Next, we examine the interrupted locations with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;perf report&lt;/code&gt;&lt;sup id=&quot;fnref:acommand&quot;&gt;&lt;a href=&quot;#fn:acommand&quot; class=&quot;footnote&quot;&gt;6&lt;/a&gt;&lt;/sup&gt;. Here’s the output (hereafter, I’m going to cut out the header and just show the samples):&lt;/p&gt;

&lt;div class=&quot;perf-annotate&quot;&gt;
 Samples |	Source code &amp;amp; Disassembly of indep-mov for task-clock (1769 samples, percent: local period)
-----------------------------------------------------------------------------------------------------------
         :
         :            Disassembly of section .text:
         :
         :            00000000004000ae &amp;lt;_start.loop&amp;gt;:
         :            _start.loop():
         :            indep-mov.asm:15
&lt;span style=&quot;color:green;&quot;&gt;      16&lt;/span&gt; : &lt;span style=&quot;color:purple;&quot;&gt;  4000ae:&lt;/span&gt;&lt;span style=&quot;color:blue;&quot;&gt;       mov    eax,0x1&lt;/span&gt;
&lt;span style=&quot;color:green;&quot;&gt;      15&lt;/span&gt; : &lt;span style=&quot;color:purple;&quot;&gt;  4000b3:&lt;/span&gt;&lt;span style=&quot;color:blue;&quot;&gt;       mov    ebx,0x2&lt;/span&gt;
&lt;span style=&quot;color:green;&quot;&gt;      22&lt;/span&gt; : &lt;span style=&quot;color:purple;&quot;&gt;  4000b8:&lt;/span&gt;&lt;span style=&quot;color:blue;&quot;&gt;       mov    edi,0x3&lt;/span&gt;
&lt;span style=&quot;color:green;&quot;&gt;      25&lt;/span&gt; : &lt;span style=&quot;color:purple;&quot;&gt;  4000bd:&lt;/span&gt;&lt;span style=&quot;color:blue;&quot;&gt;       mov    edx,0x4&lt;/span&gt;
&lt;span style=&quot;color:green;&quot;&gt;      14&lt;/span&gt; : &lt;span style=&quot;color:purple;&quot;&gt;  4000c2:&lt;/span&gt;&lt;span style=&quot;color:blue;&quot;&gt;       mov    r8d,0x5&lt;/span&gt;
&lt;span style=&quot;color:green;&quot;&gt;      19&lt;/span&gt; : &lt;span style=&quot;color:purple;&quot;&gt;  4000c8:&lt;/span&gt;&lt;span style=&quot;color:blue;&quot;&gt;       mov    r9d,0x6&lt;/span&gt;
&lt;span style=&quot;color:green;&quot;&gt;      25&lt;/span&gt; : &lt;span style=&quot;color:purple;&quot;&gt;  4000ce:&lt;/span&gt;&lt;span style=&quot;color:blue;&quot;&gt;       mov    r10d,0x7&lt;/span&gt;
&lt;span style=&quot;color:green;&quot;&gt;      18&lt;/span&gt; : &lt;span style=&quot;color:purple;&quot;&gt;  4000d4:&lt;/span&gt;&lt;span style=&quot;color:blue;&quot;&gt;       mov    r11d,0x8&lt;/span&gt;
&lt;span style=&quot;color:green;&quot;&gt;      22&lt;/span&gt; : &lt;span style=&quot;color:purple;&quot;&gt;  4000da:&lt;/span&gt;&lt;span style=&quot;color:blue;&quot;&gt;       mov    eax,0x1&lt;/span&gt;
&lt;span style=&quot;color:green;&quot;&gt;      24&lt;/span&gt; : &lt;span style=&quot;color:purple;&quot;&gt;  4000df:&lt;/span&gt;&lt;span style=&quot;color:blue;&quot;&gt;       mov    ebx,0x2&lt;/span&gt;
&lt;span style=&quot;color:green;&quot;&gt;      20&lt;/span&gt; : &lt;span style=&quot;color:purple;&quot;&gt;  4000e4:&lt;/span&gt;&lt;span style=&quot;color:blue;&quot;&gt;       mov    edi,0x3&lt;/span&gt;
&lt;span style=&quot;color:green;&quot;&gt;      29&lt;/span&gt; : &lt;span style=&quot;color:purple;&quot;&gt;  4000e9:&lt;/span&gt;&lt;span style=&quot;color:blue;&quot;&gt;       mov    edx,0x4&lt;/span&gt;
&lt;span style=&quot;color:green;&quot;&gt;      28&lt;/span&gt; : &lt;span style=&quot;color:purple;&quot;&gt;  4000ee:&lt;/span&gt;&lt;span style=&quot;color:blue;&quot;&gt;       mov    r8d,0x5&lt;/span&gt;
&lt;span style=&quot;color:green;&quot;&gt;      18&lt;/span&gt; : &lt;span style=&quot;color:purple;&quot;&gt;  4000f4:&lt;/span&gt;&lt;span style=&quot;color:blue;&quot;&gt;       mov    r9d,0x6&lt;/span&gt;
&lt;span style=&quot;color:green;&quot;&gt;      21&lt;/span&gt; : &lt;span style=&quot;color:purple;&quot;&gt;  4000fa:&lt;/span&gt;&lt;span style=&quot;color:blue;&quot;&gt;       mov    r10d,0x7&lt;/span&gt;
&lt;span style=&quot;color:green;&quot;&gt;      19&lt;/span&gt; : &lt;span style=&quot;color:purple;&quot;&gt;  400100:&lt;/span&gt;&lt;span style=&quot;color:blue;&quot;&gt;       mov    r11d,0x8&lt;/span&gt;
&lt;span style=&quot;color:green;&quot;&gt;      26&lt;/span&gt; : &lt;span style=&quot;color:purple;&quot;&gt;  400106:&lt;/span&gt;&lt;span style=&quot;color:blue;&quot;&gt;       mov    eax,0x1&lt;/span&gt;
&lt;span style=&quot;color:green;&quot;&gt;      18&lt;/span&gt; : &lt;span style=&quot;color:purple;&quot;&gt;  40010b:&lt;/span&gt;&lt;span style=&quot;color:blue;&quot;&gt;       mov    ebx,0x2&lt;/span&gt;
&lt;span style=&quot;color:green;&quot;&gt;      29&lt;/span&gt; : &lt;span style=&quot;color:purple;&quot;&gt;  400110:&lt;/span&gt;&lt;span style=&quot;color:blue;&quot;&gt;       mov    edi,0x3&lt;/span&gt;
&lt;span style=&quot;color:green;&quot;&gt;      19&lt;/span&gt; : &lt;span style=&quot;color:purple;&quot;&gt;  400115:&lt;/span&gt;&lt;span style=&quot;color:blue;&quot;&gt;       mov    edx,0x4&lt;/span&gt;
&lt;/div&gt;

&lt;p&gt;The first column shows the number of interrupts received for each instruction. Specially, the number of times an instruction would be the next instruction to execute following the interrupt.&lt;/p&gt;

&lt;p&gt;Without doing any deep statistical analysis, I don’t see any particular pattern here. Every instruction gets its time in the sun. Some columns have somewhat higher values than others, but if you repeat the measurements, the columns with higher values don’t necessarily repeat.&lt;/p&gt;

&lt;p&gt;We can try the exact same thing, but with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;add&lt;/code&gt; instructions &lt;a href=&quot;https://github.com/travisdowns/interrupt-test/blob/master/indep-add.asm&quot;&gt;like this&lt;/a&gt;:&lt;/p&gt;

&lt;div class=&quot;language-nasm highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;	&lt;span class=&quot;k&quot;&gt;add&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;eax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;add&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;ebx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;add&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;edi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;add&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;edx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;add&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;r8d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;add&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;r9d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;6&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;add&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;r10d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;7&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;add&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;r11d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;8&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We expect the execution behavior to be similar to the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mov&lt;/code&gt; case: we &lt;em&gt;do&lt;/em&gt; have dependency chains here but 8 separate ones (for each destination register) for a 1 cycle instruction so there should be little practical impact. Indeed, the results are basically identical to the last experiment so I won’t show them here (you can see them yourself with the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;indep-add&lt;/code&gt; test).&lt;/p&gt;

&lt;p&gt;Let’s get moving here and try something more interesting. This time we will again use all &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;add&lt;/code&gt; instructions, but two of the adds will depend on each other, while the other two will be independent. So the chain shared by those two adds will be twice as long (2 cycles) as the other chains (1 cycle each). &lt;a href=&quot;https://github.com/travisdowns/interrupt-test/blob/master/add-2-1-1.asm&quot;&gt;Like this&lt;/a&gt;:&lt;/p&gt;

&lt;div class=&quot;language-nasm highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;    &lt;span class=&quot;k&quot;&gt;add&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;rax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;; 2-cycle chain&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;add&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;rax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;; 2-cycle chain&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;add&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;rsi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;add&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;rdi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Here the chain through &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;rax&lt;/code&gt; should limit the throughput of the above repeated block to 1 per 2 cycles, and indeed I measure an &lt;abbr title=&quot;Instructions per cycle: calculated over an interval by measuring the number of instructions executed and the duration in cycles.&quot;&gt;IPC&lt;/abbr&gt; of 2 (4 instructions / 2 cycles = 2 &lt;abbr title=&quot;Instructions per cycle: calculated over an interval by measuring the number of instructions executed and the duration in cycles.&quot;&gt;IPC&lt;/abbr&gt;).&lt;/p&gt;

&lt;p&gt;Here’s the interrupt distribution:&lt;/p&gt;
&lt;div class=&quot;perf-annotate&quot;&gt;
       0 : &lt;span style=&quot;color:purple;&quot;&gt;  4000ae:&lt;/span&gt;&lt;span style=&quot;color:blue;&quot;&gt;       add    rax,0x1&lt;/span&gt;
&lt;span style=&quot;color:green;&quot;&gt;      82&lt;/span&gt; : &lt;span style=&quot;color:purple;&quot;&gt;  4000b2:&lt;/span&gt;&lt;span style=&quot;color:blue;&quot;&gt;       add    rax,0x2&lt;/span&gt;
&lt;span style=&quot;color:green;&quot;&gt;     112&lt;/span&gt; : &lt;span style=&quot;color:purple;&quot;&gt;  4000b6:&lt;/span&gt;&lt;span style=&quot;color:blue;&quot;&gt;       add    rsi,0x3&lt;/span&gt;
       0 : &lt;span style=&quot;color:purple;&quot;&gt;  4000ba:&lt;/span&gt;&lt;span style=&quot;color:blue;&quot;&gt;       add    rdi,0x4&lt;/span&gt;
       0 : &lt;span style=&quot;color:purple;&quot;&gt;  4000be:&lt;/span&gt;&lt;span style=&quot;color:blue;&quot;&gt;       add    rax,0x1&lt;/span&gt;
&lt;span style=&quot;color:green;&quot;&gt;      45&lt;/span&gt; : &lt;span style=&quot;color:purple;&quot;&gt;  4000c2:&lt;/span&gt;&lt;span style=&quot;color:blue;&quot;&gt;       add    rax,0x2&lt;/span&gt;
&lt;span style=&quot;color:green;&quot;&gt;     144&lt;/span&gt; : &lt;span style=&quot;color:purple;&quot;&gt;  4000c6:&lt;/span&gt;&lt;span style=&quot;color:blue;&quot;&gt;       add    rsi,0x3&lt;/span&gt;
       0 : &lt;span style=&quot;color:purple;&quot;&gt;  4000ca:&lt;/span&gt;&lt;span style=&quot;color:blue;&quot;&gt;       add    rdi,0x4&lt;/span&gt;
       0 : &lt;span style=&quot;color:purple;&quot;&gt;  4000ce:&lt;/span&gt;&lt;span style=&quot;color:blue;&quot;&gt;       add    rax,0x1&lt;/span&gt;
&lt;span style=&quot;color:green;&quot;&gt;      44&lt;/span&gt; : &lt;span style=&quot;color:purple;&quot;&gt;  4000d2:&lt;/span&gt;&lt;span style=&quot;color:blue;&quot;&gt;       add    rax,0x2&lt;/span&gt;
&lt;span style=&quot;color:green;&quot;&gt;     107&lt;/span&gt; : &lt;span style=&quot;color:purple;&quot;&gt;  4000d6:&lt;/span&gt;&lt;span style=&quot;color:blue;&quot;&gt;       add    rsi,0x3&lt;/span&gt;

(pattern repeats...)
&lt;/div&gt;

&lt;p&gt;This is certainly something new. We see that &lt;em&gt;all&lt;/em&gt; the interrupts fall on the middle two instructions, one of which is part of the addition chain and one which is not. The second of the two locations also gets about 2-3 times as many interrupts as the first.&lt;/p&gt;

&lt;h3 id=&quot;a-hypothesis&quot;&gt;A Hypothesis&lt;/h3&gt;

&lt;p&gt;Let’s make a hypothesis now so we can design more tests.&lt;/p&gt;

&lt;p&gt;Let’s guess that interrupts &lt;em&gt;select&lt;/em&gt; instructions which are the oldest unretired instruction, and that this &lt;em&gt;selected&lt;/em&gt; instruction is allowed to complete hence samples fall on the next instruction (let us call this next instruction the &lt;em&gt;sampled&lt;/em&gt; instruction). I am making the distinction between &lt;em&gt;selected&lt;/em&gt; and &lt;em&gt;sampled&lt;/em&gt; instructions rather than just saying “interrupts sample instructions that follow the oldest unretired instruction” because we are going to build our model almost entirely around the &lt;em&gt;selected&lt;/em&gt; instructions, so we want to name them. The characteristics of the ultimately sampled instructions (except their positioning after &lt;em&gt;selected&lt;/em&gt; instructions) hardly matters&lt;sup id=&quot;fnref:untrue&quot;&gt;&lt;a href=&quot;#fn:untrue&quot; class=&quot;footnote&quot;&gt;7&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;p&gt;Without a more detailed model of instruction retirement, we can’t yet explain everything we see - but the basic idea is instructions that take longer, hence are more likely to be the oldest unretired instruction, are the ones that get sampled. In particular, if there is a critical dependency chain, instructions in that chain are likely&lt;sup id=&quot;fnref:likely&quot;&gt;&lt;a href=&quot;#fn:likely&quot; class=&quot;footnote&quot;&gt;8&lt;/a&gt;&lt;/sup&gt; be sampled at some point&lt;sup id=&quot;fnref:notonly&quot;&gt;&lt;a href=&quot;#fn:notonly&quot; class=&quot;footnote&quot;&gt;9&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;p&gt;Let’s take a look at some more examples. I’m going to switch using &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mov rax, [rax]&lt;/code&gt; as my long latency instruction (4 cycles latency) and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;nop&lt;/code&gt; as the filler instruction not part of any chain. Don’t worry, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;nop&lt;/code&gt; has to allocate and retire just like any other instruction: it simply gets to skip execution. You can build all these examples with a real instruction like &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;add&lt;/code&gt; and they’ll work in the same way&lt;sup id=&quot;fnref:whynop&quot;&gt;&lt;a href=&quot;#fn:whynop&quot; class=&quot;footnote&quot;&gt;10&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;p&gt;Let’s &lt;a href=&quot;https://github.com/travisdowns/interrupt-test/blob/master/load-nop10.asm&quot;&gt;take a look&lt;/a&gt; at a load followed by 10 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;nops&lt;/code&gt;:&lt;/p&gt;

&lt;div class=&quot;language-nasm highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;loop&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;

&lt;span class=&quot;cp&quot;&gt;%rep&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;mov&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;rax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[rax]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;times&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;nop&lt;/span&gt;
&lt;span class=&quot;cp&quot;&gt;%endrep&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;dec&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rcx&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;jne&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;loop&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The result:&lt;/p&gt;

&lt;div class=&quot;perf-annotate&quot;&gt;
       0 : &lt;span style=&quot;color:purple;&quot;&gt;  4000ba:&lt;/span&gt;&lt;span style=&quot;color:blue;&quot;&gt;       mov    rax,QWORD PTR [rax]&lt;/span&gt;
&lt;span style=&quot;color:red;&quot;&gt;      33&lt;/span&gt; : &lt;span style=&quot;color:purple;&quot;&gt;  4000bd:&lt;/span&gt;&lt;span style=&quot;color:blue;&quot;&gt;       nop&lt;/span&gt;
       0 : &lt;span style=&quot;color:purple;&quot;&gt;  4000be:&lt;/span&gt;&lt;span style=&quot;color:blue;&quot;&gt;       nop&lt;/span&gt;
       0 : &lt;span style=&quot;color:purple;&quot;&gt;  4000bf:&lt;/span&gt;&lt;span style=&quot;color:blue;&quot;&gt;       nop&lt;/span&gt;
       0 : &lt;span style=&quot;color:purple;&quot;&gt;  4000c0:&lt;/span&gt;&lt;span style=&quot;color:blue;&quot;&gt;       nop&lt;/span&gt;
&lt;span style=&quot;color:green;&quot;&gt;      11&lt;/span&gt; : &lt;span style=&quot;color:purple;&quot;&gt;  4000c1:&lt;/span&gt;&lt;span style=&quot;color:blue;&quot;&gt;       nop&lt;/span&gt;
       0 : &lt;span style=&quot;color:purple;&quot;&gt;  4000c2:&lt;/span&gt;&lt;span style=&quot;color:blue;&quot;&gt;       nop&lt;/span&gt;
       0 : &lt;span style=&quot;color:purple;&quot;&gt;  4000c3:&lt;/span&gt;&lt;span style=&quot;color:blue;&quot;&gt;       nop&lt;/span&gt;
       0 : &lt;span style=&quot;color:purple;&quot;&gt;  4000c4:&lt;/span&gt;&lt;span style=&quot;color:blue;&quot;&gt;       nop&lt;/span&gt;
&lt;span style=&quot;color:green;&quot;&gt;      22&lt;/span&gt; : &lt;span style=&quot;color:purple;&quot;&gt;  4000c5:&lt;/span&gt;&lt;span style=&quot;color:blue;&quot;&gt;       nop&lt;/span&gt;
       0 : &lt;span style=&quot;color:purple;&quot;&gt;  4000c6:&lt;/span&gt;&lt;span style=&quot;color:blue;&quot;&gt;       nop&lt;/span&gt;
       0 : &lt;span style=&quot;color:purple;&quot;&gt;  4000c7:&lt;/span&gt;&lt;span style=&quot;color:blue;&quot;&gt;       mov    rax,QWORD PTR [rax]&lt;/span&gt;
&lt;span style=&quot;color:green;&quot;&gt;      15&lt;/span&gt; : &lt;span style=&quot;color:purple;&quot;&gt;  4000ca:&lt;/span&gt;&lt;span style=&quot;color:blue;&quot;&gt;       nop&lt;/span&gt;
       0 : &lt;span style=&quot;color:purple;&quot;&gt;  4000cb:&lt;/span&gt;&lt;span style=&quot;color:blue;&quot;&gt;       nop&lt;/span&gt;
       0 : &lt;span style=&quot;color:purple;&quot;&gt;  4000cc:&lt;/span&gt;&lt;span style=&quot;color:blue;&quot;&gt;       nop&lt;/span&gt;
       0 : &lt;span style=&quot;color:purple;&quot;&gt;  4000cd:&lt;/span&gt;&lt;span style=&quot;color:blue;&quot;&gt;       nop&lt;/span&gt;
&lt;span style=&quot;color:green;&quot;&gt;      13&lt;/span&gt; : &lt;span style=&quot;color:purple;&quot;&gt;  4000ce:&lt;/span&gt;&lt;span style=&quot;color:blue;&quot;&gt;       nop&lt;/span&gt;
       1 : &lt;span style=&quot;color:purple;&quot;&gt;  4000cf:&lt;/span&gt;&lt;span style=&quot;color:blue;&quot;&gt;       nop&lt;/span&gt;
       0 : &lt;span style=&quot;color:purple;&quot;&gt;  4000d0:&lt;/span&gt;&lt;span style=&quot;color:blue;&quot;&gt;       nop&lt;/span&gt;
       0 : &lt;span style=&quot;color:purple;&quot;&gt;  4000d1:&lt;/span&gt;&lt;span style=&quot;color:blue;&quot;&gt;       nop&lt;/span&gt;
&lt;span style=&quot;color:red;&quot;&gt;      35&lt;/span&gt; : &lt;span style=&quot;color:purple;&quot;&gt;  4000d2:&lt;/span&gt;&lt;span style=&quot;color:blue;&quot;&gt;       nop&lt;/span&gt;
       0 : &lt;span style=&quot;color:purple;&quot;&gt;  4000d3:&lt;/span&gt;&lt;span style=&quot;color:blue;&quot;&gt;       nop&lt;/span&gt;
       0 : &lt;span style=&quot;color:purple;&quot;&gt;  4000d4:&lt;/span&gt;&lt;span style=&quot;color:blue;&quot;&gt;       mov    rax,QWORD PTR [rax]&lt;/span&gt;
&lt;span style=&quot;color:green;&quot;&gt;      16&lt;/span&gt; : &lt;span style=&quot;color:purple;&quot;&gt;  4000d7:&lt;/span&gt;&lt;span style=&quot;color:blue;&quot;&gt;       nop&lt;/span&gt;
       0 : &lt;span style=&quot;color:purple;&quot;&gt;  4000d8:&lt;/span&gt;&lt;span style=&quot;color:blue;&quot;&gt;       nop&lt;/span&gt;
       0 : &lt;span style=&quot;color:purple;&quot;&gt;  4000d9:&lt;/span&gt;&lt;span style=&quot;color:blue;&quot;&gt;       nop&lt;/span&gt;
       0 : &lt;span style=&quot;color:purple;&quot;&gt;  4000da:&lt;/span&gt;&lt;span style=&quot;color:blue;&quot;&gt;       nop&lt;/span&gt;
&lt;span style=&quot;color:green;&quot;&gt;      14&lt;/span&gt; : &lt;span style=&quot;color:purple;&quot;&gt;  4000db:&lt;/span&gt;&lt;span style=&quot;color:blue;&quot;&gt;       nop&lt;/span&gt;
       0 : &lt;span style=&quot;color:purple;&quot;&gt;  4000dc:&lt;/span&gt;&lt;span style=&quot;color:blue;&quot;&gt;       nop&lt;/span&gt;
       0 : &lt;span style=&quot;color:purple;&quot;&gt;  4000dd:&lt;/span&gt;&lt;span style=&quot;color:blue;&quot;&gt;       nop&lt;/span&gt;
       0 : &lt;span style=&quot;color:purple;&quot;&gt;  4000de:&lt;/span&gt;&lt;span style=&quot;color:blue;&quot;&gt;       nop&lt;/span&gt;
&lt;span style=&quot;color:red;&quot;&gt;      31&lt;/span&gt; : &lt;span style=&quot;color:purple;&quot;&gt;  4000df:&lt;/span&gt;&lt;span style=&quot;color:blue;&quot;&gt;       nop&lt;/span&gt;
       0 : &lt;span style=&quot;color:purple;&quot;&gt;  4000e0:&lt;/span&gt;&lt;span style=&quot;color:blue;&quot;&gt;       nop&lt;/span&gt;
       0 : &lt;span style=&quot;color:purple;&quot;&gt;  4000e1:&lt;/span&gt;&lt;span style=&quot;color:blue;&quot;&gt;       mov    rax,QWORD PTR [rax]&lt;/span&gt;
&lt;span style=&quot;color:green;&quot;&gt;      22&lt;/span&gt; : &lt;span style=&quot;color:purple;&quot;&gt;  4000e4:&lt;/span&gt;&lt;span style=&quot;color:blue;&quot;&gt;       nop&lt;/span&gt;
       0 : &lt;span style=&quot;color:purple;&quot;&gt;  4000e5:&lt;/span&gt;&lt;span style=&quot;color:blue;&quot;&gt;       nop&lt;/span&gt;
       0 : &lt;span style=&quot;color:purple;&quot;&gt;  4000e6:&lt;/span&gt;&lt;span style=&quot;color:blue;&quot;&gt;       nop&lt;/span&gt;
       0 : &lt;span style=&quot;color:purple;&quot;&gt;  4000e7:&lt;/span&gt;&lt;span style=&quot;color:blue;&quot;&gt;       nop&lt;/span&gt;
&lt;span style=&quot;color:green;&quot;&gt;      16&lt;/span&gt; : &lt;span style=&quot;color:purple;&quot;&gt;  4000e8:&lt;/span&gt;&lt;span style=&quot;color:blue;&quot;&gt;       nop&lt;/span&gt;
       0 : &lt;span style=&quot;color:purple;&quot;&gt;  4000e9:&lt;/span&gt;&lt;span style=&quot;color:blue;&quot;&gt;       nop&lt;/span&gt;
       0 : &lt;span style=&quot;color:purple;&quot;&gt;  4000ea:&lt;/span&gt;&lt;span style=&quot;color:blue;&quot;&gt;       nop&lt;/span&gt;
       0 : &lt;span style=&quot;color:purple;&quot;&gt;  4000eb:&lt;/span&gt;&lt;span style=&quot;color:blue;&quot;&gt;       nop&lt;/span&gt;
&lt;span style=&quot;color:green;&quot;&gt;      24&lt;/span&gt; : &lt;span style=&quot;color:purple;&quot;&gt;  4000ec:&lt;/span&gt;&lt;span style=&quot;color:blue;&quot;&gt;       nop&lt;/span&gt;
       0 : &lt;span style=&quot;color:purple;&quot;&gt;  4000ed:&lt;/span&gt;&lt;span style=&quot;color:blue;&quot;&gt;       nop&lt;/span&gt;
&lt;/div&gt;

&lt;p&gt;The &lt;em&gt;selected&lt;/em&gt; instructions are the long-latency &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mov&lt;/code&gt;-chain, but also two specific &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;nop&lt;/code&gt; instructions out of the 10 that follow: those which fall 4 and 8 instructions after the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mov&lt;/code&gt;. Here we can see the impact of retirement throughput. Although the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mov&lt;/code&gt;-chain is the only thing that contributes to &lt;em&gt;execution&lt;/em&gt; latency, this Skylake CPU can &lt;a href=&quot;https://www.realworldtech.com/forum/?threadid=161978&amp;amp;curpostid=162222&quot;&gt;only retire&lt;/a&gt; up to 4 instructions per cycle (per thread). So when the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mov&lt;/code&gt; finally retires, there will be two cycles of retiring blocks of 4 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;nop&lt;/code&gt; instructions before we get to the next &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mov&lt;/code&gt;:&lt;/p&gt;

&lt;div class=&quot;language-nasm highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;;                            retire cycle&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;mov&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;rax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;QWORD&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;PTR&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[rax]&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;; 0 (execution limited)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;nop&lt;/span&gt;                        &lt;span class=&quot;c&quot;&gt;; 0&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;nop&lt;/span&gt;                        &lt;span class=&quot;c&quot;&gt;; 0&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;nop&lt;/span&gt;                        &lt;span class=&quot;c&quot;&gt;; 0&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;nop&lt;/span&gt;                        &lt;span class=&quot;c&quot;&gt;; 1 &amp;lt;-- selected nop&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;nop&lt;/span&gt;                        &lt;span class=&quot;c&quot;&gt;; 1&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;nop&lt;/span&gt;                        &lt;span class=&quot;c&quot;&gt;; 1&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;nop&lt;/span&gt;                        &lt;span class=&quot;c&quot;&gt;; 1&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;nop&lt;/span&gt;                        &lt;span class=&quot;c&quot;&gt;; 2 &amp;lt;-- selected nop&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;nop&lt;/span&gt;                        &lt;span class=&quot;c&quot;&gt;; 2&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;nop&lt;/span&gt;                        &lt;span class=&quot;c&quot;&gt;; 2&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;mov&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;rax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;QWORD&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;PTR&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[rax]&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;; 4 (execution limited)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;nop&lt;/span&gt;                        &lt;span class=&quot;c&quot;&gt;; 4&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;nop&lt;/span&gt;                        &lt;span class=&quot;c&quot;&gt;; 4&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;In this example, the retirement of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mov&lt;/code&gt; instructions are “execution limited” - i.e., their retirement cycle is determined by when they are done executing, not by any details of the retirement engine. The retirement of the other instructions, on the other hand, is determined by the retirement behavior: they are “ready” early but cannot retire because the in-order retirement pointer hasn’t reached them yet (it is held up waiting for the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mov&lt;/code&gt; to execute).&lt;/p&gt;

&lt;p&gt;So those selected &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;nop&lt;/code&gt; instructions aren’t particularly special: they aren’t slower than the rest or causing any bottleneck. They are simply selected because the pattern of retirement following the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mov&lt;/code&gt; is predictable. Note also that it is the &lt;em&gt;first&lt;/em&gt; &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;nop&lt;/code&gt; that in the group of 4 that would otherwise retire that is selected&lt;sup id=&quot;fnref:already&quot;&gt;&lt;a href=&quot;#fn:already&quot; class=&quot;footnote&quot;&gt;11&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;p&gt;This means that we can construct &lt;a href=&quot;https://github.com/travisdowns/interrupt-test/blob/master/load-add2.asm&quot;&gt;an example&lt;/a&gt; where an instruction on the critical path never gets selected:&lt;/p&gt;

&lt;div class=&quot;language-nasm highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;cp&quot;&gt;%rep&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;mov&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;rax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[rax]&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;nop&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;nop&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;nop&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;nop&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;nop&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;add&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;rax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
&lt;span class=&quot;cp&quot;&gt;%endrep&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The results:&lt;/p&gt;

&lt;div class=&quot;perf-annotate&quot;&gt;
       0 : &lt;span style=&quot;color:purple;&quot;&gt;  4000ba:&lt;/span&gt;&lt;span style=&quot;color:blue;&quot;&gt;       mov    rax,QWORD PTR [rax]&lt;/span&gt;
&lt;span style=&quot;color:red;&quot;&gt;      94&lt;/span&gt; : &lt;span style=&quot;color:purple;&quot;&gt;  4000bd:&lt;/span&gt;&lt;span style=&quot;color:blue;&quot;&gt;       nop&lt;/span&gt;
       0 : &lt;span style=&quot;color:purple;&quot;&gt;  4000be:&lt;/span&gt;&lt;span style=&quot;color:blue;&quot;&gt;       nop&lt;/span&gt;
       0 : &lt;span style=&quot;color:purple;&quot;&gt;  4000bf:&lt;/span&gt;&lt;span style=&quot;color:blue;&quot;&gt;       nop&lt;/span&gt;
       0 : &lt;span style=&quot;color:purple;&quot;&gt;  4000c0:&lt;/span&gt;&lt;span style=&quot;color:blue;&quot;&gt;       nop&lt;/span&gt;
&lt;span style=&quot;color:green;&quot;&gt;      17&lt;/span&gt; : &lt;span style=&quot;color:purple;&quot;&gt;  4000c1:&lt;/span&gt;&lt;span style=&quot;color:blue;&quot;&gt;       nop&lt;/span&gt;
       0 : &lt;span style=&quot;color:purple;&quot;&gt;  4000c2:&lt;/span&gt;&lt;span style=&quot;color:blue;&quot;&gt;       add    rax,0x0&lt;/span&gt;
       0 : &lt;span style=&quot;color:purple;&quot;&gt;  4000c6:&lt;/span&gt;&lt;span style=&quot;color:blue;&quot;&gt;       mov    rax,QWORD PTR [rax]&lt;/span&gt;
&lt;span style=&quot;color:red;&quot;&gt;      78&lt;/span&gt; : &lt;span style=&quot;color:purple;&quot;&gt;  4000c9:&lt;/span&gt;&lt;span style=&quot;color:blue;&quot;&gt;       nop&lt;/span&gt;
       0 : &lt;span style=&quot;color:purple;&quot;&gt;  4000ca:&lt;/span&gt;&lt;span style=&quot;color:blue;&quot;&gt;       nop&lt;/span&gt;
       0 : &lt;span style=&quot;color:purple;&quot;&gt;  4000cb:&lt;/span&gt;&lt;span style=&quot;color:blue;&quot;&gt;       nop&lt;/span&gt;
       0 : &lt;span style=&quot;color:purple;&quot;&gt;  4000cc:&lt;/span&gt;&lt;span style=&quot;color:blue;&quot;&gt;       nop&lt;/span&gt;
&lt;span style=&quot;color:green;&quot;&gt;      18&lt;/span&gt; : &lt;span style=&quot;color:purple;&quot;&gt;  4000cd:&lt;/span&gt;&lt;span style=&quot;color:blue;&quot;&gt;       nop&lt;/span&gt;
       0 : &lt;span style=&quot;color:purple;&quot;&gt;  4000ce:&lt;/span&gt;&lt;span style=&quot;color:blue;&quot;&gt;       add    rax,0x0&lt;/span&gt;
&lt;/div&gt;

&lt;p&gt;The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;add&lt;/code&gt; instruction is on the critical path: it increases the execution time of the block from 4 cycles to 6 cycles&lt;sup id=&quot;fnref:4to6&quot;&gt;&lt;a href=&quot;#fn:4to6&quot; class=&quot;footnote&quot;&gt;12&lt;/a&gt;&lt;/sup&gt;, yet it is never selected. The retirement pattern looks like:
&lt;a name=&quot;lookhere&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;language-nasm highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;                        &lt;span class=&quot;o&quot;&gt;/-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;scheduled&lt;/span&gt;
                       &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;  &lt;span class=&quot;o&quot;&gt;/-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ready&lt;/span&gt;
                       &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;  &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;  &lt;span class=&quot;o&quot;&gt;/-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;complete&lt;/span&gt;
                       &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;  &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;  &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;  &lt;span class=&quot;o&quot;&gt;/-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;retired&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;mov&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;rax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[rax]&lt;/span&gt;  &lt;span class=&quot;c&quot;&gt;; 0  0  5  5 &amp;lt;-- selected&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;nop&lt;/span&gt;              &lt;span class=&quot;c&quot;&gt;; 0  0  0  5 &amp;lt;-- sample&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;nop&lt;/span&gt;              &lt;span class=&quot;c&quot;&gt;; 0  0  0  5&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;nop&lt;/span&gt;              &lt;span class=&quot;c&quot;&gt;; 0  0  0  5&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;nop&lt;/span&gt;              &lt;span class=&quot;c&quot;&gt;; 1  1  1  6 &amp;lt;-- selected&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;nop&lt;/span&gt;              &lt;span class=&quot;c&quot;&gt;; 1  1  1  6 &amp;lt;-- sampled&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;add&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;rax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;      &lt;span class=&quot;c&quot;&gt;; 1  5  6  6&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;mov&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;rax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[rax]&lt;/span&gt;  &lt;span class=&quot;c&quot;&gt;; 1  6 11 11 &amp;lt;-- selected&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;nop&lt;/span&gt;              &lt;span class=&quot;c&quot;&gt;; 2  2  2 11 &amp;lt;-- sampled&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;nop&lt;/span&gt;              &lt;span class=&quot;c&quot;&gt;; 2  2  2 11&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;nop&lt;/span&gt;              &lt;span class=&quot;c&quot;&gt;; 2  2  2 11&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;nop&lt;/span&gt;              &lt;span class=&quot;c&quot;&gt;; 2  2  2 12 &amp;lt;-- selected&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;nop&lt;/span&gt;              &lt;span class=&quot;c&quot;&gt;; 3  3  3 12 &amp;lt;-- sampled&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;add&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;rax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;      &lt;span class=&quot;c&quot;&gt;; 3 11 12 12&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;mov&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;rax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[rax]&lt;/span&gt;  &lt;span class=&quot;c&quot;&gt;; 3 12 17 17 &amp;lt;-- selected&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;nop&lt;/span&gt;              &lt;span class=&quot;c&quot;&gt;; 3  3  3 17 &amp;lt;-- sampled&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;On the right hand side, I’ve annotated each instruction with several key cycle values, described below.&lt;/p&gt;

&lt;p&gt;The &lt;em&gt;scheduled&lt;/em&gt; column indicates when the instruction enters the scheduler and hence could execute if all its dependencies were met. This column is very simple: we assume that there are no front-end bottlenecks and hence we schedule (aka “allocate”) 4 instructions every cycle. This part is in-order: instructions enter the scheduler in program order.&lt;/p&gt;

&lt;p&gt;The &lt;em&gt;ready&lt;/em&gt; column indicates when all dependencies of a scheduled instruction have executed and hence the instruction is ready to execute. In this simple model, an instruction always begins executing when it is ready. A more complicated model would also need to model contention for execution ports, but here we don’t have any such contention. Instruction readiness occurs &lt;em&gt;out of order&lt;/em&gt;: you can see that many instructions become ready before older instructions (e.g., the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;nop&lt;/code&gt; instructions are generally ready before the preceding &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mov&lt;/code&gt; or &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;add&lt;/code&gt; instructions). To calculate this column take the maximum of the &lt;em&gt;ready&lt;/em&gt; column for this instruction and the &lt;em&gt;completed&lt;/em&gt; column for all previous instructions whose outputs are inputs to this instruction.&lt;/p&gt;

&lt;p&gt;The &lt;em&gt;complete&lt;/em&gt; column indicates when an instruction finishes execution. In this model it simply takes the value of the &lt;em&gt;ready&lt;/em&gt; column plus the instruction latency, which is 0 for the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;nop&lt;/code&gt; instructions (they don’t execute at all, so they have 0 effective latency), 1 for the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;add&lt;/code&gt; instruction and 5 for the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mov&lt;/code&gt;. Like the &lt;em&gt;ready&lt;/em&gt; column this happens out of order.&lt;/p&gt;

&lt;p&gt;Finally, the &lt;em&gt;retired&lt;/em&gt; column, what we’re really after, shows when the instruction retires.  The rule is fairly simple: an instruction cannot retire until it is complete, and the instruction before it must be retired or retiring on this cycle. No more than 4 instructions can retire in a cycle. As a consequence of the “previous instruction must retired” part, this column is only increasing and so like the first column, retirement is &lt;em&gt;in order&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Once we have the &lt;em&gt;retired&lt;/em&gt; column filled out, we can identify the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&amp;lt;-- selected&lt;/code&gt; instructions&lt;sup id=&quot;fnref:possibly&quot;&gt;&lt;a href=&quot;#fn:possibly&quot; class=&quot;footnote&quot;&gt;13&lt;/a&gt;&lt;/sup&gt;: they are the ones where the retirement cycle increases. In this case, selected instructions are always either the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mov&lt;/code&gt; instruction (because of its long latency, it holds up retirement), or the fourth &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;nop&lt;/code&gt; after the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mov&lt;/code&gt; (because of the “only retire 4 per cycle” rule, this &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;nop&lt;/code&gt; is at the head of the group that retires in the cycle after the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mov&lt;/code&gt; retires). Finally, the &lt;em&gt;sampled&lt;/em&gt; instructions which are those will actually show up in the interrupt report are simply the instruction following each selected instruction.&lt;/p&gt;

&lt;p&gt;Here, the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;add&lt;/code&gt; is never selected because it executes in the cycle after the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mov&lt;/code&gt;, so it is eligible for retirement in the next cycle and hence doesn’t slow down retirement and so doesn’t behave much differently than a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;nop&lt;/code&gt; for the purposes of retirement. We can change the position of the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;add&lt;/code&gt; slightly, so it falls in the same 4-instruction retirement window as the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mov&lt;/code&gt;, &lt;a href=&quot;https://github.com/travisdowns/interrupt-test/blob/master/load-add3.asm&quot;&gt;like this&lt;/a&gt;:&lt;/p&gt;

&lt;div class=&quot;language-nasm highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;cp&quot;&gt;%rep&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;mov&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;rax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[rax]&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;nop&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;nop&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;add&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;rax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;nop&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;nop&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;nop&lt;/span&gt;
&lt;span class=&quot;cp&quot;&gt;%endrep&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We’ve only slid the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;add&lt;/code&gt; up a few places. The number of instructions is the same and this block executes in 6 cycles, identical to the last example. However, the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;add&lt;/code&gt; instruction now always gets selected:&lt;/p&gt;

&lt;div class=&quot;perf-annotate&quot;&gt;
&lt;span style=&quot;color:green;&quot;&gt;      21&lt;/span&gt; : &lt;span style=&quot;color:purple;&quot;&gt;  4000ba:&lt;/span&gt;&lt;span style=&quot;color:blue;&quot;&gt;       mov    rax,QWORD PTR [rax]&lt;/span&gt;
&lt;span style=&quot;color:red;&quot;&gt;      54&lt;/span&gt; : &lt;span style=&quot;color:purple;&quot;&gt;  4000bd:&lt;/span&gt;&lt;span style=&quot;color:blue;&quot;&gt;       nop&lt;/span&gt;
       0 : &lt;span style=&quot;color:purple;&quot;&gt;  4000be:&lt;/span&gt;&lt;span style=&quot;color:blue;&quot;&gt;       nop&lt;/span&gt;
       0 : &lt;span style=&quot;color:purple;&quot;&gt;  4000bf:&lt;/span&gt;&lt;span style=&quot;color:blue;&quot;&gt;       add    rax,0x0&lt;/span&gt;
&lt;span style=&quot;color:green;&quot;&gt;      15&lt;/span&gt; : &lt;span style=&quot;color:purple;&quot;&gt;  4000c3:&lt;/span&gt;&lt;span style=&quot;color:blue;&quot;&gt;       nop&lt;/span&gt;
       0 : &lt;span style=&quot;color:purple;&quot;&gt;  4000c4:&lt;/span&gt;&lt;span style=&quot;color:blue;&quot;&gt;       nop&lt;/span&gt;
       0 : &lt;span style=&quot;color:purple;&quot;&gt;  4000c5:&lt;/span&gt;&lt;span style=&quot;color:blue;&quot;&gt;       nop&lt;/span&gt;
       0 : &lt;span style=&quot;color:purple;&quot;&gt;  4000c6:&lt;/span&gt;&lt;span style=&quot;color:blue;&quot;&gt;       mov    rax,QWORD PTR [rax]&lt;/span&gt;
&lt;span style=&quot;color:red;&quot;&gt;      88&lt;/span&gt; : &lt;span style=&quot;color:purple;&quot;&gt;  4000c9:&lt;/span&gt;&lt;span style=&quot;color:blue;&quot;&gt;       nop&lt;/span&gt;
       0 : &lt;span style=&quot;color:purple;&quot;&gt;  4000ca:&lt;/span&gt;&lt;span style=&quot;color:blue;&quot;&gt;       nop&lt;/span&gt;
       0 : &lt;span style=&quot;color:purple;&quot;&gt;  4000cb:&lt;/span&gt;&lt;span style=&quot;color:blue;&quot;&gt;       add    rax,0x0&lt;/span&gt;
&lt;span style=&quot;color:green;&quot;&gt;      14&lt;/span&gt; : &lt;span style=&quot;color:purple;&quot;&gt;  4000cf:&lt;/span&gt;&lt;span style=&quot;color:blue;&quot;&gt;       nop&lt;/span&gt;
       0 : &lt;span style=&quot;color:purple;&quot;&gt;  4000d0:&lt;/span&gt;&lt;span style=&quot;color:blue;&quot;&gt;       nop&lt;/span&gt;
       0 : &lt;span style=&quot;color:purple;&quot;&gt;  4000d1:&lt;/span&gt;&lt;span style=&quot;color:blue;&quot;&gt;       nop&lt;/span&gt;
       0 : &lt;span style=&quot;color:purple;&quot;&gt;  4000d2:&lt;/span&gt;&lt;span style=&quot;color:blue;&quot;&gt;       mov    rax,QWORD PTR [rax]&lt;/span&gt;
&lt;span style=&quot;color:red;&quot;&gt;      91&lt;/span&gt; : &lt;span style=&quot;color:purple;&quot;&gt;  4000d5:&lt;/span&gt;&lt;span style=&quot;color:blue;&quot;&gt;       nop&lt;/span&gt;
       0 : &lt;span style=&quot;color:purple;&quot;&gt;  4000d6:&lt;/span&gt;&lt;span style=&quot;color:blue;&quot;&gt;       nop&lt;/span&gt;
       0 : &lt;span style=&quot;color:purple;&quot;&gt;  4000d7:&lt;/span&gt;&lt;span style=&quot;color:blue;&quot;&gt;       add    rax,0x0&lt;/span&gt;
&lt;span style=&quot;color:green;&quot;&gt;      13&lt;/span&gt; : &lt;span style=&quot;color:purple;&quot;&gt;  4000db:&lt;/span&gt;&lt;span style=&quot;color:blue;&quot;&gt;       nop&lt;/span&gt;
&lt;/div&gt;

&lt;p&gt;Here’s the cycle analysis and retirement pattern for this version:&lt;/p&gt;

&lt;div class=&quot;language-nasm highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;                        &lt;span class=&quot;o&quot;&gt;/-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;scheduled&lt;/span&gt;
                       &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;  &lt;span class=&quot;o&quot;&gt;/-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ready&lt;/span&gt;
                       &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;  &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;  &lt;span class=&quot;o&quot;&gt;/-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;complete&lt;/span&gt;
                       &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;  &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;  &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;  &lt;span class=&quot;o&quot;&gt;/-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;retired&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;mov&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;rax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[rax]&lt;/span&gt;  &lt;span class=&quot;c&quot;&gt;; 0  0  5  5 &amp;lt;-- selected&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;nop&lt;/span&gt;              &lt;span class=&quot;c&quot;&gt;; 0  0  0  5 &amp;lt;-- sample&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;nop&lt;/span&gt;              &lt;span class=&quot;c&quot;&gt;; 0  0  0  5&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;add&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;rax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;      &lt;span class=&quot;c&quot;&gt;; 0  5  6  6 &amp;lt;-- selected&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;nop&lt;/span&gt;              &lt;span class=&quot;c&quot;&gt;; 1  1  1  6 &amp;lt;-- sampled&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;nop&lt;/span&gt;              &lt;span class=&quot;c&quot;&gt;; 1  1  1  6&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;nop&lt;/span&gt;              &lt;span class=&quot;c&quot;&gt;; 1  1  1  6&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;mov&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;rax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[rax]&lt;/span&gt;  &lt;span class=&quot;c&quot;&gt;; 1  6 11 11 &amp;lt;-- selected&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;nop&lt;/span&gt;              &lt;span class=&quot;c&quot;&gt;; 2  2  2 11 &amp;lt;-- sampled&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;nop&lt;/span&gt;              &lt;span class=&quot;c&quot;&gt;; 2  2  2 11&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;add&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;rax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;      &lt;span class=&quot;c&quot;&gt;; 2 11 12 12 &amp;lt;-- selected&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;nop&lt;/span&gt;              &lt;span class=&quot;c&quot;&gt;; 2  2  2 12 &amp;lt;-- sampled&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;nop&lt;/span&gt;              &lt;span class=&quot;c&quot;&gt;; 3  3  3 12&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;nop&lt;/span&gt;              &lt;span class=&quot;c&quot;&gt;; 3  3  3 12&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;mov&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;rax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[rax]&lt;/span&gt;  &lt;span class=&quot;c&quot;&gt;; 3 12 17 17 &amp;lt;-- selected&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;nop&lt;/span&gt;              &lt;span class=&quot;c&quot;&gt;; 3  3  3 17 &amp;lt;-- sampled&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Now might be a good time to note that we also care about the actual sample counts, and not just their presence or absence. Here, the samples associated with the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mov&lt;/code&gt; are more frequent than the samples associated with the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;add&lt;/code&gt;. In fact, there are about 4.9 samples for &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mov&lt;/code&gt; for every sample for &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;add&lt;/code&gt; (calculated over the full results). That lines up almost exactly with the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mov&lt;/code&gt; having a latency of 5 and the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;add&lt;/code&gt; a latency of 1: the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mov&lt;/code&gt; will be the oldest unretired instruction 5 times as often as the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;add&lt;/code&gt;. So the sample counts are very meaningful in this case.&lt;/p&gt;

&lt;p&gt;Going back to the cycle charts, we know the &lt;em&gt;selected&lt;/em&gt; instructions are those where the retirement cycle increases. To that we add that the &lt;em&gt;size&lt;/em&gt; of the increase determines their &lt;em&gt;selection weight&lt;/em&gt;: the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mov&lt;/code&gt; instruction has a weight of 5, since it jumps (for example) from 6 to 11 so it is the oldest unretired instruction for 5 cycles, while the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;nop&lt;/code&gt; instructions have a weight of 1.&lt;/p&gt;

&lt;p&gt;This lets you measure &lt;em&gt;in-situ&lt;/em&gt; the latency of various instructions, as long as you have accounted for the retirement behavior. For example, measuring the following block (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;rdx&lt;/code&gt; is zero at runtime):&lt;/p&gt;

&lt;div class=&quot;language-nasm highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;    &lt;span class=&quot;k&quot;&gt;mov&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;rax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[rax]&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;mov&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;rax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rax&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rdx&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Results in samples accumulating in a 4:5 ratio for the first and second lines: reflecting the fact that the second load has a latency of 5 due to complex addressing, while the first load takes only 4 cycles.&lt;/p&gt;

&lt;h3 id=&quot;branches&quot;&gt;Branches&lt;/h3&gt;

&lt;p&gt;What about branches? I don’t find anything special about branches: whether taken or untaken they seem to retire normally and fit the pattern described above. I am not going to show the results but you can play with the &lt;a href=&quot;https://github.com/travisdowns/interrupt-test/blob/master/branches.asm&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;branches&lt;/code&gt; test&lt;/a&gt; yourself if you want.&lt;/p&gt;

&lt;h3 id=&quot;atomic-operations&quot;&gt;Atomic Operations&lt;/h3&gt;

&lt;p&gt;What about atomic operations? Here the story does get interesting.&lt;/p&gt;

&lt;p&gt;I’m going to use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;lock add QWORD [rbx], 1&lt;/code&gt; as my default atomic instruction, but the story seems similar for all of them. Alone, this instruction has a “latency”&lt;sup id=&quot;fnref:atomiclat&quot;&gt;&lt;a href=&quot;#fn:atomiclat&quot; class=&quot;footnote&quot;&gt;14&lt;/a&gt;&lt;/sup&gt; of 18 cycles. Let’s put it in parallel with a couple of &lt;abbr title=&quot;Single Instruction Multiple Data: an ISA type or ISA extension like Intel's AVX or ARM's NEON that can perform multiple identical operations on elements packed into a SIMD register.&quot;&gt;SIMD&lt;/abbr&gt; instructions that have a total latency of 20 cycles alone:&lt;/p&gt;

&lt;div class=&quot;language-nasm highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;    &lt;span class=&quot;n&quot;&gt;vpmulld&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;xmm0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;xmm0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;xmm0&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;vpmulld&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;xmm0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;xmm0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;xmm0&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;lock&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;add&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;QWORD&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[rbx],&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This loop &lt;em&gt;still takes 20 cycles&lt;/em&gt; to execute. That is, the atomic costs nothing in runtime: the performance is the same if you comment it out. The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;vpmulld&lt;/code&gt; dependency chain is long enough to hide the cost of the atomic. Let’s take a look at the interrupt distribution for this code:&lt;/p&gt;

&lt;div class=&quot;perf-annotate&quot;&gt;
       0 : &lt;span style=&quot;color:purple;&quot;&gt;  4000c8:&lt;/span&gt;&lt;span style=&quot;color:blue;&quot;&gt;       vpmulld xmm0,xmm0,xmm0&lt;/span&gt;
      12 : &lt;span style=&quot;color:purple;&quot;&gt;  4000cd:&lt;/span&gt;&lt;span style=&quot;color:blue;&quot;&gt;       vpmulld xmm0,xmm0,xmm0&lt;/span&gt;
      10 : &lt;span style=&quot;color:purple;&quot;&gt;  4000d2:&lt;/span&gt;&lt;span style=&quot;color:blue;&quot;&gt;       lock add QWORD PTR [rbx],0x1&lt;/span&gt;
&lt;span style=&quot;color:red;&quot;&gt;     244&lt;/span&gt; : &lt;span style=&quot;color:purple;&quot;&gt;  4000d7:&lt;/span&gt;&lt;span style=&quot;color:blue;&quot;&gt;       vpmulld xmm0,xmm0,xmm0&lt;/span&gt;
       0 : &lt;span style=&quot;color:purple;&quot;&gt;  4000dc:&lt;/span&gt;&lt;span style=&quot;color:blue;&quot;&gt;       vpmulld xmm0,xmm0,xmm0&lt;/span&gt;
&lt;span style=&quot;color:green;&quot;&gt;      26&lt;/span&gt; : &lt;span style=&quot;color:purple;&quot;&gt;  4000e1:&lt;/span&gt;&lt;span style=&quot;color:blue;&quot;&gt;       lock add QWORD PTR [rbx],0x1&lt;/span&gt;
&lt;span style=&quot;color:red;&quot;&gt;     299&lt;/span&gt; : &lt;span style=&quot;color:purple;&quot;&gt;  4000e6:&lt;/span&gt;&lt;span style=&quot;color:blue;&quot;&gt;       vpmulld xmm0,xmm0,xmm0&lt;/span&gt;
       0 : &lt;span style=&quot;color:purple;&quot;&gt;  4000eb:&lt;/span&gt;&lt;span style=&quot;color:blue;&quot;&gt;       vpmulld xmm0,xmm0,xmm0&lt;/span&gt;
&lt;span style=&quot;color:green;&quot;&gt;      35&lt;/span&gt; : &lt;span style=&quot;color:purple;&quot;&gt;  4000f0:&lt;/span&gt;&lt;span style=&quot;color:blue;&quot;&gt;       lock add QWORD PTR [rbx],0x1&lt;/span&gt;
&lt;span style=&quot;color:red;&quot;&gt;     277&lt;/span&gt; : &lt;span style=&quot;color:purple;&quot;&gt;  4000f5:&lt;/span&gt;&lt;span style=&quot;color:blue;&quot;&gt;       vpmulld xmm0,xmm0,xmm0&lt;/span&gt;
       0 : &lt;span style=&quot;color:purple;&quot;&gt;  4000fa:&lt;/span&gt;&lt;span style=&quot;color:blue;&quot;&gt;       vpmulld xmm0,xmm0,xmm0&lt;/span&gt;
&lt;span style=&quot;color:green;&quot;&gt;      33&lt;/span&gt; : &lt;span style=&quot;color:purple;&quot;&gt;  4000ff:&lt;/span&gt;&lt;span style=&quot;color:blue;&quot;&gt;       lock add QWORD PTR [rbx],0x1&lt;/span&gt;
&lt;span style=&quot;color:red;&quot;&gt;     302&lt;/span&gt; : &lt;span style=&quot;color:purple;&quot;&gt;  400104:&lt;/span&gt;&lt;span style=&quot;color:blue;&quot;&gt;       vpmulld xmm0,xmm0,xmm0&lt;/span&gt;
       0 : &lt;span style=&quot;color:purple;&quot;&gt;  400109:&lt;/span&gt;&lt;span style=&quot;color:blue;&quot;&gt;       vpmulld xmm0,xmm0,xmm0&lt;/span&gt;
&lt;span style=&quot;color:green;&quot;&gt;      33&lt;/span&gt; : &lt;span style=&quot;color:purple;&quot;&gt;  40010e:&lt;/span&gt;&lt;span style=&quot;color:blue;&quot;&gt;       lock add QWORD PTR [rbx],0x1&lt;/span&gt;
&lt;span style=&quot;color:red;&quot;&gt;     272&lt;/span&gt; : &lt;span style=&quot;color:purple;&quot;&gt;  400113:&lt;/span&gt;&lt;span style=&quot;color:blue;&quot;&gt;       vpmulld xmm0,xmm0,xmm0&lt;/span&gt;
       0 : &lt;span style=&quot;color:purple;&quot;&gt;  400118:&lt;/span&gt;&lt;span style=&quot;color:blue;&quot;&gt;       vpmulld xmm0,xmm0,xmm0&lt;/span&gt;
&lt;span style=&quot;color:green;&quot;&gt;      31&lt;/span&gt; : &lt;span style=&quot;color:purple;&quot;&gt;  40011d:&lt;/span&gt;&lt;span style=&quot;color:blue;&quot;&gt;       lock add QWORD PTR [rbx],0x1&lt;/span&gt;
&lt;span style=&quot;color:red;&quot;&gt;     280&lt;/span&gt; : &lt;span style=&quot;color:purple;&quot;&gt;  400122:&lt;/span&gt;&lt;span style=&quot;color:blue;&quot;&gt;       vpmulld xmm0,xmm0,xmm0&lt;/span&gt;
       0 : &lt;span style=&quot;color:purple;&quot;&gt;  400127:&lt;/span&gt;&lt;span style=&quot;color:blue;&quot;&gt;       vpmulld xmm0,xmm0,xmm0&lt;/span&gt;
&lt;span style=&quot;color:green;&quot;&gt;      40&lt;/span&gt; : &lt;span style=&quot;color:purple;&quot;&gt;  40012c:&lt;/span&gt;&lt;span style=&quot;color:blue;&quot;&gt;       lock add QWORD PTR [rbx],0x1&lt;/span&gt;
&lt;span style=&quot;color:red;&quot;&gt;     277&lt;/span&gt; : &lt;span style=&quot;color:purple;&quot;&gt;  400131:&lt;/span&gt;&lt;span style=&quot;color:blue;&quot;&gt;       vpmulld xmm0,xmm0,xmm0&lt;/span&gt;
       0 : &lt;span style=&quot;color:purple;&quot;&gt;  400136:&lt;/span&gt;&lt;span style=&quot;color:blue;&quot;&gt;       vpmulld xmm0,xmm0,xmm0&lt;/span&gt;
&lt;span style=&quot;color:green;&quot;&gt;      21&lt;/span&gt; : &lt;span style=&quot;color:purple;&quot;&gt;  40013b:&lt;/span&gt;&lt;span style=&quot;color:blue;&quot;&gt;       lock add QWORD PTR [rbx],0x1&lt;/span&gt;
&lt;span style=&quot;color:red;&quot;&gt;     282&lt;/span&gt; : &lt;span style=&quot;color:purple;&quot;&gt;  400140:&lt;/span&gt;&lt;span style=&quot;color:blue;&quot;&gt;       vpmulld xmm0,xmm0,xmm0&lt;/span&gt;
       0 : &lt;span style=&quot;color:purple;&quot;&gt;  400145:&lt;/span&gt;&lt;span style=&quot;color:blue;&quot;&gt;       vpmulld xmm0,xmm0,xmm0&lt;/span&gt;
&lt;span style=&quot;color:green;&quot;&gt;      35&lt;/span&gt; : &lt;span style=&quot;color:purple;&quot;&gt;  40014a:&lt;/span&gt;&lt;span style=&quot;color:blue;&quot;&gt;       lock add QWORD PTR [rbx],0x1&lt;/span&gt;
&lt;span style=&quot;color:red;&quot;&gt;     291&lt;/span&gt; : &lt;span style=&quot;color:purple;&quot;&gt;  40014f:&lt;/span&gt;&lt;span style=&quot;color:blue;&quot;&gt;       vpmulld xmm0,xmm0,xmm0&lt;/span&gt;
       0 : &lt;span style=&quot;color:purple;&quot;&gt;  400154:&lt;/span&gt;&lt;span style=&quot;color:blue;&quot;&gt;       vpmulld xmm0,xmm0,xmm0&lt;/span&gt;
&lt;span style=&quot;color:green;&quot;&gt;      35&lt;/span&gt; : &lt;span style=&quot;color:purple;&quot;&gt;  400159:&lt;/span&gt;&lt;span style=&quot;color:blue;&quot;&gt;       lock add QWORD PTR [rbx],0x1&lt;/span&gt;
&lt;span style=&quot;color:red;&quot;&gt;     270&lt;/span&gt; : &lt;span style=&quot;color:purple;&quot;&gt;  40015e:&lt;/span&gt;&lt;span style=&quot;color:blue;&quot;&gt;       dec    rcx&lt;/span&gt;
       0 : &lt;span style=&quot;color:purple;&quot;&gt;  400161:&lt;/span&gt;&lt;span style=&quot;color:blue;&quot;&gt;       jne    4000c8 &amp;lt;_start.loop&amp;gt;&lt;/span&gt;

&lt;/div&gt;

&lt;p&gt;The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;lock add&lt;/code&gt; instructions are selected by the interrupt close to 90% of the time, despite not contributing to the execution time. Based on our mental model, these instructions should be able to run ahead of the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;vpmulld&lt;/code&gt; loop and hence be ready to retire as soon as they are the head of the &lt;abbr title=&quot;Re-order buffer: n ordered buffer which stores in-progress instructions on an out-of-order processor.&quot;&gt;ROB&lt;/abbr&gt;. The effect that we see here is because &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;lock&lt;/code&gt;-prefixed instructions are &lt;em&gt;execute at retire&lt;/em&gt;. This is a special type of instruction that waits until it at the head of the &lt;abbr title=&quot;Re-order buffer: n ordered buffer which stores in-progress instructions on an out-of-order processor.&quot;&gt;ROB&lt;/abbr&gt; before it executes&lt;sup id=&quot;fnref:execretire&quot;&gt;&lt;a href=&quot;#fn:execretire&quot; class=&quot;footnote&quot;&gt;15&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;p&gt;So this instruction will &lt;em&gt;always&lt;/em&gt; take a certain minimum amount of time as the oldest unretired instruction: it never retires immediately regardless of the surrounding instructions. In this case, it means retirement spends most of its time waiting for the locked instructions, while execution spends most of its time waiting for the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;vpmulld&lt;/code&gt;. Note that the retirement time added by the locked instructions wasn’t additive with that from &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;vpmulld&lt;/code&gt;: the time it spends waiting for retirement is subtracted from the time that would otherwise be spent waiting on the multiplication retirement. That’s why you end up with a lopsided split, not 50/50. We can see this more clearly if we double the number of multiplications to 4:&lt;/p&gt;

&lt;div class=&quot;language-nasm highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;    &lt;span class=&quot;n&quot;&gt;vpmulld&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;xmm0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;xmm0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;xmm0&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;vpmulld&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;xmm0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;xmm0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;xmm0&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;vpmulld&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;xmm0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;xmm0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;xmm0&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;vpmulld&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;xmm0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;xmm0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;xmm0&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;lock&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;add&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;QWORD&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[rbx],&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This takes 40 cycles to execute, and the interrupt pattern looks like:&lt;/p&gt;

&lt;div class=&quot;perf-annotate&quot;&gt;
       0 : &lt;span style=&quot;color:purple;&quot;&gt;  4000c8:&lt;/span&gt;&lt;span style=&quot;color:blue;&quot;&gt;       vpmulld xmm0,xmm0,xmm0&lt;/span&gt;
      18 : &lt;span style=&quot;color:purple;&quot;&gt;  4000cd:&lt;/span&gt;&lt;span style=&quot;color:blue;&quot;&gt;       vpmulld xmm0,xmm0,xmm0&lt;/span&gt;
&lt;span style=&quot;color:green;&quot;&gt;      49&lt;/span&gt; : &lt;span style=&quot;color:purple;&quot;&gt;  4000d2:&lt;/span&gt;&lt;span style=&quot;color:blue;&quot;&gt;       vpmulld xmm0,xmm0,xmm0&lt;/span&gt;
&lt;span style=&quot;color:green;&quot;&gt;     133&lt;/span&gt; : &lt;span style=&quot;color:purple;&quot;&gt;  4000d7:&lt;/span&gt;&lt;span style=&quot;color:blue;&quot;&gt;       vpmulld xmm0,xmm0,xmm0&lt;/span&gt;
&lt;span style=&quot;color:green;&quot;&gt;     152&lt;/span&gt; : &lt;span style=&quot;color:purple;&quot;&gt;  4000dc:&lt;/span&gt;&lt;span style=&quot;color:blue;&quot;&gt;       lock add QWORD PTR [rbx],0x1&lt;/span&gt;
&lt;span style=&quot;color:green;&quot;&gt;     263&lt;/span&gt; : &lt;span style=&quot;color:purple;&quot;&gt;  4000e1:&lt;/span&gt;&lt;span style=&quot;color:blue;&quot;&gt;       vpmulld xmm0,xmm0,xmm0&lt;/span&gt;
       0 : &lt;span style=&quot;color:purple;&quot;&gt;  4000e6:&lt;/span&gt;&lt;span style=&quot;color:blue;&quot;&gt;       vpmulld xmm0,xmm0,xmm0&lt;/span&gt;
&lt;span style=&quot;color:green;&quot;&gt;      61&lt;/span&gt; : &lt;span style=&quot;color:purple;&quot;&gt;  4000eb:&lt;/span&gt;&lt;span style=&quot;color:blue;&quot;&gt;       vpmulld xmm0,xmm0,xmm0&lt;/span&gt;
&lt;span style=&quot;color:green;&quot;&gt;     160&lt;/span&gt; : &lt;span style=&quot;color:purple;&quot;&gt;  4000f0:&lt;/span&gt;&lt;span style=&quot;color:blue;&quot;&gt;       vpmulld xmm0,xmm0,xmm0&lt;/span&gt;
&lt;span style=&quot;color:green;&quot;&gt;     168&lt;/span&gt; : &lt;span style=&quot;color:purple;&quot;&gt;  4000f5:&lt;/span&gt;&lt;span style=&quot;color:blue;&quot;&gt;       lock add QWORD PTR [rbx],0x1&lt;/span&gt;
&lt;span style=&quot;color:green;&quot;&gt;     251&lt;/span&gt; : &lt;span style=&quot;color:purple;&quot;&gt;  4000fa:&lt;/span&gt;&lt;span style=&quot;color:blue;&quot;&gt;       vpmulld xmm0,xmm0,xmm0&lt;/span&gt;
       0 : &lt;span style=&quot;color:purple;&quot;&gt;  4000ff:&lt;/span&gt;&lt;span style=&quot;color:blue;&quot;&gt;       vpmulld xmm0,xmm0,xmm0&lt;/span&gt;
&lt;span style=&quot;color:green;&quot;&gt;      59&lt;/span&gt; : &lt;span style=&quot;color:purple;&quot;&gt;  400104:&lt;/span&gt;&lt;span style=&quot;color:blue;&quot;&gt;       vpmulld xmm0,xmm0,xmm0&lt;/span&gt;
&lt;span style=&quot;color:green;&quot;&gt;     166&lt;/span&gt; : &lt;span style=&quot;color:purple;&quot;&gt;  400109:&lt;/span&gt;&lt;span style=&quot;color:blue;&quot;&gt;       vpmulld xmm0,xmm0,xmm0&lt;/span&gt;
&lt;span style=&quot;color:green;&quot;&gt;     162&lt;/span&gt; : &lt;span style=&quot;color:purple;&quot;&gt;  40010e:&lt;/span&gt;&lt;span style=&quot;color:blue;&quot;&gt;       lock add QWORD PTR [rbx],0x1&lt;/span&gt;
&lt;span style=&quot;color:green;&quot;&gt;     267&lt;/span&gt; : &lt;span style=&quot;color:purple;&quot;&gt;  400113:&lt;/span&gt;&lt;span style=&quot;color:blue;&quot;&gt;       vpmulld xmm0,xmm0,xmm0&lt;/span&gt;
       0 : &lt;span style=&quot;color:purple;&quot;&gt;  400118:&lt;/span&gt;&lt;span style=&quot;color:blue;&quot;&gt;       vpmulld xmm0,xmm0,xmm0&lt;/span&gt;
&lt;span style=&quot;color:green;&quot;&gt;      60&lt;/span&gt; : &lt;span style=&quot;color:purple;&quot;&gt;  40011d:&lt;/span&gt;&lt;span style=&quot;color:blue;&quot;&gt;       vpmulld xmm0,xmm0,xmm0&lt;/span&gt;
&lt;span style=&quot;color:green;&quot;&gt;     160&lt;/span&gt; : &lt;span style=&quot;color:purple;&quot;&gt;  400122:&lt;/span&gt;&lt;span style=&quot;color:blue;&quot;&gt;       vpmulld xmm0,xmm0,xmm0&lt;/span&gt;
&lt;span style=&quot;color:green;&quot;&gt;     155&lt;/span&gt; : &lt;span style=&quot;color:purple;&quot;&gt;  400127:&lt;/span&gt;&lt;span style=&quot;color:blue;&quot;&gt;       lock add QWORD PTR [rbx],0x1&lt;/span&gt;
&lt;span style=&quot;color:green;&quot;&gt;     218&lt;/span&gt; : &lt;span style=&quot;color:purple;&quot;&gt;  40012c:&lt;/span&gt;&lt;span style=&quot;color:blue;&quot;&gt;       vpmulld xmm0,xmm0,xmm0&lt;/span&gt;
       0 : &lt;span style=&quot;color:purple;&quot;&gt;  400131:&lt;/span&gt;&lt;span style=&quot;color:blue;&quot;&gt;       vpmulld xmm0,xmm0,xmm0&lt;/span&gt;
&lt;span style=&quot;color:green;&quot;&gt;      58&lt;/span&gt; : &lt;span style=&quot;color:purple;&quot;&gt;  400136:&lt;/span&gt;&lt;span style=&quot;color:blue;&quot;&gt;       vpmulld xmm0,xmm0,xmm0&lt;/span&gt;
&lt;span style=&quot;color:green;&quot;&gt;     144&lt;/span&gt; : &lt;span style=&quot;color:purple;&quot;&gt;  40013b:&lt;/span&gt;&lt;span style=&quot;color:blue;&quot;&gt;       vpmulld xmm0,xmm0,xmm0&lt;/span&gt;
&lt;span style=&quot;color:green;&quot;&gt;     154&lt;/span&gt; : &lt;span style=&quot;color:purple;&quot;&gt;  400140:&lt;/span&gt;&lt;span style=&quot;color:blue;&quot;&gt;       lock add QWORD PTR [rbx],0x1&lt;/span&gt;

pattern continues...
&lt;/div&gt;

&lt;p&gt;The sample counts are similar for &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;lock add&lt;/code&gt; but they’ve now increased to a comparable amount (in total) for the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;vmulld&lt;/code&gt; instructions&lt;sup id=&quot;fnref:uneven&quot;&gt;&lt;a href=&quot;#fn:uneven&quot; class=&quot;footnote&quot;&gt;16&lt;/a&gt;&lt;/sup&gt;. In fact, we can calculate how long the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;lock add&lt;/code&gt; instruction takes to retire, using the ratio of the times it was selected compared to the known block throughput of 40 cycles. I get about a 38%-40% rate over a couple of runs which corresponds to a retire time of 15-16 cycles, only slightly less than then back-to-back latency&lt;sup id=&quot;fnref:atomiclat:1&quot;&gt;&lt;a href=&quot;#fn:atomiclat&quot; class=&quot;footnote&quot;&gt;14&lt;/a&gt;&lt;/sup&gt; of this instruction.&lt;/p&gt;

&lt;p&gt;Can we do anything with this information?&lt;/p&gt;

&lt;p&gt;Well one idea is that it lets us fairly precisely map out the retirement timing of instructions. For example, we can set up an instruction to test, and a parallel series of instructions with a known latency. Then we observe what is selected by interrupts: the instruction under test or the end of the known-latency chain. Whichever is selected has longer retirement latency and the known-latency chain can be adjusted to narrow it down exactly.&lt;/p&gt;

&lt;p&gt;Of course, this sounds way harder than the usual way of measuring latency: a long series of back-to-back instrucitons, but it does let us measure some things “in situ” without a long chain, and we can measure instructions that don’t have an obvious way to chain (e.g,. have no output like stores or different-domain instructions).&lt;/p&gt;

&lt;h3 id=&quot;todo&quot;&gt;TODO&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Explain the variable (non-self synchronizing) results in terms of retire window patterns&lt;/li&gt;
  &lt;li&gt;Check interruptible instructions&lt;/li&gt;
  &lt;li&gt;Check &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mfence&lt;/code&gt; and friends&lt;/li&gt;
  &lt;li&gt;Check the execution effect of atomic instructions (e.g., blocking load ports)&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;comments&quot;&gt;Comments&lt;/h3&gt;

&lt;p&gt;Feedback of any type is welcome. I don’t have a comments system&lt;sup id=&quot;fnref:comments&quot;&gt;&lt;a href=&quot;#fn:comments&quot; class=&quot;footnote&quot;&gt;17&lt;/a&gt;&lt;/sup&gt; yet, so as usual I’ll outsource discussion to &lt;a href=&quot;https://news.ycombinator.com/item?id=20751186&quot;&gt;this HackerNews thread&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;thanks-and-attribution&quot;&gt;Thanks and Attribution&lt;/h3&gt;

&lt;p&gt;Thanks to &lt;abbr title=&quot;HackerNews&quot;&gt;HN&lt;/abbr&gt; user rrss for pointing out errors in my cycle chart.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://commons.wikimedia.org/wiki/File:Intel_8259.svg&quot;&gt;Intel 8259 image&lt;/a&gt; by Wikipedia user German under &lt;a href=&quot;https://creativecommons.org/licenses/by-sa/3.0&quot;&gt;CC BY-SA 3.0&lt;/a&gt;.&lt;/p&gt;

&lt;p class=&quot;info&quot;&gt;If you liked this post, check out the &lt;a href=&quot;/&quot;&gt;homepage&lt;/a&gt; for others you might enjoy&lt;sup id=&quot;fnref:noenjoy&quot;&gt;&lt;a href=&quot;#fn:noenjoy&quot; class=&quot;footnote&quot;&gt;18&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;hr /&gt;
&lt;hr /&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;div class=&quot;footnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:triggered&quot;&gt;
      &lt;p&gt;By &lt;em&gt;externally triggered&lt;/em&gt; I mean pretty much any interrupt that doesn’t result directly from the code currently running on the logical core (such as an &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;int&lt;/code&gt; instruction or some fault/trap). So it includes includes timer interrupts, cross CPU inter-processor interrupts, etc. &lt;a href=&quot;#fnref:triggered&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:twoc&quot;&gt;
      &lt;p&gt;The two choices basically correspond to letting the current instruction finish, or aborting it in order to run the interrupt. &lt;a href=&quot;#fnref:twoc&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:whymov&quot;&gt;
      &lt;p&gt;I use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mov&lt;/code&gt; here to avoid creating any dependency chains, i.e., the destination of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mov&lt;/code&gt; is &lt;em&gt;write-only&lt;/em&gt; unlike most x86 integer instructions which both read and write their first operand. &lt;a href=&quot;#fnref:whymov&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:srcnotes&quot;&gt;
      &lt;p&gt;Here I show the full loop, and the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;%rep&lt;/code&gt; directive which repeats (unrolls) the contained block the specified number of times, but from this point on I may just show the inner block itself with the understanding that it is unrolled with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;%rep&lt;/code&gt; some number of times (to reduce the impact of the loop) and contained with a loop of ~1,000,000 iterations so we get a reasonable number of samples. For the exact code you can always refer to the &lt;a href=&quot;https://github.com/travisdowns/interrupt-test&quot;&gt;repo&lt;/a&gt;. &lt;a href=&quot;#fnref:srcnotes&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:ipc&quot;&gt;
      &lt;p&gt;In general I’ll just say “this code executes like this” or “the bottleneck is this”, without futher justification as the examples are usually simple with one obvious bottleneck. Even though I won’t always mentioned it, I &lt;em&gt;have&lt;/em&gt; checked that the examples perform as expected, usually with a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;perf stat&lt;/code&gt; to confirm the &lt;abbr title=&quot;Instructions per cycle: calculated over an interval by measuring the number of instructions executed and the duration in cycles.&quot;&gt;IPC&lt;/abbr&gt; or cycles per iteration or whatever. &lt;a href=&quot;#fnref:ipc&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:acommand&quot;&gt;
      &lt;p&gt;The full command I use is something like &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;perfc annotate -Mintel --stdio --no-source --stdio-color --show-nr-samples&lt;/code&gt;. For interactive analysis just drop the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;--stdio&lt;/code&gt; part to get the tui UI. &lt;a href=&quot;#fnref:acommand&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:untrue&quot;&gt;
      &lt;p&gt;Of course we will see that in some cases the &lt;em&gt;selected&lt;/em&gt; and &lt;em&gt;sampled&lt;/em&gt; instructions can actually be the same, in the case of interruptible instructions. &lt;a href=&quot;#fnref:untrue&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:likely&quot;&gt;
      &lt;p&gt;Originally I thought it was guaranteed that instructions on the critical path would be eligible for sampling, but it is not: later we construct an example where a critical instruction never gets selected. &lt;a href=&quot;#fnref:likely&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:notonly&quot;&gt;
      &lt;p&gt;Note that I am specifically &lt;em&gt;not&lt;/em&gt; saying the only selected instructions will be from the chain. As we will see soon, retirement limits mean that unless the chain instructions are fairly dense (i.e., at least 1 every 4 instructions), other instructions may appear also. &lt;a href=&quot;#fnref:notonly&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:whynop&quot;&gt;
      &lt;p&gt;I use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;nop&lt;/code&gt; because it has no inputs or outputs so I don’t have to be careful not to create dependency chains (e.g., ensuring each &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;add&lt;/code&gt; goes to a distinct register), and it doesn’t use any execution units, so we won’t steal a unit from the chain on the critical path. &lt;a href=&quot;#fnref:whynop&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:already&quot;&gt;
      &lt;p&gt;We have already seen this effect in all of the earlier examples: it is why the long-latency &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;add&lt;/code&gt; is selected, even though that in the cycle that it retires the three subsequent instructions also retire. This behavior is nice because it ensures that longer latency instructions are generally selected, rather than some unrelated instruction (i.e., the usual skid is +1 not +4). &lt;a href=&quot;#fnref:already&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:4to6&quot;&gt;
      &lt;p&gt;The increase is from 4 to 6, rather than 4 to 5, because of +1 cycle for the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;add&lt;/code&gt; itself (obvious) and +1 cycle because any ALU instruction in the address computation path of a pointer chasing loop disables the 4-cycle load fast path (much less obvious). &lt;a href=&quot;#fnref:4to6&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:possibly&quot;&gt;
      &lt;p&gt;Of course this doesn’t mean the instruction &lt;em&gt;will&lt;/em&gt; be selected, we only have a few thousand interrupts over one million or more interrupts, so usually nothign happens at all. These are just the locations that are highly likely to be chosen when an interrupt does arrive. &lt;a href=&quot;#fnref:possibly&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:atomiclat&quot;&gt;
      &lt;p&gt;Latency is in scare quotes here because the apparent latency of atomic operations doesn’t behave like most other instructions: if you measure the throughput of back-to-back atomic instructions you get higher performance if the instructions form a dependency chain, rather than if they don’t! The latency of atomics doesn’t fit neatly into an input-to-output model as their &lt;em&gt;execute at retirement&lt;/em&gt; behavior causes other bottlenecks. &lt;a href=&quot;#fnref:atomiclat&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt; &lt;a href=&quot;#fnref:atomiclat:1&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:execretire&quot;&gt;
      &lt;p&gt;That’s the simple way of looking at it: reality is more complex as usual. Locked instructions may execute some of their instructions before they are ready to retire: e.g., loading the the required value and the ALU operation. However, the key “unlock” operation which verifies the result of the earlier operations and commits the results, in an atomic way, happens at retire and this is responsible for the majority of the cost of this instruction. &lt;a href=&quot;#fnref:execretire&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:uneven&quot;&gt;
      &lt;p&gt;The uneven pattern among the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;vpmulld&lt;/code&gt; instructions is because the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;lock add&lt;/code&gt; instruction eats the retire cycles of the first &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;vpmulld&lt;/code&gt; that occur after (they can retire quickly because they are already complete), so only the later ones have a full allocation. &lt;a href=&quot;#fnref:uneven&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:comments&quot;&gt;
      &lt;p&gt;If anyone has a recommendation or a if anyone knows of a comments system that works with static sites, and which is not Disqus, has no ads, is free and fast, and lets me own the comment data (or at least export it in a reasonable format), I am all ears. &lt;a href=&quot;#fnref:comments&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:noenjoy&quot;&gt;
      &lt;p&gt;On the other hand, if you didn’t enjoy it, I have to wonder how you made it this far! Maybe you started off liking it, then felt invested and compelled to finish it? Beware the sunk cost fallacy! &lt;a href=&quot;#fnref:noenjoy&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><author><name>Travis Downs</name><email>travis.downs@gmail.com</email></author><category term="blog" /><category term="performance" /><category term="benchmarking" /><summary type="html"></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/assets/interrupts/Intel_8259.png" /><media:content medium="image" url="http://localhost:4000/assets/interrupts/Intel_8259.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Performance speed limits</title><link href="http://localhost:4000/blog/2019/06/11/speed-limits.html" rel="alternate" type="text/html" title="Performance speed limits" /><published>2019-06-11T00:00:00+02:00</published><updated>2019-06-11T00:00:00+02:00</updated><id>http://localhost:4000/blog/2019/06/11/speed-limits</id><content type="html" xml:base="http://localhost:4000/blog/2019/06/11/speed-limits.html">&lt;h2 id=&quot;how-fast-can-it-go&quot;&gt;How fast can it go?&lt;/h2&gt;

&lt;p&gt;Sometimes you just want to know how fast your code &lt;em&gt;can&lt;/em&gt; go, without benchmarking it. Sometimes you have benchmarked it and want to know how close you are to the maximum speed. Often you just need to know what the current limiting factor is, to guide your optimization decisions.&lt;/p&gt;

&lt;p&gt;Well this post is about that determining that &lt;em&gt;speed limit&lt;/em&gt;&lt;sup id=&quot;fnref:speedlemire&quot;&gt;&lt;a href=&quot;#fn:speedlemire&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;. It’s not a comprehensive performance evaluation methodology, but for many &lt;em&gt;small&lt;/em&gt; pieces of code it will work very well.&lt;/p&gt;

&lt;p style=&quot;text-align: center;&quot;&gt;&lt;img src=&quot;/assets/speed-limits/speed-limit-50-ns.svg&quot; alt=&quot;Speed Limit&quot; width=&quot;300px&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;table-of-contents&quot;&gt;Table of Contents&lt;/h2&gt;

&lt;p&gt;This post is intended to be read from top to bottom, but if it’s not your first time here or you just want to skip to a part you find interesting, here you go:&lt;/p&gt;

&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#how-fast-can-it-go&quot; id=&quot;markdown-toc-how-fast-can-it-go&quot;&gt;How fast can it go?&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#table-of-contents&quot; id=&quot;markdown-toc-table-of-contents&quot;&gt;Table of Contents&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#the-limits&quot; id=&quot;markdown-toc-the-limits&quot;&gt;The Limits&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#big-list-of-caveats&quot; id=&quot;markdown-toc-big-list-of-caveats&quot;&gt;Big List of Caveats&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#pipeline-width&quot; id=&quot;markdown-toc-pipeline-width&quot;&gt;Pipeline Width&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#remedies&quot; id=&quot;markdown-toc-remedies&quot;&gt;Remedies&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#portexecution-unit-limits&quot; id=&quot;markdown-toc-portexecution-unit-limits&quot;&gt;Port/Execution Unit Limits&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#tools&quot; id=&quot;markdown-toc-tools&quot;&gt;Tools&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#measuring-it&quot; id=&quot;markdown-toc-measuring-it&quot;&gt;Measuring It&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#remedies-1&quot; id=&quot;markdown-toc-remedies-1&quot;&gt;Remedies&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#load-throughput-limit&quot; id=&quot;markdown-toc-load-throughput-limit&quot;&gt;Load Throughput Limit&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#split-cache-lines&quot; id=&quot;markdown-toc-split-cache-lines&quot;&gt;Split Cache Lines&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#remedies-2&quot; id=&quot;markdown-toc-remedies-2&quot;&gt;Remedies&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#memory-and-cache-bandwidth&quot; id=&quot;markdown-toc-memory-and-cache-bandwidth&quot;&gt;Memory and Cache Bandwidth&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#remedies-3&quot; id=&quot;markdown-toc-remedies-3&quot;&gt;Remedies&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#carried-dependency-chains&quot; id=&quot;markdown-toc-carried-dependency-chains&quot;&gt;Carried Dependency Chains&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#tools-1&quot; id=&quot;markdown-toc-tools-1&quot;&gt;Tools&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#remedies-4&quot; id=&quot;markdown-toc-remedies-4&quot;&gt;Remedies&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#front-end-effects&quot; id=&quot;markdown-toc-front-end-effects&quot;&gt;Front End Effects&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#store-throughput-limit&quot; id=&quot;markdown-toc-store-throughput-limit&quot;&gt;Store Throughput Limit&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#split-cache-lines-1&quot; id=&quot;markdown-toc-split-cache-lines-1&quot;&gt;Split Cache Lines&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#remedies-5&quot; id=&quot;markdown-toc-remedies-5&quot;&gt;Remedies&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#complex-addressing-limit&quot; id=&quot;markdown-toc-complex-addressing-limit&quot;&gt;Complex Addressing Limit&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#remedies-6&quot; id=&quot;markdown-toc-remedies-6&quot;&gt;Remedies&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#taken-branches&quot; id=&quot;markdown-toc-taken-branches&quot;&gt;Taken Branches&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#out-of-order-limits&quot; id=&quot;markdown-toc-out-of-order-limits&quot;&gt;Out of Order Limits&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#reorder-buffer-size&quot; id=&quot;markdown-toc-reorder-buffer-size&quot;&gt;Reorder Buffer Size&lt;/a&gt;        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#remedies-7&quot; id=&quot;markdown-toc-remedies-7&quot;&gt;Remedies&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#load-buffer&quot; id=&quot;markdown-toc-load-buffer&quot;&gt;Load Buffer&lt;/a&gt;        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#remedies-8&quot; id=&quot;markdown-toc-remedies-8&quot;&gt;Remedies&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#store-buffer&quot; id=&quot;markdown-toc-store-buffer&quot;&gt;Store Buffer&lt;/a&gt;        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#remedies-9&quot; id=&quot;markdown-toc-remedies-9&quot;&gt;Remedies&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#scheduler&quot; id=&quot;markdown-toc-scheduler&quot;&gt;Scheduler&lt;/a&gt;        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#remedies-10&quot; id=&quot;markdown-toc-remedies-10&quot;&gt;Remedies&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#register-file-size-limit&quot; id=&quot;markdown-toc-register-file-size-limit&quot;&gt;Register File Size Limit&lt;/a&gt;        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#remedies-11&quot; id=&quot;markdown-toc-remedies-11&quot;&gt;Remedies&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#branches-in-flight&quot; id=&quot;markdown-toc-branches-in-flight&quot;&gt;Branches in Flight&lt;/a&gt;        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#remedies-12&quot; id=&quot;markdown-toc-remedies-12&quot;&gt;Remedies&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#calls-in-flight&quot; id=&quot;markdown-toc-calls-in-flight&quot;&gt;Calls in Flight&lt;/a&gt;        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#remedies-13&quot; id=&quot;markdown-toc-remedies-13&quot;&gt;Remedies&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#thank-you&quot; id=&quot;markdown-toc-thank-you&quot;&gt;Thank You&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#comments&quot; id=&quot;markdown-toc-comments&quot;&gt;Comments&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;the-limits&quot;&gt;The Limits&lt;/h2&gt;

&lt;p&gt;There are many possible limits that apply to code executing on a CPU, and in principle the achieved speed will simply be determined by the lowest of all the limits that apply to the code in question. That is, the code will execute &lt;em&gt;only as fast as its narrowest bottleneck&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;So I’ll just list some of the known bottlenecks here, starting with common factors first, down through some fairly obscure and rarely discussed ones. The real-world numbers come mostly from Intel x86 CPUs, because that’s what I know off of top of my head, but the concepts mostly apply in general as well, although often different limit values.&lt;/p&gt;

&lt;p&gt;Where possible I’ve included specific figures for &lt;em&gt;modern&lt;/em&gt; Intel chips and sometimes AMD CPUs. I’m happy to add numbers for other non-x86 CPUs if anyone out there is interested in providing them.&lt;/p&gt;

&lt;h3 id=&quot;big-list-of-caveats&quot;&gt;Big List of Caveats&lt;/h3&gt;

&lt;p&gt;First, lets start with this list of very important caveats.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The limits discussed below generally apply to loops of any size, and also straight line or branchy code with no loops in sight. Unfortunately, it is only really possible to apply the simple analyses described to loops, since a steady state will be reached and the limit values will apply. For most straight line code, however, no steady state is reached and the actual behavior depends on many details of the architecture such as various internal buffer and queue sizes. Analyzing such code sections basically requires a detailed simulation, not a back-of-napkin estimate as we attempt here.&lt;/li&gt;
  &lt;li&gt;Similarly, even large loops may not reach a steady state, if the loop is big enough that iterations don’t completely overlap. This is discussed a bit more in the &lt;a href=&quot;#out-of-order-limits&quot;&gt;Out of Order Limits&lt;/a&gt; section.&lt;/li&gt;
  &lt;li&gt;The limits below are all &lt;em&gt;upper bounds&lt;/em&gt;, i.e., the CPU will never go faster than this (in a steady state) - but it doesn’t mean you can achieve these limits in every case. For each limit, I have found code that you gets you to the limit - but you can’t expect that to be the case every time. There may be inefficiencies in the implementation, or unmodeled effects that make the actual limit lower in practice. Don’t call Intel and complain that you aren’t achieving your two loads per cycle! It’s a speed &lt;em&gt;limit&lt;/em&gt;, not a guaranteed maximum&lt;sup id=&quot;fnref:thatsaid&quot;&gt;&lt;a href=&quot;#fn:thatsaid&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;.&lt;/li&gt;
  &lt;li&gt;There are known limits not discussed below, such as instruction throughput for not-fully-pipelined instructions.&lt;/li&gt;
  &lt;li&gt;There are certainly also unknown limits or not well understood limits not discussed here.&lt;/li&gt;
  &lt;li&gt;More caveats are mentioned in the individual sections.&lt;/li&gt;
  &lt;li&gt;I simply ignore branch prediction for now: this post just got too long (it’s a problem I have). It also deserves a whole post to itself.&lt;/li&gt;
  &lt;li&gt;This methodology is unsuitable for analyzing entire applications - it works best for a small hotspot of say 1 to 50 lines of code, which hopefully produce less than about 50 assembly instructions. Trying to apply it to larger stuff may lead to madness. I highly recommend &lt;a href=&quot;https://software.intel.com/en-us/vtune-amplifier-cookbook-top-down-microarchitecture-analysis-method&quot;&gt;Intel’s Top-Down&lt;/a&gt; analysis method for more complex tasks. It always starts with performance counter measurements and tries to identify the problems from there. A free implementation is available in Andi Kleen’s &lt;a href=&quot;https://github.com/andikleen/pmu-tools&quot;&gt;pmu-tools&lt;/a&gt; for Linux. On Windows, free licenses of VTune are available though the 90-day community license for System Studio.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;pipeline-width&quot;&gt;Pipeline Width&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Intel:&lt;/strong&gt; Maximum 4 fused-uops&lt;sup id=&quot;fnref:ICL&quot;&gt;&lt;a href=&quot;#fn:ICL&quot; class=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt; per cycle&lt;br /&gt;
&lt;strong&gt;AMD:&lt;/strong&gt; Maximum 5 fused-uops per cycle&lt;/p&gt;

&lt;p&gt;At a fundamental level, every CPU can execute only a maximum number of operations per cycle. For many early CPUs, this was always less than one per cycle, but modern pipelined &lt;a href=&quot;https://en.wikipedia.org/wiki/Superscalar_processor&quot;&gt;superscalar&lt;/a&gt; processors can execute more than one per cycle, up to some limit. This underlying limit is not always be imposed in the same place, e.g., some CPUs may be limited by instruction encoding, others by register renaming or retirement - but there is always a limit (sometimes more than one limit depending on what you are counting).&lt;/p&gt;

&lt;p&gt;For modern Intel chips this limit is 4 &lt;em&gt;fused-domain&lt;/em&gt;&lt;sup id=&quot;fnref:fused-domain&quot;&gt;&lt;a href=&quot;#fn:fused-domain&quot; class=&quot;footnote&quot;&gt;4&lt;/a&gt;&lt;/sup&gt; operations, and for modern AMD it is 5 macro-operations. So if your loop contains N fused-uops, it will never execute at more than 1 iteration per cycle.&lt;/p&gt;

&lt;p&gt;Consider the following simple loop, which separately adds up the top and bottom 16-bit halves of every 32-bit integer in an array:&lt;/p&gt;

&lt;div class=&quot;language-c++ highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kt&quot;&gt;uint32_t&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;top&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bottom&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;size_t&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;kt&quot;&gt;uint32_t&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;elem&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;elem&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;];&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;top&lt;/span&gt;    &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;elem&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;16&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;bottom&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;elem&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;amp;&lt;/span&gt; &lt;span class=&quot;mh&quot;&gt;0xFFFF&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;elem&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;];&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;top&lt;/span&gt;    &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;elem&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;16&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;bottom&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;elem&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;amp;&lt;/span&gt; &lt;span class=&quot;mh&quot;&gt;0xFFFF&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This compiles to the following assembly:&lt;/p&gt;

&lt;div class=&quot;language-nasm highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;top&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;mov&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;r8d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;DWORD&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rdi&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rcx&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;]&lt;/span&gt;          &lt;span class=&quot;c&quot;&gt;; 1&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;mov&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;edx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;DWORD&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rdi&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rcx&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mh&quot;&gt;0x4&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;]&lt;/span&gt;      &lt;span class=&quot;c&quot;&gt;; 2&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;add&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;rcx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mh&quot;&gt;0x2&lt;/span&gt;                        &lt;span class=&quot;c&quot;&gt;; 3&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;mov&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;r11d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;r8d&lt;/span&gt;                       &lt;span class=&quot;c&quot;&gt;; 4&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;movzx&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;r8d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;r8w&lt;/span&gt;                        &lt;span class=&quot;c&quot;&gt;; 5&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;mov&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;r9d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;edx&lt;/span&gt;                        &lt;span class=&quot;c&quot;&gt;; 6&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;shr&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;r11d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mh&quot;&gt;0x10&lt;/span&gt;                      &lt;span class=&quot;c&quot;&gt;; 7&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;movzx&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;edx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dx&lt;/span&gt;                         &lt;span class=&quot;c&quot;&gt;; 8&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;shr&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;r9d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mh&quot;&gt;0x10&lt;/span&gt;                       &lt;span class=&quot;c&quot;&gt;; 9&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;add&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;edx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;r8d&lt;/span&gt;                        &lt;span class=&quot;c&quot;&gt;; 10&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;add&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;r9d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;r11d&lt;/span&gt;                       &lt;span class=&quot;c&quot;&gt;; 11&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;add&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;eax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;edx&lt;/span&gt;                        &lt;span class=&quot;c&quot;&gt;; 12&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;add&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;r10d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;r9d&lt;/span&gt;                       &lt;span class=&quot;c&quot;&gt;; 13&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;cmp&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;rcx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rsi&lt;/span&gt;                        &lt;span class=&quot;c&quot;&gt;; (fuses w/ jb)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;jb&lt;/span&gt;     &lt;span class=&quot;n&quot;&gt;top&lt;/span&gt;                            &lt;span class=&quot;c&quot;&gt;; 14&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;I’ve annotated the total &lt;abbr title=&quot;Micro-operation: instructions are translated into one or more uops, which are simple operations executed by the CPU's execution units.&quot;&gt;uop&lt;/abbr&gt; count on each line: there is nothing tricky here as instruction is one fused &lt;abbr title=&quot;Micro-operation: instructions are translated into one or more uops, which are simple operations executed by the CPU's execution units.&quot;&gt;uop&lt;/abbr&gt;, except for the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;cmp; jb&lt;/code&gt; pair which &lt;abbr title=&quot;The fusing of an ALU operation and subsequent jump, such as `dec eax; jnz label` into one operation&quot;&gt;macro-fuse&lt;/abbr&gt; into a single &lt;abbr title=&quot;Micro-operation: instructions are translated into one or more uops, which are simple operations executed by the CPU's execution units.&quot;&gt;uop&lt;/abbr&gt;. The are 14 uops in this loop, so at best, on my Intel laptop I expect this loop to take 14 / 4 = 3.5 cycles per iteration (1.75 cycles per element). Indeed, when I time this&lt;sup id=&quot;fnref:sum-halves&quot;&gt;&lt;a href=&quot;#fn:sum-halves&quot; class=&quot;footnote&quot;&gt;5&lt;/a&gt;&lt;/sup&gt; I get 3.51 cycles per iteration, so we are executing 3.99 fused uops per cycle, and we have certainly hit the pipeline width speed limit.&lt;/p&gt;

&lt;p&gt;For more complicated code where you don’t actually want to calculate the &lt;abbr title=&quot;Micro-operation: instructions are translated into one or more uops, which are simple operations executed by the CPU's execution units.&quot;&gt;uop&lt;/abbr&gt; count by hand, you can use performance counters - the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;uops_issued.any&lt;/code&gt; counter counts fused-domain uops:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ ./uarch-bench.sh --timer=perf --test-name=cpp/sum-halves --extra-events=uops_issued.any
...
Resolved and programmed event 'uops_issued.any' to 'cpu/config=0x10e/', caps: R:1 UT:1 ZT:1 index: 0x1
Running benchmarks groups using timer perf

** Running group cpp : Tests written in C++ **
                               Benchmark    Cycles    uops_i
        Sum 16-bit halves of array elems      3.51     14.03
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The counter reflects the 14 uops/iteration we calculated by looking at the assembly&lt;sup id=&quot;fnref:extra-3&quot;&gt;&lt;a href=&quot;#fn:extra-3&quot; class=&quot;footnote&quot;&gt;6&lt;/a&gt;&lt;/sup&gt;. If you &lt;em&gt;calculate&lt;/em&gt; a value very close to 4 uops per cycle using this metric, you know without examining the code that you are bumping up against this speed limit.&lt;/p&gt;

&lt;h3 id=&quot;remedies&quot;&gt;Remedies&lt;/h3&gt;

&lt;p&gt;In a way this is the simplest of the limits to understand: you simply can’t execute any more operations per cycle. You code is already maximally efficient in an operations/cycle sense: you don’t have to worry about cache misses, expensive operations, too many jumps, branch mispredictions or anything like that because they aren’t limiting you.&lt;/p&gt;

&lt;p&gt;Your only goal is to reduce the number of operations (in the fused domain), which usually means reducing the number of instructions. You can do that by:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Removing instructions, i.e., “classic” instruction-oriented optimization. Way too involved to cover in a bullet point, but briefly you can try to unroll loops (indeed, by unrolling the loop above, I cut execution time by ~15%), use different instructions that are more efficient, remove instructions (e.g., the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mov r11d,r8d&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mov r9d,edx&lt;/code&gt; are not necessary and could be removed with a slight reoganization), etc. If you are writing in a high level language you can’t do this &lt;em&gt;directly&lt;/em&gt;, but you can try to understand the assembly the compiler is generating and make changes to the code or compiler flags that get it to do what you want.&lt;/li&gt;
  &lt;li&gt;Vectorization. Try to do more work with one instruction. This is an obvious huge win for this method. If you compile the same code with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-O3&lt;/code&gt; rather than &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-O2&lt;/code&gt;, gcc vectorizes it (and doesn’t even do a great job&lt;sup id=&quot;fnref:gcc-notgreat&quot;&gt;&lt;a href=&quot;#fn:gcc-notgreat&quot; class=&quot;footnote&quot;&gt;7&lt;/a&gt;&lt;/sup&gt;) and we get a 4.6x speedup, to 0.76 cycles per iteration (0.38 cycles per element). If you vectorized it by hand or massaged the auto-vectorization a bit more I think you could get to an additional 3x speed, down to roughly 0.125 cycles per element.&lt;/li&gt;
  &lt;li&gt;Micro-fusion. Somewhat specific to x86, but you can look for opportunities to fold a load and an ALU operation together, since such micro-fused operations only count as one in the fused domain, compared to two for the separate instructions. This generally applies only for values loaded and used once, but &lt;em&gt;rarely&lt;/em&gt; it may even be profitable to load the same value &lt;em&gt;twice&lt;/em&gt; from memory, in two different instructions, in order to eliminate a standalone &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mov&lt;/code&gt; from memory. This is more complicated than I make it sound because of the &lt;a href=&quot;https://stackoverflow.com/q/26046634&quot;&gt;complication of de-lamination&lt;/a&gt;, which varies by model and is not fully described&lt;sup id=&quot;fnref:delamopt&quot;&gt;&lt;a href=&quot;#fn:delamopt&quot; class=&quot;footnote&quot;&gt;8&lt;/a&gt;&lt;/sup&gt; in the optimization manual.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;portexecution-unit-limits&quot;&gt;Port/Execution Unit Limits&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Intel, AMD:&lt;/strong&gt; One operation per port, per cycle&lt;/p&gt;

&lt;p&gt;Let us use our newfound knowledge of the pipeline width limitation, and tackle another example loop:&lt;/p&gt;

&lt;div class=&quot;language-c++ highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kt&quot;&gt;uint32_t&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;mul_by&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;uint32_t&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;size_t&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;uint32_t&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;m&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;kt&quot;&gt;uint32_t&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sum&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;size_t&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;len&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;++&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;kt&quot;&gt;uint32_t&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;];&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;sum&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;m&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The loop compiles to the following assembly. I’ve marked &lt;abbr title=&quot;Micro-operation: instructions are translated into one or more uops, which are simple operations executed by the CPU's execution units.&quot;&gt;uop&lt;/abbr&gt; counts as before.&lt;/p&gt;

&lt;div class=&quot;language-nasm highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;mi&quot;&gt;930&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;mov&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;r10d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;DWORD&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rdi&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rcx&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mh&quot;&gt;0x4&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;;  1 y = data[i + 1]&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;mov&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;r8d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;r10d&lt;/span&gt;                   &lt;span class=&quot;c&quot;&gt;;  2 setup up r8d to hold result of multiplies&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;imul&lt;/span&gt;   &lt;span class=&quot;n&quot;&gt;r8d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ecx&lt;/span&gt;                    &lt;span class=&quot;c&quot;&gt;;  3 i * y&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;imul&lt;/span&gt;   &lt;span class=&quot;n&quot;&gt;r8d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;edx&lt;/span&gt;                    &lt;span class=&quot;c&quot;&gt;;  4 ↑ * m&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;imul&lt;/span&gt;   &lt;span class=&quot;n&quot;&gt;r8d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ecx&lt;/span&gt;                    &lt;span class=&quot;c&quot;&gt;;  5 ↑ * i&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;add&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;rcx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mh&quot;&gt;0x1&lt;/span&gt;                    &lt;span class=&quot;c&quot;&gt;;  6 i++&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;imul&lt;/span&gt;   &lt;span class=&quot;n&quot;&gt;r8d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;r9d&lt;/span&gt;                    &lt;span class=&quot;c&quot;&gt;;  7 ↑ * x&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;mov&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;r9d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;r10d&lt;/span&gt;                   &lt;span class=&quot;c&quot;&gt;;  8 stash y for next iteration&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;add&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;eax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;r8d&lt;/span&gt;                    &lt;span class=&quot;c&quot;&gt;;  9 sum += ...&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;cmp&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;rcx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rsi&lt;/span&gt;                    &lt;span class=&quot;c&quot;&gt;;    i &amp;lt; len (fuses with jne)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;jne&lt;/span&gt;    &lt;span class=&quot;mi&quot;&gt;930&lt;/span&gt;                        &lt;span class=&quot;c&quot;&gt;; 10&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Despite the source containing two loads per iteration (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;x = data[i]&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;y = data[i + 1]&lt;/code&gt;), the compiler was clever enough to reduce that to one, since &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;y&lt;/code&gt; in iteration &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;n&lt;/code&gt; becomes &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;x&lt;/code&gt; in iteration &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;n + 1&lt;/code&gt;, so it saves the loaded value in a register across iterations.&lt;/p&gt;

&lt;p&gt;So we can just apply our pipeline width technique to this loop, right? We count 10 uops (again, the only trick is that &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;cmp; jne&lt;/code&gt; are &lt;abbr title=&quot;The fusing of an ALU operation and subsequent jump, such as `dec eax; jnz label` into one operation&quot;&gt;macro-fused&lt;/abbr&gt;). We can confirm it in &lt;abbr title=&quot;Microarchitecture: a specific implementation of an ISA, e.g., &amp;quot;Haswell microarchitecture&amp;quot;.&quot;&gt;uarch&lt;/abbr&gt;-bench:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ ./uarch-bench.sh --timer=perf --test-name=cpp/mul-4 --extra-events=uops_issued.any,uops_retired.retire_slots
....
** Running group cpp : Tests written in C++ **
                               Benchmark    Cycles    uops_i    uops_r
                    Four multiplications      ????     10.01     10.00
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Right, 10 uops. So this should take 10 / 4 = 2.5 cycles per iteration on modern Intel then, right? No. The hidden &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;????&lt;/code&gt; value in the benchmark output indicates that it actually takes 4.01 cycles.&lt;/p&gt;

&lt;p&gt;What gives? As it turns out, the limitation is the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;imul&lt;/code&gt; instructions. Although up to four &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;imul&lt;/code&gt; instructions can be &lt;em&gt;issued&lt;sup id=&quot;fnref:issued&quot;&gt;&lt;a href=&quot;#fn:issued&quot; class=&quot;footnote&quot;&gt;9&lt;/a&gt;&lt;/sup&gt;&lt;/em&gt; every cycle, there is only a single scalar multiplication unit on the CPU, and so only one multiplication can begin execution every cycle. Since there are four multiplications in the loop, it takes at least four cycles to execute it, and in fact that’s exactly what we find.&lt;/p&gt;

&lt;p&gt;On modern chips all operations execute only through a limited number of ports&lt;sup id=&quot;fnref:ports&quot;&gt;&lt;a href=&quot;#fn:ports&quot; class=&quot;footnote&quot;&gt;10&lt;/a&gt;&lt;/sup&gt; and for multiplications that is always only &lt;abbr title=&quot;port 1 (GP and SIMD ALU, integer mul)&quot;&gt;p1&lt;/abbr&gt;. You can get this information from Agner’s &lt;a href=&quot;https://www.agner.org/optimize/#manual_instr_tab&quot;&gt;instruction tables&lt;/a&gt;:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/speed-limits/agner-imul.png&quot; alt=&quot;Agner's port usage info&quot; /&gt;&lt;/p&gt;

&lt;p&gt;… or from &lt;a href=&quot;http://uops.info/html-instr/IMUL_R32_R32.html&quot;&gt;uops.info&lt;/a&gt;:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/speed-limits/uops-info-imul.png&quot; alt=&quot;uops-info port usage info&quot; /&gt;&lt;/p&gt;

&lt;p&gt;On modern Intel some simple integer arithmetic (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;add&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sub&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;inc&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;dec&lt;/code&gt;), bitwise operation (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;or&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;and&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;xor&lt;/code&gt;) and flag setting tests (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;test&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;cmp&lt;/code&gt;) run on four ports, so you aren’t very likely to see a port bottleneck for these operations (since the pipeline width bottleneck is more general and is also four), but many operations compete for only a few ports. For example, shift instructions and bit test/set operations like &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;bt&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;btr&lt;/code&gt; and friends use only &lt;abbr title=&quot;port 1 (GP and SIMD ALU, integer mul)&quot;&gt;p1&lt;/abbr&gt; and &lt;abbr title=&quot;port 6 (GP ALU, all branches)&quot;&gt;p6&lt;/abbr&gt;. More advanced bit operations like &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;popcnt&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;tzcnt&lt;/code&gt; execute only &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;p1&lt;/code&gt;, and so on. Note that in some cases instructions which can go to wide variety of ports, such as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;add&lt;/code&gt; may execute on a port that is under contention by other instructions rather than on the less loaded ports: a scheduling quirk that can reduce performance. Why that happens is &lt;a href=&quot;http://stackoverflow.com/questions/40681331/how-are-x86-uops-scheduled-exactly&quot;&gt;not fully understood&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;One of the most common cases of port contention is with vector operations. There are only three vector ports, so the best case is three vector operations per cycle, and for AVX-512 there are only two ports so the best case is two per cycle. Furthermore, only a few operations can use all three ports (mostly simple integer arithmetic and bitwise operations and 32 and 64-bit &lt;abbr title=&quot;When discussing assembly instructions an immediate is a value embedded in the instruction itself, e.g., the 1 in add eax, 1.&quot;&gt;immediate&lt;/abbr&gt; blends) - many are restricted to one or two ports. In particular, shuffles run only on &lt;abbr title=&quot;port 5 (GP and SIMD ALU, vector shuffles)&quot;&gt;p5&lt;/abbr&gt; and can be a bottleneck for shuffle heavy algorithm.&lt;/p&gt;

&lt;h3 id=&quot;tools&quot;&gt;Tools&lt;/h3&gt;

&lt;p&gt;In the example above it was easy to see the port pressure because the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;imul&lt;/code&gt; instructions go to only a single port, and the remainder of the instructions are mostly simple instructions that can go to any of four ports, so a 4 cycle &lt;em&gt;solution&lt;/em&gt; to the port assignment problem is easy to find. In more complex cases, with many instructions that go to many ports, it is less clear what the ideal solution is (and even less clear what the CPU will actually do without testing it), so you can use one of a few tools:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Intel IACA&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Tries to solve for port pressure (algorithm unclear) and displays it in a table. Has reached end of life but can still be downloaded &lt;a href=&quot;https://software.intel.com/en-us/articles/intel-architecture-code-analyzer&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;RRZE-HPC OSACA&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Essnentially an open-source version if IACA. Displays cumulative port pressure in a similar way to IACA, although it simply divides each instruction evenly among the ports it can use and doesn’t look for a more ideal solution. On &lt;a href=&quot;https://github.com/RRZE-HPC/OSACA&quot;&gt;github&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;LLVM-MCA&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Another tool similar to IACA and OSACA, shows port pressure in a similar way and attempts to find an ideal solution (algorithm unclear, but it’s open source so someone could check). Comes with LLVM 7 or higher and documentation is &lt;a href=&quot;https://llvm.org/docs/CommandGuide/llvm-mca.html&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;measuring-it&quot;&gt;Measuring It&lt;/h3&gt;

&lt;p&gt;You can measure the actual port pressure using the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;perf&lt;/code&gt; and the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;uops_dispatched_port&lt;/code&gt; counters. For example, to measure the full port pressure across all 8 ports, you can do the following in &lt;abbr title=&quot;Microarchitecture: a specific implementation of an ISA, e.g., &amp;quot;Haswell microarchitecture&amp;quot;.&quot;&gt;uarch&lt;/abbr&gt;-bench:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;./uarch-bench.sh --timer=perf --test-name=cpp/mul-4 --extra-events=uops_dispatched_port.port_0,uops_dispatched_port.port_1,uops_dispatched_port.port_2,uops_dispatched_port.port_3,uops_dispatched_port.port_4,uops_dispatched_port.port_5,uops_dispatched_port.port_6,uops_dispatched_port.port_7
...
Running benchmarks groups using timer perf

** Running group cpp : Tests written in C++ **
           Benchmark       Cycles       uops_d       uops_d       uops_d       uops_d       uops_d       uops_d       uops_d       uops_d
Four multiplications         4.00         1.06         4.00         0.50         0.50         0.00         0.97         1.81         0.00
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;While noting that that the column naming scheme is &lt;a href=&quot;https://github.com/travisdowns/uarch-bench/issues/50&quot;&gt;really bad&lt;/a&gt; in this case, we see that the port1 (the 3rd numeric column) has 4 operations dispatched every iteration, and iterations take 4 cycles, so the port is active every cycle, i.e., 100% pressure. None of the other ports have significant pressure at all, they are all active less than 50% of the time.&lt;/p&gt;

&lt;h3 id=&quot;remedies-1&quot;&gt;Remedies&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Of course, any solution that removes instructions causing port pressure can help, so most of the same remedies that apply to the &lt;em&gt;pipeline width&lt;/em&gt; limit also apply here.&lt;/li&gt;
  &lt;li&gt;Additionally, you might try replacing instructions which contend for a high-pressure port with others that use different ports, even if the replacement results in more total instructions/uops. For example, sometimes &lt;abbr title=&quot;port 5 (GP and SIMD ALU, vector shuffles)&quot;&gt;p5&lt;/abbr&gt; shuffle operations can be replaced with blend operations: you need more total blends but the resulting code can be faster since the blends execute on otherwise underused &lt;abbr title=&quot;port 0 (GP and SIMD ALU, not-taken branches)&quot;&gt;p0&lt;/abbr&gt; and &lt;abbr title=&quot;port 1 (GP and SIMD ALU, integer mul)&quot;&gt;p1&lt;/abbr&gt;. Some 32 and 64-bit register-to-register broadcasts that use &lt;abbr title=&quot;port 5 (GP and SIMD ALU, vector shuffles)&quot;&gt;p5&lt;/abbr&gt; don’t use &lt;abbr title=&quot;port 5 (GP and SIMD ALU, vector shuffles)&quot;&gt;p5&lt;/abbr&gt; at all if you instead use a memory source, a rare case where memory source can be &lt;em&gt;faster&lt;/em&gt; than register source for the same operation.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;load-throughput-limit&quot;&gt;Load Throughput Limit&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Intel, AMD:&lt;/strong&gt; 2 loads per cycle&lt;/p&gt;

&lt;p&gt;Modern Intel and AMD chips (and many others) have a limit of two loads per cycle, which you can achieve if both loads hit in L1. You could just consider this the same as the “port pressure” limit, since there only two load ports - but the limit is interesting enough to call out on its own.&lt;/p&gt;

&lt;p&gt;Of course, like all limits this is a best case scenario: you might achieve much less than two loads if you are not hitting in L1 or even for L1-resident data due to things like bank conflicts present on AMD and older Intel chips&lt;sup id=&quot;fnref:bankconf&quot;&gt;&lt;a href=&quot;#fn:bankconf&quot; class=&quot;footnote&quot;&gt;11&lt;/a&gt;&lt;/sup&gt;. Still, it is interesting to note how &lt;em&gt;high&lt;/em&gt; this limit is: given the pipeline width of four, fully &lt;em&gt;half&lt;/em&gt; of your instructions can be loads while still running at maximum speed. In a throughput sense, loads that hit in cache are not all that expensive even compared to simple ALU ops.&lt;/p&gt;

&lt;p&gt;It’s not all that common to this hit this limit, but you can certainly do it. The loads have to be mostly independent (not part of a carried dependency chain), since otherwise the load latency will limit you more than the throughput.&lt;/p&gt;

&lt;p&gt;It’s not all &lt;em&gt;that&lt;/em&gt; common to hit this limit, but it can often happen in an indirect load scenario (where part of the load address is itself calculated using a value from memory), or when heavy use of lookup tables is made. Consider the following loop, does an indirect loop in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;data&lt;/code&gt; based on the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;offsets&lt;/code&gt; array and sums the values it finds&lt;sup id=&quot;fnref:written-weirdly&quot;&gt;&lt;a href=&quot;#fn:written-weirdly&quot; class=&quot;footnote&quot;&gt;12&lt;/a&gt;&lt;/sup&gt;:&lt;/p&gt;

&lt;div class=&quot;language-c++ highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;do&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;sum1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;offsets&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]];&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;sum2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;offsets&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]];&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;while&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This compiles to the following assembly:&lt;/p&gt;

&lt;div class=&quot;language-nasm highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;mi&quot;&gt;88&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;                                       &lt;span class=&quot;c&quot;&gt;; total fused uops&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;mov&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;r8d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;DWORD&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;PTR&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rsi&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rdx&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mh&quot;&gt;0x4&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;]&lt;/span&gt;  &lt;span class=&quot;c&quot;&gt;; 1&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;add&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;ecx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;DWORD&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;PTR&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rdi&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;r8&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;]&lt;/span&gt;       &lt;span class=&quot;c&quot;&gt;; 2&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;mov&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;r8d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;DWORD&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;PTR&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rsi&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rdx&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mh&quot;&gt;0x8&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;]&lt;/span&gt;  &lt;span class=&quot;c&quot;&gt;; 3&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;add&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;eax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;DWORD&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;PTR&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rdi&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;r8&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;]&lt;/span&gt;       &lt;span class=&quot;c&quot;&gt;; 4&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;sub&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;rdx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mh&quot;&gt;0x2&lt;/span&gt;                        &lt;span class=&quot;c&quot;&gt;; (fuses w/ jne)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;jne&lt;/span&gt;    &lt;span class=&quot;mi&quot;&gt;88&lt;/span&gt;                             &lt;span class=&quot;c&quot;&gt;; 5&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;There are only 5 fused-uops&lt;sup id=&quot;fnref:delam2&quot;&gt;&lt;a href=&quot;#fn:delam2&quot; class=&quot;footnote&quot;&gt;13&lt;/a&gt;&lt;/sup&gt; here, so maybe this executes in 1.25 cycles? Not so fast - it takes 2 cycles because there are 4 loads and we have a speed limit of 2 loads per cycle&lt;sup id=&quot;fnref:add-indirect&quot;&gt;&lt;a href=&quot;#fn:add-indirect&quot; class=&quot;footnote&quot;&gt;14&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;p&gt;Note that gather instructions count “one” against this limit for &lt;em&gt;each&lt;/em&gt; element they they load. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;vpgatherdd ymm0, ...&lt;/code&gt; for example, counts as 8 against this limit since it loads eight elements.&lt;/p&gt;

&lt;h3 id=&quot;split-cache-lines&quot;&gt;Split Cache Lines&lt;/h3&gt;

&lt;p&gt;For the purposes of this speed limit, on Intel, all loads that hit in the L1 cache count as one, except loads that split a cache line, which count as two. A split cache line load is of at least two bytes and crosses a 64-byte boundary. If your loads are &lt;abbr title=&quot;Naturally aligned data is data whose location in memory is a multiple of its size, e.g., a 4 byte element whose address is a multiple of 4 bytes.&quot;&gt;naturally aligned&lt;/abbr&gt;, you will never split a cache line. If your loads have totally random alignment, how often you split a cache line depends on the load size: for a load of N bytes, you’ll split a cache line with probability (N-1)/64. Hence, 32-bit random unaligned loads split less than 5% of the time but 256-bit AVX loads split 48% of the time and AVX-512 loads more than 98% of the time.&lt;/p&gt;

&lt;p&gt;On AMD Zen1 loads suffer a penalty when crossing any 32-byte boundary - such loads also count as two against the load limit. 32-byte (AVX) loads also count as two on Zen1 since the implemented vector path is only 128-bit, so two loads are needed. Any 32-byte load that is not 16-byte aligned counts as three, since in that case exactly one of the 16-byte halve will cross a 32-byte boundary.&lt;/p&gt;

&lt;h3 id=&quot;remedies-2&quot;&gt;Remedies&lt;/h3&gt;

&lt;p&gt;If you are lucky enough to hit this limit, you just need less loads. Note that the limit is not expressed in terms of the &lt;em&gt;number of bytes loaded&lt;/em&gt;, but in the number of separate loads. So sometimes you can combine two or more adjacent loads into a single load. An obvious application of that is vector loads: 32-byte AVX loads &lt;em&gt;still&lt;/em&gt; have the same limit of two per cycle as byte loads. It is difficult to use vector loads in concert with scalar code however: although you can do 8x 32-bit loads at once, if you want to feed those loads to scalar code you have trouble, because you can’t efficiently get that data into scalar registers&lt;sup id=&quot;fnref:vector-scalar&quot;&gt;&lt;a href=&quot;#fn:vector-scalar&quot; class=&quot;footnote&quot;&gt;15&lt;/a&gt;&lt;/sup&gt;. That is, you’ll have to work on vectorizing the code that consumes the loads as well.&lt;/p&gt;

&lt;p&gt;You can also sometimes use wider scalar loads in this way. In the example above, we do four 32-bit loads - two of which are scattered (the access to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;data[]&lt;/code&gt;), but two of which are adjacent (the accesses to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;offsets[i - 1]&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;offsets[i - 2]&lt;/code&gt;). We could combine those two adjacent loads into one 64-bit load, like so&lt;sup id=&quot;fnref:portable&quot;&gt;&lt;a href=&quot;#fn:portable&quot; class=&quot;footnote&quot;&gt;16&lt;/a&gt;&lt;/sup&gt;:&lt;/p&gt;

&lt;div class=&quot;language-c++ highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;do&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;kt&quot;&gt;uint64_t&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;twooffsets&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;memcpy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;amp;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;twooffsets&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;offsets&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;sizeof&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;uint64_t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;));&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;sum1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;twooffsets&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;];&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;sum2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;twooffsets&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;amp;&lt;/span&gt; &lt;span class=&quot;mh&quot;&gt;0xFFFFFFFF&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;];&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;while&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This compiles to:&lt;/p&gt;

&lt;div class=&quot;language-nasm highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;mi&quot;&gt;98&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;                                        &lt;span class=&quot;c&quot;&gt;; total fused uops&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;mov&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;rcx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;QWORD&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;PTR&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rsi&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rdx&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mh&quot;&gt;0x8&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;]&lt;/span&gt;   &lt;span class=&quot;c&quot;&gt;; 1&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;mov&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;r9&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rcx&lt;/span&gt;                          &lt;span class=&quot;c&quot;&gt;; 2&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;mov&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;ecx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ecx&lt;/span&gt;                         &lt;span class=&quot;c&quot;&gt;; 3&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;shr&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;r9&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mh&quot;&gt;0x20&lt;/span&gt;                         &lt;span class=&quot;c&quot;&gt;; 4&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;add&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;eax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;DWORD&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;PTR&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rdi&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rcx&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;]&lt;/span&gt;       &lt;span class=&quot;c&quot;&gt;; 5&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;add&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;r8d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;DWORD&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;PTR&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rdi&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;r9&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;]&lt;/span&gt;        &lt;span class=&quot;c&quot;&gt;; 6&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;sub&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;rdx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mh&quot;&gt;0x2&lt;/span&gt;                         &lt;span class=&quot;c&quot;&gt;; (fuses w/ jne)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;jne&lt;/span&gt;    &lt;span class=&quot;mi&quot;&gt;98&lt;/span&gt;                              &lt;span class=&quot;c&quot;&gt;; 7&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We have 7 fused-domain uops rather than 5, yet this runs in 1.81 cycles, about 10% faster. The theoretical limit based on pipeline width is 7 / 4 = 1.75 cycles, so we are probably getting collisions on &lt;abbr title=&quot;port 6 (GP ALU, all branches)&quot;&gt;p6&lt;/abbr&gt; between the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;shr&lt;/code&gt; and the taken branch (unrolling a bit more would help). Clang 5.0 manages to do better, by one &lt;abbr title=&quot;Micro-operation: instructions are translated into one or more uops, which are simple operations executed by the CPU's execution units.&quot;&gt;uop&lt;/abbr&gt;:&lt;/p&gt;

&lt;div class=&quot;language-nasm highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;mi&quot;&gt;70&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;mov&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;r8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;QWORD&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;PTR&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rsi&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rdx&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mh&quot;&gt;0x8&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;mov&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;r9d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;r8d&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;shr&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;r8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mh&quot;&gt;0x20&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;add&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;ecx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;DWORD&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;PTR&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rdi&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;r8&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;add&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;eax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;DWORD&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;PTR&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rdi&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;r9&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;add&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;rdx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mh&quot;&gt;0xfffffffffffffffe&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;jne&lt;/span&gt;    &lt;span class=&quot;mi&quot;&gt;70&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;It avoided the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mov r9,rcx&lt;/code&gt; instruction by combining that and the zero extension (which is effectively the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&amp;amp; 0xFFFFFFFF&lt;/code&gt;) into a single &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mov r9d,rd8&lt;/code&gt;. It runs at 1.67 cycles per iteration, saving 20% over the 4-load version, but still slower than the 1.5 limit implied by the 4-wide fused-domain limit.&lt;/p&gt;

&lt;p&gt;This code is an obvious candidate for vectorization with gather, which could in principle approach 1.25 cycles per iteration (8 gathered loads + 1 256-bit load from &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;offset&lt;/code&gt; per 4 iterations) and newer clang versions even manage to do it, if you allow some inlining so they can see the size and alignment of the buffer. However, &lt;a href=&quot;https://gist.github.com/travisdowns/b8294098c5082886f4a043ef8b6607bd&quot;&gt;the result&lt;/a&gt; is not good: it was more than twice as slow as the scalar approach.&lt;/p&gt;

&lt;h2 id=&quot;memory-and-cache-bandwidth&quot;&gt;Memory and Cache Bandwidth&lt;/h2&gt;

&lt;p&gt;The load and store limits discuss the ideal scenario where loads and stores hit in L1 (or hit in L1 “on average” enough to not slow things down), but there are throughput limits for other levels of the cache. If your know your loads hit primarily in a particular level of the cache you can use these limits to get a speed limit.&lt;/p&gt;

&lt;p&gt;The limits are listed in &lt;em&gt;cache lines per cycle&lt;/em&gt; and not in bytes, because that’s how you need to count the accesses: in unique cache lines accessed. The hardware transfers full lines. You can achieve these limits, but you may not be able to consume all the bytes from each cache line, because demand accesses to the L1 cache cannot occur on the same cycle that the L1 cache receives data from the outer cache levels. So, for example, the L2 can provide 64 bytes of data to the L1 cache per cycle, but you cannot &lt;em&gt;also&lt;/em&gt; access 64 bytes every cycle since the L1 cannot satisfy those reads from the core &lt;em&gt;and&lt;/em&gt; the incoming data from the L2 every cycle. All the gory details are &lt;a href=&quot;https://github.com/travisdowns/uarch-bench/wiki/How-much-bandwidth-does-the-L2-have-to-give,-anyway%3F&quot;&gt;over here&lt;/a&gt;.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Vendor&lt;/th&gt;
      &lt;th&gt;Microarchitecture&lt;/th&gt;
      &lt;th&gt;L2&lt;/th&gt;
      &lt;th&gt;L3 (Shared)&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Intel&lt;/td&gt;
      &lt;td&gt;&lt;abbr title=&quot;Intel's Cannon Lake (client) architecture, the i3-8121U was the only SKU ever released&quot;&gt;CNL&lt;/abbr&gt;&lt;/td&gt;
      &lt;td&gt;0.75&lt;/td&gt;
      &lt;td&gt;0.2 - 0.3&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Intel&lt;/td&gt;
      &lt;td&gt;&lt;abbr title=&quot;Intel's Skylake (server) architecture including Skylake-SP, Skylake-X and Skylake-W&quot;&gt;SKX&lt;/abbr&gt;&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;~0.1 (?)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Intel&lt;/td&gt;
      &lt;td&gt;&lt;abbr title=&quot;Intel's Skylake (client) architecture, aka 6th Generation Intel Core i3,i5,i7&quot;&gt;SKL&lt;/abbr&gt;&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0.2 - 0.3&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Intel&lt;/td&gt;
      &lt;td&gt;&lt;abbr title=&quot;Intel's Haswell architecture, aka 4th Generation Intel Core i3,i5,i7&quot;&gt;HSW&lt;/abbr&gt;&lt;/td&gt;
      &lt;td&gt;0.5&lt;/td&gt;
      &lt;td&gt;0.2 - 0.3&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;AMD&lt;/td&gt;
      &lt;td&gt;Zen&lt;/td&gt;
      &lt;td&gt;0.5&lt;/td&gt;
      &lt;td&gt;0.5&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;AMD&lt;/td&gt;
      &lt;td&gt;Zen2&lt;/td&gt;
      &lt;td&gt;0.5&lt;/td&gt;
      &lt;td&gt;0.5&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;The very poor figure of 0.1 cache lines per cycle (about 6-7 bytes a cycle) from L3 on &lt;abbr title=&quot;Intel's Skylake (server) architecture including Skylake-SP, Skylake-X and Skylake-W&quot;&gt;SKX&lt;/abbr&gt; is at odds with Intel’s manuals, but it’s what I measured on a W-2104. For architectures earlier than Haswell I think the numbers will be similar back to Sandy Bridge.&lt;/p&gt;

&lt;p&gt;If your accesses go to a mix of cache levels: you will probably get slightly worse bandwidth than what you’d get if you calculated the speed limit based on the assumption the cache levels can be accessed independently.&lt;/p&gt;

&lt;p&gt;Memory bandwidth is a bit more complicated. You can calculate your theoretical value based on your memory channel count (or look it up on ARK), but this is complicated by the fact that many chips cannot reach the maximum bandwidth from a single core since they cannot generate enough requests to saturate the DRAM bus, due to limited fill buffers. So you are better off just measuring it.&lt;/p&gt;

&lt;h3 id=&quot;remedies-3&quot;&gt;Remedies&lt;/h3&gt;

&lt;p&gt;The usual remedies to improve caching performance apply: pack your structures more tightly, try to ensure locality of reference and prefetcher friendly access patterns, use cache blocking, etc.&lt;/p&gt;

&lt;h2 id=&quot;carried-dependency-chains&quot;&gt;Carried Dependency Chains&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Sum of latencies in the longest carried dependency chain&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Everything discussed so far is a limited based on &lt;em&gt;throughput&lt;/em&gt; - the machine can only do so many things per cycle, and we count the number of things and apply those limits to determine the speed limit. We don’t care about how long each instruction takes to finish (as long as we can &lt;em&gt;start&lt;/em&gt; one per cycle), or from where it gets its inputs. In practice, that can matter a lot.&lt;/p&gt;

&lt;p&gt;Let’s consider, for example, a modified version of the multiply loop above, one that’s a lot simpler:&lt;/p&gt;

&lt;div class=&quot;language-c++ highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;size_t&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;++&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;kt&quot;&gt;uint32_t&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;];&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;product&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This does only a single multiplication per iteration, and compiles to the following tight loop:&lt;/p&gt;

&lt;div class=&quot;language-c++ highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;mi&quot;&gt;50&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;imul&lt;/span&gt;   &lt;span class=&quot;n&quot;&gt;eax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;DWORD&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;PTR&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rdi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;add&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;rdi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mh&quot;&gt;0x4&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;cmp&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;rdi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rdx&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;jne&lt;/span&gt;    &lt;span class=&quot;mi&quot;&gt;50&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;That’s only 3 fused uops, so our pipeline speed limit is 0.75 cycles/iteration. But wait, we know the imul needs &lt;abbr title=&quot;port 1 (GP and SIMD ALU, integer mul)&quot;&gt;p1&lt;/abbr&gt;, and the other two operations can go to other ports, so the &lt;abbr title=&quot;port 1 (GP and SIMD ALU, integer mul)&quot;&gt;p1&lt;/abbr&gt; pressure means a limit of 1 cycle/iteration. What does the real world have to say?&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;./uarch-bench.sh --timer=perf --test-name=cpp/mul-chain --extra-events=$PE_PORTS
              Benchmark       Cycles       uops_d       uops_d       uops_d       uops_d       uops_d       uops_d       uops_d       uops_d
Chained multiplications         2.98         0.50         1.00         0.50         0.50         0.00         0.50         1.00         0.00
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Bleh, 2.98 cycles, or 3x slower than we predicted.&lt;/p&gt;

&lt;p&gt;What happened? As it turns out, the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;imul&lt;/code&gt; instruction has a &lt;em&gt;latency&lt;/em&gt; of 3 cycles. That means that the result is not available until 3 cycles after the operation starts executing. This contrasts with 1 latency cycle for most simple arithmetic operations. Since on each iteration the multiply instruction depends on the result of the result of the &lt;em&gt;previous&lt;/em&gt; iteration’s multiply&lt;sup id=&quot;fnref:srcdest&quot;&gt;&lt;a href=&quot;#fn:srcdest&quot; class=&quot;footnote&quot;&gt;17&lt;/a&gt;&lt;/sup&gt;, every multiply can only start when the previous one finished, i.e., 3 cycles later. So 3 cycles is the speed limit for this loop.&lt;/p&gt;

&lt;p&gt;Note that we mostly care about &lt;em&gt;loop carried&lt;/em&gt; dependencies, which are dependency chains that cross loop iterations, i.e., where some output register in one iteration is used as an input register for the same chain in the next iteration. In the example, the carried chain involves only &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;eax&lt;/code&gt;, but more complex chains are common in practice. In the earlier example, the four &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;imul&lt;/code&gt; instructions &lt;em&gt;did&lt;/em&gt; form a chain:&lt;/p&gt;

&lt;div class=&quot;language-nasm highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;mi&quot;&gt;930&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;mov&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;r10d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;DWORD&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rdi&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rcx&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mh&quot;&gt;0x4&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;; load&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;mov&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;r8d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;r10d&lt;/span&gt;                   &lt;span class=&quot;c&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;imul&lt;/span&gt;   &lt;span class=&quot;n&quot;&gt;r8d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ecx&lt;/span&gt;                    &lt;span class=&quot;c&quot;&gt;; imul1&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;imul&lt;/span&gt;   &lt;span class=&quot;n&quot;&gt;r8d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;edx&lt;/span&gt;                    &lt;span class=&quot;c&quot;&gt;; imul2&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;imul&lt;/span&gt;   &lt;span class=&quot;n&quot;&gt;r8d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ecx&lt;/span&gt;                    &lt;span class=&quot;c&quot;&gt;; imul3&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;add&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;rcx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mh&quot;&gt;0x1&lt;/span&gt;                    &lt;span class=&quot;c&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;imul&lt;/span&gt;   &lt;span class=&quot;n&quot;&gt;r8d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;r9d&lt;/span&gt;                    &lt;span class=&quot;c&quot;&gt;; imul4&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;mov&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;r9d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;r10d&lt;/span&gt;                   &lt;span class=&quot;c&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;add&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;eax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;r8d&lt;/span&gt;                    &lt;span class=&quot;c&quot;&gt;; add&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;cmp&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;rcx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rsi&lt;/span&gt;                    &lt;span class=&quot;c&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;jne&lt;/span&gt;    &lt;span class=&quot;mi&quot;&gt;930&lt;/span&gt;                        &lt;span class=&quot;c&quot;&gt;;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Note how each &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;imul&lt;/code&gt; depends on the previous through the input/output &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;r8d&lt;/code&gt;. Finally, the result is added to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;eax&lt;/code&gt; ,and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;eax&lt;/code&gt; is indeed used as input in the next iteration, so do we have a loop-carried dependency chain? Yes - but a very small one involving only &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;eax&lt;/code&gt;. The dependency chain looks like this:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;iteration 1       load -&amp;gt; imul1 -&amp;gt; imul -&amp;gt; imul -&amp;gt; imul -&amp;gt; add
                                                            |
                                                            v
iteration 2       load -&amp;gt; imul1 -&amp;gt; imul -&amp;gt; imul -&amp;gt; imul -&amp;gt; add
                                                            |
                                                            v
iteration 3       load -&amp;gt; imul1 -&amp;gt; imul -&amp;gt; imul -&amp;gt; imul -&amp;gt; add
                                                            |
                                                            v
etc ...                                                    ...
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;So yes, there is a dependent chain there, and the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;imul&lt;/code&gt; instructions are &lt;em&gt;connected&lt;/em&gt; to that chain, but they don’t participate in the carried part. Only the single-cycle latency &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;add&lt;/code&gt; instruction participates in the carried dependency chain, so the implied speed limit is 1 cycle/iteration. In fact, all of our examples so far have had carried dependency chains, but they have all been small enough never to be the dominating factor. You may also have &lt;em&gt;multiple&lt;/em&gt; carried dependency chains in a loop: the speed limit is set by the longest.&lt;/p&gt;

&lt;p&gt;I’ve only touched on this topic and won’t go much further here: for a deeper look check out Fabian Giesen’s &lt;a href=&quot;https://fgiesen.wordpress.com/2018/03/05/a-whirlwind-introduction-to-dataflow-graphs/&quot;&gt;A whirlwind introduction to dataflow graphs&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Finally, you may have noticed something interesting about the benchmark result of 2.98 cycles. In every other case, the measured time was equal or slightly &lt;em&gt;more&lt;/em&gt; than the speed limit, due to test overhead. How were we able to break the speed limit in this case and come &lt;em&gt;under&lt;/em&gt; 3.00 cycles, albeit by less than 1%? Maybe it’s just measurement error - the clocks aren’t precise enough to time this more precisely?&lt;/p&gt;

&lt;p&gt;Nope. The effect is real and is due to the structure of the test. We run the multiplication code shown above on a buffer of 4096 elements, so there are 4096 iterations. The benchmark loop that calls that function, &lt;em&gt;itself&lt;/em&gt; runs 1000 iterations, each one calling the 4096-iteration inner loop. What happens to get the 2.98 is that in between each call of the inner loop, the multiplication chains &lt;em&gt;can&lt;/em&gt; be overlapped. Each chain is 4096-elements long, but the start of each function starts a new chain:&lt;/p&gt;

&lt;div class=&quot;language-c++ highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kt&quot;&gt;uint32_t&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;mul_chain&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;uint32_t&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;size_t&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;uint32_t&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;m&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;kt&quot;&gt;uint32_t&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;product&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;size_t&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;++&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
      &lt;span class=&quot;c1&quot;&gt;// ...&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Note the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;product = 1&lt;/code&gt; - that’s a new chain. So some small amount of overlap is possible near the end of each loop, which shaves about 80-90 cycles off the loop time (i.e., something like ~30 multiplications get to overlap). The size of the overlap is limited by the &lt;abbr title=&quot;Out-of-order execution allows CPUs to execute instructions out of order with respect to the source.&quot;&gt;out-of-order&lt;/abbr&gt; buffer structures in the CPU, in particular the &lt;a href=&quot;https://en.wikipedia.org/wiki/Re-order_buffer&quot;&gt;re-order buffer&lt;/a&gt; and &lt;a href=&quot;https://en.wikipedia.org/wiki/Reservation_station&quot;&gt;scheduler&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;tools-1&quot;&gt;Tools&lt;/h3&gt;

&lt;p&gt;As fun as tracing out dependency chains by hand is, you’ll eventually want a tool to do this for you. All of IACA, OSACA and llvm-mca can do this type of latency analysis and identity loop carried dependencies implicitly. For example, llvm-mca &lt;a href=&quot;https://godbolt.org/z/tD6dd-&quot;&gt;correctly identifies&lt;/a&gt; that this loop will take 3 cycles/iteration.&lt;/p&gt;

&lt;h3 id=&quot;remedies-4&quot;&gt;Remedies&lt;/h3&gt;

&lt;p&gt;The basic remedy is that you have to shorten or break up the dependency chains.&lt;/p&gt;

&lt;p&gt;For example, maybe you can use lower latency instructions like addition or shift instead of multiplication. A more generally applicable trick is to turn one long dependency chain into several parallel ones. In the example above, the associativity property of integer multiplication&lt;sup id=&quot;fnref:assoc&quot;&gt;&lt;a href=&quot;#fn:assoc&quot; class=&quot;footnote&quot;&gt;18&lt;/a&gt;&lt;/sup&gt; allows us to do the multiplications in any order. In particular, we could accumulate every third element into a separate product and multiply them all at the end, like so:&lt;/p&gt;

&lt;div class=&quot;language-c++ highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;    &lt;span class=&quot;kt&quot;&gt;uint32_t&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p3&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p4&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;size_t&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;p1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;];&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;p2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;];&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;p3&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;];&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;p4&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;];&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
    &lt;span class=&quot;kt&quot;&gt;uint32_t&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;product&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p3&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This test runs at 1.00 cycles per iteration, so the latency chain speed limit has been removed. Well, it’s still there: each iteration above takes at least 3 cycles because of the four carried dependency chains between each iteration, but since we are doing 4x as much work now, the &lt;abbr title=&quot;port 1 (GP and SIMD ALU, integer mul)&quot;&gt;p1&lt;/abbr&gt; port limit becomes the dominant limit.&lt;/p&gt;

&lt;p&gt;Compilers can sometimes make this transformation for you, but not always. In particular, gcc is reluctant to unroll loops at any optimization level, and unrolling loops is often a prerequisite for this transformation, so often you are stuck doing it by hand.&lt;/p&gt;

&lt;h2 id=&quot;front-end-effects&quot;&gt;Front End Effects&lt;/h2&gt;

&lt;p&gt;I’m going to largely gloss over this one. It really deserves a whole blog post, but in recent Intel and AMD architectures the prevalence of front-end effects being the limiting factor in loops has dropped a lot. The introduction of the &lt;abbr title=&quot;Micro-operation: instructions are translated into one or more uops, which are simple operations executed by the CPU's execution units.&quot;&gt;uop&lt;/abbr&gt; cache and better decoders means that it is not as common as it used to be. For a complete&lt;sup id=&quot;fnref:sklfe&quot;&gt;&lt;a href=&quot;#fn:sklfe&quot; class=&quot;footnote&quot;&gt;19&lt;/a&gt;&lt;/sup&gt; treatment see Agner’s &lt;a href=&quot;https://www.agner.org/optimize/#manual_microarch&quot;&gt;microarchitecture guide&lt;/a&gt;, starting with section 9.1 through 9.7 for Sandy Bridge (and then the corresponding sections for each later &lt;abbr title=&quot;Microarchitecture: a specific implementation of an ISA, e.g., &amp;quot;Haswell microarchitecture&amp;quot;.&quot;&gt;uarch&lt;/abbr&gt; you are interested in).&lt;/p&gt;

&lt;p&gt;If you see an effect that depends on code alignment, especially in a cyclic pattern with a period 16, 32 or 64 bytes, it is very likely to be a front-end effect. There are &lt;a href=&quot;https://twitter.com/trav_downs/status/1124152129294409729&quot;&gt;hacks you can use to test this&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;First are simple absolute front-end limits to delivered uops/cycle depending on where the uops are coming from&lt;sup id=&quot;fnref:lsdno&quot;&gt;&lt;a href=&quot;#fn:lsdno&quot; class=&quot;footnote&quot;&gt;20&lt;/a&gt;&lt;/sup&gt;:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Table 1: Uops delivered per cycle&lt;/strong&gt;&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Architecture&lt;/th&gt;
      &lt;th&gt;Microcode (&lt;abbr title=&quot;Intel's name for the microcode engine: a component handles complex instructions which require more than 4 uops using microcode which feeds uops directly into the IDQ.&quot;&gt;MSROM&lt;/abbr&gt;)&lt;/th&gt;
      &lt;th&gt;Decoder (&lt;abbr title=&quot;Intel's name for the &amp;quot;legacy&amp;quot; decoder, i.e., the decoder that usually decodes instructions when they are not found in the MSROM.&quot;&gt;MITE&lt;/abbr&gt;)&lt;/th&gt;
      &lt;th&gt;Uop cache (DSB)&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&amp;lt;= Broadwell&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&amp;gt;= Skylake&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;6&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;These might look like important values. I even made a table, one of only two in this whole post. They aren’t very important though, because they are all equal to or larger than the pipeline limit of 4. In fact it is &lt;a href=&quot;https://twitter.com/trav_downs/status/1106403269792788480&quot;&gt;hard&lt;/a&gt; to even carefully design a micro-benchmark which definitively shows the difference between the 5-wide decode on &lt;abbr title=&quot;Intel's Skylake (client) architecture, aka 6th Generation Intel Core i3,i5,i7&quot;&gt;SKL&lt;/abbr&gt; and the 4-wide on Haswell and earlier. So you can mostly ignore these numbers.&lt;/p&gt;

&lt;p&gt;The more important limitations are specific to the individual sources. For example:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The legacy decoder (&lt;abbr title=&quot;Intel's name for the &amp;quot;legacy&amp;quot; decoder, i.e., the decoder that usually decodes instructions when they are not found in the MSROM.&quot;&gt;MITE&lt;/abbr&gt;) can only handle up to 16 instruction bytes per cycle, so any time instruction length averages more than four bytes decode throughput will necessarily be lower than four. Certain patterns will have worse throughput than predicted by this formula, e.g., 7 instructions in a 16 byte block will decode in a 6-1-6-1 pattern.&lt;/li&gt;
  &lt;li&gt;Only one of the 4 or 5 legacy decoders can handle instructions which generate more than one &lt;abbr title=&quot;Micro-operation: instructions are translated into one or more uops, which are simple operations executed by the CPU's execution units.&quot;&gt;uop&lt;/abbr&gt;, so a series of instructions which generate 2 uops will only decode at 1 per cycle (2 uops per cycle).&lt;/li&gt;
  &lt;li&gt;Only one &lt;abbr title=&quot;Micro-operation: instructions are translated into one or more uops, which are simple operations executed by the CPU's execution units.&quot;&gt;uop&lt;/abbr&gt; cache entry (with up to 6 uops) can be accessed per cycle. For larger loops this is rarely a bottleneck, but it means that any loop that crosses a &lt;abbr title=&quot;Micro-operation: instructions are translated into one or more uops, which are simple operations executed by the CPU's execution units.&quot;&gt;uop&lt;/abbr&gt; cache boundary (32 bytes up to and including Broadwell, 64 bytes in Skylake and beyond) will always take 2 cycles, since two &lt;abbr title=&quot;Micro-operation: instructions are translated into one or more uops, which are simple operations executed by the CPU's execution units.&quot;&gt;uop&lt;/abbr&gt; cache entries are involved. It is not unusual to find small loops which normally take as little as 1 cycle split by such boundaries suddenly taking 2 cycles.&lt;/li&gt;
  &lt;li&gt;Instructions which use microcode, such as gather (pre-Skylake) have additional restrictions and throughput limitations.&lt;/li&gt;
  &lt;li&gt;The &lt;abbr title=&quot;Lysergic acid diethylamide or Loop stream detector, but in the context of this blog probably the latter: The so-called loop buffer that can cache small loops of up to ~64 uops on recent Intel architectures. Not actually a separate structure: the hardware justs locks the loop down in the IDQ.&quot;&gt;LSD&lt;/abbr&gt; suffers from reduced throughput at the boundary between one iteration and the next, although hardware unrolling reduces the impact of the effect. Full details &lt;a href=&quot;https://stackoverflow.com/a/39940932&quot;&gt;are on Stack Overflow&lt;/a&gt;. Note that the &lt;abbr title=&quot;Lysergic acid diethylamide or Loop stream detector, but in the context of this blog probably the latter: The so-called loop buffer that can cache small loops of up to ~64 uops on recent Intel architectures. Not actually a separate structure: the hardware justs locks the loop down in the IDQ.&quot;&gt;LSD&lt;/abbr&gt; is disabled on most recent CPUs due to a bug. It is re-enabled on some of the most recent chips (&lt;abbr title=&quot;Intel's Cannon Lake (client) architecture, the i3-8121U was the only SKU ever released&quot;&gt;CNL&lt;/abbr&gt; and maybe Cascade Lake).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Again, this is only scratching the surface - see Agner for a comprehensive treatment.&lt;/p&gt;

&lt;h2 id=&quot;store-throughput-limit&quot;&gt;Store Throughput Limit&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;1 store per cycle&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Modern Intel and AMD CPUs can perform at most one store per cycle. No matter what, you won’t exceed that. For many algorithms that make a predictable number of stores, this is a useful upper bound on a performance. For example, a 32-bit radix sort that makes 4 passes and does a store per element for each pass will never operate faster than 4 cycles per element (in radix sort, actual performance usually ends up much worse so this isn’t the dominant factor for most implementations).&lt;/p&gt;

&lt;p&gt;This limit applies also to vector scatter instructions, where each element counts as “one” against this limit. Like loads, a store that crosses a cache line counts as two, but other unaligned stores only count as one on Intel. On AMD the situation is more complicated: the penalties for stores that cross a boundary is larger, and it’s not just 64-byte boundaries that matter - more &lt;a href=&quot;https://www.realworldtech.com/forum/?threadid=176780&amp;amp;curpostid=176849&quot;&gt;details here&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;split-cache-lines-1&quot;&gt;Split Cache Lines&lt;/h3&gt;

&lt;p&gt;On Intel, stores that cross a cache line boundary (64 bytes) count as two, but stores of any other alignment suffer no penalty.&lt;/p&gt;

&lt;p&gt;On AMD Zen, any store which crosses a 16 byte boundary suffers a significant penalty: such stores can only execute one per &lt;em&gt;five&lt;/em&gt; cycles, so maybe you should count these as five for the purposes of this limit. However, it is possible that this penalty isn’t cumulative with other stores but just represents worst case where many such stores occur back-to-back but the performance when mixed with non-crossing stores is better than this worst case. For example 5 non-crossing store + 1 crossing one might not count as 10 but rather 6 or 7. More testing needed on that one. Suffice it to say you should avoid boundary-crossing stores if you can.&lt;/p&gt;

&lt;h3 id=&quot;remedies-5&quot;&gt;Remedies&lt;/h3&gt;

&lt;p&gt;Remove unnecessary stores from your core loops. If you are often storing the same value repeatedly to the same location, it can even be profitable to check that the value is different, which requires a load, and only do the store if different, since this can replace a store with a load. Most of all, you want to take advantage of vectorized stores if possible: you can do 8x 32-bit stores in one cycle with a single vectorized store. Of course, if your stores are not contiguous, this will be difficult or impossible.&lt;/p&gt;

&lt;h2 id=&quot;complex-addressing-limit&quot;&gt;Complex Addressing Limit&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Max of 1 load (any addressing) concurrent with a store with complex addressing per cycle.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;This limit is Intel specific.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;The load and store limits above are written as if they are independent. That is, they imply that you can do 2 loads &lt;strong&gt;and&lt;/strong&gt; 1 store per cycle. Sometimes that is true, but it depends on the addressing modes used.&lt;/p&gt;

&lt;p&gt;Each load and store operation needs an &lt;em&gt;address generation&lt;/em&gt; which happens in an &lt;abbr title=&quot;Address Generation Unit&quot;&gt;AGU&lt;/abbr&gt;. There are three AGUs on modern Intel chips: &lt;abbr title=&quot;port 2 (load/store AGU)&quot;&gt;p2&lt;/abbr&gt;, &lt;abbr title=&quot;port 3 (load/store AGU)&quot;&gt;p3&lt;/abbr&gt; and &lt;abbr title=&quot;port 7 (limited store AGU)&quot;&gt;p7&lt;/abbr&gt;. However, &lt;abbr title=&quot;port 7 (limited store AGU)&quot;&gt;p7&lt;/abbr&gt; is restricted: it can &lt;em&gt;only&lt;/em&gt; be used by stores, and it can only be used if the store addressing mode is simple. &lt;a href=&quot;https://stackoverflow.com/a/51664696&quot;&gt;Simple addressing&lt;/a&gt; is anything that is of the form &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;[base_reg + offset]&lt;/code&gt; where &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;offset&lt;/code&gt; is in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;[0, 2047]&lt;/code&gt;. So &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;[rax + 1024]&lt;/code&gt; is simple addressing, but all of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;[rax + 4096]&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;[rax + rcx * 2]&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;[rax * 2]&lt;/code&gt; are not.&lt;/p&gt;

&lt;p&gt;To apply this limit, count &lt;em&gt;all&lt;/em&gt; load and any stores with complex addressing: these operations cannot execute at more than 2 per cycle.&lt;/p&gt;

&lt;h3 id=&quot;remedies-6&quot;&gt;Remedies&lt;/h3&gt;

&lt;p&gt;At the assembly level, the main remedy is make sure that your stores use simple addressing modes. Usually you do this by incrementing a pointer by the size of the element rather indexed addressing modes.&lt;/p&gt;

&lt;p&gt;That is, rather than this:&lt;/p&gt;

&lt;div class=&quot;language-nasm highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;mov&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rdi&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rax&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rdx&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;add&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;You want this:&lt;/p&gt;

&lt;div class=&quot;language-nasm highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;mov&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[rdi],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rdx&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;add&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rdi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Of course, that’s often simpler said than done: indexed addressing modes are very useful for using a single loop counter to access multiple arrays, and also when the value of the loop counter is directly used in the loop (as opposed to simply being used for addressing). For example, consider the following loop which writes the element-wise sum of two arrays to a third array:&lt;/p&gt;

&lt;div class=&quot;language-c++ highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;size_t&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;size_t&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;++&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;];&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The loop compiles to the following assembly:&lt;/p&gt;

&lt;div class=&quot;language-nasm highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;L3&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;mov&lt;/span&gt;     &lt;span class=&quot;n&quot;&gt;r8d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;DWORD&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;PTR&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rsi&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rax&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;add&lt;/span&gt;     &lt;span class=&quot;n&quot;&gt;r8d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;DWORD&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;PTR&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rdi&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rax&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;mov&lt;/span&gt;     &lt;span class=&quot;n&quot;&gt;DWORD&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;PTR&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rdx&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rax&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;r8d&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;add&lt;/span&gt;     &lt;span class=&quot;n&quot;&gt;rax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;cmp&lt;/span&gt;     &lt;span class=&quot;n&quot;&gt;rcx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rax&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;jne&lt;/span&gt;     &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;L3&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This loop will be limited by the complex addressing limitation to 1.5 cycles per iteration, since there are 1 store that uses complex addressing, plus one load.&lt;/p&gt;

&lt;p&gt;We could use separate pointers for each array and increment all of them, like:&lt;/p&gt;

&lt;div class=&quot;language-nasm highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;L3&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;mov&lt;/span&gt;     &lt;span class=&quot;n&quot;&gt;r8d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;DWORD&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;PTR&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[rsi]&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;add&lt;/span&gt;     &lt;span class=&quot;n&quot;&gt;r8d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;DWORD&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;PTR&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[rdi]&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;mov&lt;/span&gt;     &lt;span class=&quot;n&quot;&gt;DWORD&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;PTR&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[rdx],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;r8d&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;add&lt;/span&gt;     &lt;span class=&quot;n&quot;&gt;rsi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;add&lt;/span&gt;     &lt;span class=&quot;n&quot;&gt;rdi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;add&lt;/span&gt;     &lt;span class=&quot;n&quot;&gt;rdx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;cmp&lt;/span&gt;     &lt;span class=&quot;n&quot;&gt;rcx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rdx&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;jne&lt;/span&gt;     &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;L3&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Everything uses simple addressing, great! However, we’ve added two uops and so the speed limit is pipeline width: 7/4 = 1.75, so it will probably be slower than before.&lt;/p&gt;

&lt;p&gt;The trick is to only use simple addressing for the store, and calculate the load addresses relative to the store address:&lt;/p&gt;

&lt;div class=&quot;language-nasm highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;L3&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;mov&lt;/span&gt;     &lt;span class=&quot;n&quot;&gt;eax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;DWORD&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;PTR&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rdx&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rsi&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;; rsi and rdi have been adjusted so that&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;add&lt;/span&gt;     &lt;span class=&quot;n&quot;&gt;eax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;DWORD&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;PTR&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rdx&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rdi&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;; rsi+rdx points to a and rdi+rdx to b&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;mov&lt;/span&gt;     &lt;span class=&quot;n&quot;&gt;DWORD&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;PTR&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[rdx],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;eax&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;add&lt;/span&gt;     &lt;span class=&quot;n&quot;&gt;rdx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;cmp&lt;/span&gt;     &lt;span class=&quot;n&quot;&gt;rcx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rdx&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;ja&lt;/span&gt;      &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;L3&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;When working in a higher level language, you may not always be able to convince the compiler to generate the code we want as it might simply see through our transformations. In this case, however, &lt;a href=&quot;https://godbolt.org/z/PPutUu&quot;&gt;we can convince&lt;/a&gt; gcc to generate the code we want by writing out the transformation ourselves:&lt;/p&gt;

&lt;div class=&quot;language-c++ highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;sum2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;size_t&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;end&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;d&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;kt&quot;&gt;ptrdiff_t&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a_offset&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
    &lt;span class=&quot;kt&quot;&gt;ptrdiff_t&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b_offset&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;d&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;end&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;d&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;++&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;d&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;d&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a_offset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;d&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b_offset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This is UB all over the place if you pass in arbitrary arrays, because we subtract unrelated pointers (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;a - d&lt;/code&gt;) and use pointer arithmetic which outside of the bounds of the original array (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;d + a_offset&lt;/code&gt;) - but I’m not aware of any compiler that will take advantage of this (as a standalone function it seems unlikely that will ever be the case: because the arrays all &lt;em&gt;could&lt;/em&gt; be related, so the function isn’t always UB). Still you should avoid stuff like this unless you have a &lt;em&gt;really&lt;/em&gt; good reason to push the boundaries. You could achieve the same effect with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;uintptr_t&lt;/code&gt; which isn’t UB but only unspecified, and that will work on every platform I’m aware of.&lt;/p&gt;

&lt;p&gt;Another way to get simple addressing without adding too much overhead for separate loop pointers is to unroll the loop a little bit. The increment only needs to be done once per iteration, so every unroll reduces the cost.&lt;/p&gt;

&lt;p&gt;Note that even if stores have non-complex addressing, it may not be possible to sustain 2 loads/1 store, because the store may sometimes choose one of the port 2 or port 3 AGUs instead, starving a load that cycle.&lt;/p&gt;

&lt;h2 id=&quot;taken-branches&quot;&gt;Taken Branches&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Intel: 1 per 2 cycles (see exception below)&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;If you believe the instruction tables, one taken branch can be executed per cycle, but experiments show that this is true only for very small loops with a single backwards branch. For larger loops or any forward branches, the limit is 1 per 2 cycles.&lt;/p&gt;

&lt;p&gt;So avoid many dense taken branches: organize the likely path instead as untaken. This is something you want to do anyways for front-end throughput and code density.&lt;/p&gt;

&lt;h2 id=&quot;out-of-order-limits&quot;&gt;Out of Order Limits&lt;/h2&gt;

&lt;p&gt;Here we will cover several limits which all affect the effective window over which the processor can reorder instructions. These limits all have the same pattern: in order to execute instructions out of order, the CPU needs to track in-flight operations in certain structures. If any of these structures becomes full, the effect is the same: no more operations are issued until space in that structure is freed. Already issued instructions can still execute, but no more operations else will enter the pool of waiting ops. In general, we talk about the &lt;em&gt;&lt;abbr title=&quot;Out-of-order execution allows CPUs to execute instructions out of order with respect to the source.&quot;&gt;out-of-order&lt;/abbr&gt; window&lt;/em&gt; which is roughly the number of instructions/operations that can be in progress, counting from the oldest in-progress instruction to the newest. The limits in this section put an effective limit on this window.&lt;/p&gt;

&lt;p&gt;While the effect is the same for each limit, the size of the structures and which operations that are tracked in them vary, so we focus on describing that.&lt;/p&gt;

&lt;p&gt;Note that the size of the window is not a hard performance limit in itself: you can’t use it to directly establish an upper bound on cycles per iterations or whatever (i.e., the units for the window aren’t “per cycle”) - but you can use it in concert with other analysis to refine the estimate.&lt;/p&gt;

&lt;p&gt;Until now, we have been implicitly assuming an &lt;em&gt;infinite&lt;/em&gt; out of order window. That’s why we said, for example, that only loop carried dependencies matter when calculating dependency chains; the implicit assumption is that there is enough &lt;abbr title=&quot;Out-of-order execution allows CPUs to execute instructions out of order with respect to the source.&quot;&gt;out-of-order&lt;/abbr&gt; magic to reorder different loop iterations to hide the effect of all the other chains. Of course, on real CPUs, there is a limit to the magic: if your loops have 1,000 instructions per iteration, and the &lt;abbr title=&quot;Out-of-order execution allows CPUs to execute instructions out of order with respect to the source.&quot;&gt;out-of-order&lt;/abbr&gt; window is only 100 instructions, the CPU will not be able to overlap the much of each iteration at all: the different iterations are too far apart in instruction stream for significant overlap.&lt;/p&gt;

&lt;p&gt;All the discussion here refers to the &lt;em&gt;dynamic instruction stream&lt;/em&gt; - which is the actual stream of instructions seen by the CPU. This is opposed to the static instruction stream, which is the series of instructions as they appear in the binary. Inside a &lt;abbr title=&quot;a straight-line code sequence with no branches in except to the entry and no branches out except at the exit (Wikipedia).&quot;&gt;basic block&lt;/abbr&gt;, static and dynamic instruction streams are the same: the difference is that the dynamic stream follows all jumps, so it is a trace of actual execution.&lt;/p&gt;

&lt;p&gt;For example, take the following nested loops, with inner and outer iteration counts of 2 and 4:&lt;/p&gt;

&lt;div class=&quot;language-nasm highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;    &lt;span class=&quot;k&quot;&gt;xor&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rdx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rdx&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;mov&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;outer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;mov&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rcx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;inner&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;add&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rdx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rcx&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;dec&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rcx&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;jnz&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;inner&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;dec&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rax&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;jnz&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;outer&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The static instruction stream is just want you see above, 8 instructions in total. The dynamic instruction stream traces what happens at runtime, so the inner loop appears 8 times, for example:&lt;/p&gt;

&lt;div class=&quot;language-nasm highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;    &lt;span class=&quot;k&quot;&gt;xor&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rdx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rdx&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;mov&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;

    &lt;span class=&quot;c&quot;&gt;; first iteration of outer loop&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;mov&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rcx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;

    &lt;span class=&quot;c&quot;&gt;; inner loop 4x&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;add&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rdx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rcx&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;dec&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rcx&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;jnz&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;inner&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;add&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rdx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rcx&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;dec&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rcx&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;jnz&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;inner&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;add&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rdx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rcx&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;dec&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rcx&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;jnz&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;inner&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;add&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rdx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rcx&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;dec&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rcx&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;jnz&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;inner&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;jnz&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;outer&lt;/span&gt;

    &lt;span class=&quot;c&quot;&gt;; second iteration of outer loop&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;mov&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rcx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;

    &lt;span class=&quot;c&quot;&gt;; inner loop 4x&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;add&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rdx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rcx&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;dec&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rcx&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;jnz&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;inner&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;add&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rdx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rcx&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;dec&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rcx&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;jnz&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;inner&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;add&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rdx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rcx&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;dec&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rcx&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;jnz&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;inner&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;add&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rdx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rcx&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;dec&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rcx&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;jnz&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;inner&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;jnz&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;outer&lt;/span&gt;

    &lt;span class=&quot;c&quot;&gt;; done!&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;All that to say that when you are thinking about out of order window, you have to think about the dynamic instruction/&lt;abbr title=&quot;Micro-operation: instructions are translated into one or more uops, which are simple operations executed by the CPU's execution units.&quot;&gt;uop&lt;/abbr&gt; stream, not the static one. For a loop body with no jumps or calls, you can ignore this distinction. We also talk about &lt;em&gt;older&lt;/em&gt;, &lt;em&gt;oldest&lt;/em&gt;, &lt;em&gt;youngest&lt;/em&gt;, etc instructions - this simply refers to the relative position of instructions or operations in the dynamic stream: the first encountered instructions are the oldest (in the stream above, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;xor rdx, rdx&lt;/code&gt; is the oldest) and the most recently encountered instructions are the youngest.&lt;/p&gt;

&lt;p&gt;With that background out of the way, let’s look at the various &lt;abbr title=&quot;Out-of-order execution allows CPUs to execute instructions out of order with respect to the source.&quot;&gt;OoO&lt;/abbr&gt; limits next. Most of these limits have the same &lt;em&gt;effect&lt;/em&gt; which is to limit the available &lt;abbr title=&quot;Out-of-order execution allows CPUs to execute instructions out of order with respect to the source.&quot;&gt;out-of-order&lt;/abbr&gt; window, stalling issue until a resource becomes available. They differ mostly in &lt;em&gt;what&lt;/em&gt; they count, and how many of that thing can be buffered.&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;ooo-table&quot;&gt;&lt;/a&gt;First, here’s a big table of all the resource sizes&lt;sup id=&quot;fnref:snote&quot;&gt;&lt;a href=&quot;#fn:snote&quot; class=&quot;footnote&quot;&gt;21&lt;/a&gt;&lt;/sup&gt; we’ll talk about the following sections.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Vendor&lt;/th&gt;
      &lt;th&gt;Uarch&lt;/th&gt;
      &lt;th&gt;&lt;abbr title=&quot;Re-order buffer: n ordered buffer which stores in-progress instructions on an out-of-order processor.&quot;&gt;ROB&lt;/abbr&gt; Size&lt;/th&gt;
      &lt;th&gt;Sched (RS)&lt;/th&gt;
      &lt;th&gt;Load Buffer&lt;/th&gt;
      &lt;th&gt;Store Buffer&lt;/th&gt;
      &lt;th&gt;Integer &lt;abbr title=&quot;Physical register file: The hardware registers used for renaming architectural (source visible) registers, usually much larger in number than the architectural register count.&quot;&gt;PRF&lt;/abbr&gt;&lt;/th&gt;
      &lt;th&gt;Vector &lt;abbr title=&quot;Physical register file: The hardware registers used for renaming architectural (source visible) registers, usually much larger in number than the architectural register count.&quot;&gt;PRF&lt;/abbr&gt;&lt;/th&gt;
      &lt;th&gt;Branches (BOB)&lt;/th&gt;
      &lt;th&gt;Calls&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Intel&lt;/td&gt;
      &lt;td&gt;Sandy Bridge&lt;/td&gt;
      &lt;td&gt;168&lt;/td&gt;
      &lt;td&gt;54&lt;/td&gt;
      &lt;td&gt;64&lt;/td&gt;
      &lt;td&gt;36&lt;/td&gt;
      &lt;td&gt;160&lt;/td&gt;
      &lt;td&gt;144&lt;/td&gt;
      &lt;td&gt;48&lt;/td&gt;
      &lt;td&gt;15&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Intel&lt;/td&gt;
      &lt;td&gt;Ivy Bridge&lt;/td&gt;
      &lt;td&gt;168&lt;/td&gt;
      &lt;td&gt;54&lt;/td&gt;
      &lt;td&gt;64&lt;/td&gt;
      &lt;td&gt;36&lt;/td&gt;
      &lt;td&gt;160&lt;/td&gt;
      &lt;td&gt;144&lt;/td&gt;
      &lt;td&gt;48&lt;/td&gt;
      &lt;td&gt;15&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Intel&lt;/td&gt;
      &lt;td&gt;Haswell&lt;/td&gt;
      &lt;td&gt;192&lt;/td&gt;
      &lt;td&gt;60&lt;/td&gt;
      &lt;td&gt;72&lt;/td&gt;
      &lt;td&gt;42&lt;/td&gt;
      &lt;td&gt;168&lt;/td&gt;
      &lt;td&gt;168&lt;/td&gt;
      &lt;td&gt;48&lt;/td&gt;
      &lt;td&gt;14&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Intel&lt;/td&gt;
      &lt;td&gt;Broadwell&lt;/td&gt;
      &lt;td&gt;192&lt;/td&gt;
      &lt;td&gt;64&lt;/td&gt;
      &lt;td&gt;72&lt;/td&gt;
      &lt;td&gt;42&lt;/td&gt;
      &lt;td&gt;168&lt;/td&gt;
      &lt;td&gt;168&lt;/td&gt;
      &lt;td&gt;48&lt;/td&gt;
      &lt;td&gt;14&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Intel&lt;/td&gt;
      &lt;td&gt;Skylake[-X]&lt;/td&gt;
      &lt;td&gt;224&lt;/td&gt;
      &lt;td&gt;97&lt;/td&gt;
      &lt;td&gt;72&lt;/td&gt;
      &lt;td&gt;56&lt;/td&gt;
      &lt;td&gt;180&lt;/td&gt;
      &lt;td&gt;168&lt;/td&gt;
      &lt;td&gt;48&lt;/td&gt;
      &lt;td&gt;14?&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Intel&lt;/td&gt;
      &lt;td&gt;Sunny Cove&lt;/td&gt;
      &lt;td&gt;352&lt;/td&gt;
      &lt;td&gt;?&lt;/td&gt;
      &lt;td&gt;128&lt;/td&gt;
      &lt;td&gt;72&lt;/td&gt;
      &lt;td&gt;?&lt;/td&gt;
      &lt;td&gt;?&lt;/td&gt;
      &lt;td&gt;48&lt;/td&gt;
      &lt;td&gt;?&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;AMD&lt;/td&gt;
      &lt;td&gt;Zen&lt;/td&gt;
      &lt;td&gt;192&lt;/td&gt;
      &lt;td&gt;180&lt;sup id=&quot;fnref:zensched&quot;&gt;&lt;a href=&quot;#fn:zensched&quot; class=&quot;footnote&quot;&gt;22&lt;/a&gt;&lt;/sup&gt;&lt;/td&gt;
      &lt;td&gt;72&lt;/td&gt;
      &lt;td&gt;44&lt;/td&gt;
      &lt;td&gt;168&lt;/td&gt;
      &lt;td&gt;160&lt;/td&gt;
      &lt;td&gt;?&lt;/td&gt;
      &lt;td&gt;?&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;AMD&lt;/td&gt;
      &lt;td&gt;Zen2&lt;/td&gt;
      &lt;td&gt;224&lt;/td&gt;
      &lt;td&gt;188&lt;sup id=&quot;fnref:zen2sched&quot;&gt;&lt;a href=&quot;#fn:zen2sched&quot; class=&quot;footnote&quot;&gt;23&lt;/a&gt;&lt;/sup&gt;&lt;/td&gt;
      &lt;td&gt;?&lt;/td&gt;
      &lt;td&gt;48&lt;/td&gt;
      &lt;td&gt;180&lt;/td&gt;
      &lt;td&gt;160&lt;/td&gt;
      &lt;td&gt;?&lt;/td&gt;
      &lt;td&gt;?&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h3 id=&quot;reorder-buffer-size&quot;&gt;Reorder Buffer Size&lt;/h3&gt;

&lt;p&gt;The &lt;abbr title=&quot;Re-order buffer: n ordered buffer which stores in-progress instructions on an out-of-order processor.&quot;&gt;ROB&lt;/abbr&gt; is the largest and most general out of order buffer: all uops, even those that don’t execute such as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;nop&lt;/code&gt; or zeroing idioms, take a slot in the &lt;abbr title=&quot;Re-order buffer: n ordered buffer which stores in-progress instructions on an out-of-order processor.&quot;&gt;ROB&lt;/abbr&gt;. This structure holds instructions from the point at which they are allocated (issued, in Intel speak) until they retire. It puts a hard upper limit on the &lt;abbr title=&quot;Out-of-order execution allows CPUs to execute instructions out of order with respect to the source.&quot;&gt;OoO&lt;/abbr&gt; window as measured from the oldest un-retired instruction to the youngest instruction that can be issued. On Intel, the &lt;abbr title=&quot;Re-order buffer: n ordered buffer which stores in-progress instructions on an out-of-order processor.&quot;&gt;ROB&lt;/abbr&gt; holds micro-fused ops, so the size is measured in the fused-domain.&lt;/p&gt;

&lt;p&gt;As an example, a load instruction takes a cache miss which means it cannot retire until the miss is complete. Let’s say the load takes 300 cycles to finish, which is a typical latency. Then, on an Haswell machine with a &lt;abbr title=&quot;Re-order buffer: n ordered buffer which stores in-progress instructions on an out-of-order processor.&quot;&gt;ROB&lt;/abbr&gt; size of 192, &lt;em&gt;at most&lt;/em&gt; 191 additional instructions can execute while waiting for the load: at that point the &lt;abbr title=&quot;Re-order buffer: n ordered buffer which stores in-progress instructions on an out-of-order processor.&quot;&gt;ROB&lt;/abbr&gt; window is exhausted and the core stalls. This puts an upper bound on the maximum &lt;abbr title=&quot;Instructions per cycle: calculated over an interval by measuring the number of instructions executed and the duration in cycles.&quot;&gt;IPC&lt;/abbr&gt; of the region of 192 / 300 = 0.64. It also puts a bound on the maximum &lt;abbr title=&quot;Memory level parallelism: having multiple misses to memory outstanding from a single core. When used as a metric, it refers to the average number of outstanding requests over some period.&quot;&gt;MLP&lt;/abbr&gt; achievable, since only loads that appear in the next 191 instructions can (potentially) execute in parallel with the original miss. In fact, this behavior is used by Henry Wong’s &lt;a href=&quot;https://github.com/travisdowns/robsize&quot;&gt;robsize tool&lt;/a&gt; to measure the &lt;abbr title=&quot;Re-order buffer: n ordered buffer which stores in-progress instructions on an out-of-order processor.&quot;&gt;ROB&lt;/abbr&gt; size and other &lt;abbr title=&quot;Out-of-order execution allows CPUs to execute instructions out of order with respect to the source.&quot;&gt;OoO&lt;/abbr&gt; buffer sizes, using a missed load followed by a series of filler instructions and finally another load miss. By varying the number of filler instructions and checking whether the loads executed in parallel or serially, the &lt;abbr title=&quot;Re-order buffer: n ordered buffer which stores in-progress instructions on an out-of-order processor.&quot;&gt;ROB&lt;/abbr&gt; size can be &lt;a href=&quot;http://blog.stuffedcow.net/2013/05/measuring-rob-capacity/&quot;&gt;determined experimentally&lt;/a&gt;.&lt;/p&gt;

&lt;h4 id=&quot;remedies-7&quot;&gt;Remedies&lt;/h4&gt;

&lt;p&gt;If you are hitting the &lt;abbr title=&quot;Re-order buffer: n ordered buffer which stores in-progress instructions on an out-of-order processor.&quot;&gt;ROB&lt;/abbr&gt; size limit, you should switch from optimizing the code for the usual metrics and instead try to reduce the number of uops. For example, a slower (longer latency, less throughput) instruction can be used to replace two instructions which would otherwise be faster. Similarly, micro-fusion helps because the &lt;abbr title=&quot;Re-order buffer: n ordered buffer which stores in-progress instructions on an out-of-order processor.&quot;&gt;ROB&lt;/abbr&gt; limit counts in the fused domain.&lt;/p&gt;

&lt;p&gt;Reorganizing the instruction stream can help too: if you hit the &lt;abbr title=&quot;Re-order buffer: n ordered buffer which stores in-progress instructions on an out-of-order processor.&quot;&gt;ROB&lt;/abbr&gt; limit after a specific long-latency instruction (usually a load miss) you may want to move expensive instructions into the shadow of that instruction so they can execute while the long latency instruction executes. In this way, there will be less work to do when the instruction completes. Similarly, you may want to “jam” loads that miss together: rather than spreading them out where they would naturally occur, putting them close together allows more of them to fit in the &lt;abbr title=&quot;Re-order buffer: n ordered buffer which stores in-progress instructions on an out-of-order processor.&quot;&gt;ROB&lt;/abbr&gt; window.&lt;/p&gt;

&lt;p&gt;In the specific case of load misses, software prefetching can help a lot: it enables you to start a load early, but prefetches can retire before the load completes, so there is no stalling. For example, if you issue the prefetch 200 instructions before the &lt;abbr title=&quot;A true load that appears in the source code or assembly, as opposed to loads initiated by software or hardware prefetch.&quot;&gt;demand load&lt;/abbr&gt; instruction, you have essentially broadened the &lt;abbr title=&quot;Re-order buffer: n ordered buffer which stores in-progress instructions on an out-of-order processor.&quot;&gt;ROB&lt;/abbr&gt; by 200 instructions as it applies to that load.&lt;/p&gt;

&lt;h3 id=&quot;load-buffer&quot;&gt;Load Buffer&lt;/h3&gt;

&lt;p&gt;Every load operation, needs a load buffer entry. This means the total &lt;abbr title=&quot;Out-of-order execution allows CPUs to execute instructions out of order with respect to the source.&quot;&gt;OoO&lt;/abbr&gt; window is limited by the number loads appearing in the window. Typical load buffer sizes (72 on &lt;abbr title=&quot;Intel's Skylake (client) architecture, aka 6th Generation Intel Core i3,i5,i7&quot;&gt;SKL&lt;/abbr&gt;) seem to be about one third of the &lt;abbr title=&quot;Re-order buffer: n ordered buffer which stores in-progress instructions on an out-of-order processor.&quot;&gt;ROB&lt;/abbr&gt; size, so if more than about one out of three operations is a load, you are more likely to be limited by the load buffer than the &lt;abbr title=&quot;Re-order buffer: n ordered buffer which stores in-progress instructions on an out-of-order processor.&quot;&gt;ROB&lt;/abbr&gt;.&lt;/p&gt;

&lt;p&gt;Gathers need as many entries as there are loaded elements to load in the gather. Sometimes loads are hidden - remember that things like &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pop&lt;/code&gt; involve a load: in general anything that executes an op on &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;p2&lt;/code&gt; or &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;p3&lt;/code&gt; which is not a store (i.e., does not execute anything on &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;p4&lt;/code&gt;) needs an entry in the load buffer.&lt;/p&gt;

&lt;h4 id=&quot;remedies-8&quot;&gt;Remedies&lt;/h4&gt;

&lt;p&gt;First, you should evaluate whether getting under this limit will be helpful: it may be that you will almost immediately hit another &lt;abbr title=&quot;Out-of-order execution allows CPUs to execute instructions out of order with respect to the source.&quot;&gt;OoO&lt;/abbr&gt; limit, and it also may be that increasing the &lt;abbr title=&quot;Out-of-order execution allows CPUs to execute instructions out of order with respect to the source.&quot;&gt;OoO&lt;/abbr&gt; window isn’t that useful if the extra included instructions can’t execute or aren’t a bottleneck.&lt;/p&gt;

&lt;p&gt;In any case, the remedy is to use fewer loads, or in some cases to reorganize loads relative to other instructions so that the window implied by the full load buffer contains the most useful instructions (in particular, contains long latency instructions like load misses). You can try to combine narrower loads into wider ones. You can ensure you keep values in registers as much as possible, and inline functions that would otherwise pass arguments through memory (e.g., certain structures) to avoid pointless loads. If you need to spill some registers, consider spilling registers to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;xmm&lt;/code&gt; or &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ymm&lt;/code&gt; vector registers rather than the stack.&lt;/p&gt;

&lt;h3 id=&quot;store-buffer&quot;&gt;Store Buffer&lt;/h3&gt;

&lt;p&gt;Similarly to the load buffer, the store buffer is required for every operation that involves a store. In fact, filling up the store buffer is pretty much the only way stores can bottleneck performance. Unlike loads, nobody is waiting for a store to complete, except in the case of store-to-load forwarding - but there, by definition, the value is sitting inside the store queue ready to use, so there is no equivalent of the long load miss which blocks dependent operations. You can have long store misses, but they happen after the store has already retired and is sitting in the store buffer (or write-combining buffer). So stores primarily cause a problem if there are enough of them such that the store buffer fill up.&lt;/p&gt;

&lt;p&gt;Store buffers are usually smaller than load buffers, about two thirds the size, typically. This reflects the fact that most programs have more loads than stores.&lt;/p&gt;

&lt;h4 id=&quot;remedies-9&quot;&gt;Remedies&lt;/h4&gt;

&lt;p&gt;Similar to the load buffer, you want less stores. Ensure you aren’t doing unnecessary spilling to the stack, that you merge stores where possible, that you aren’t doing dead stores (e.g., zeroing a structure before immediately overwriting it anyways) and so on. On some platform giving the compiler more information about array of structure alignment helps it merge stores.&lt;/p&gt;

&lt;p&gt;Vectorization of loops with consecutive stores helps a lot since it can turn (for example) 8 32-bit stores into a single 256-bit store, which only takes one entry in the store buffer.&lt;/p&gt;

&lt;p&gt;Scatter operations available in AVX-512 don’t really help: they take one store buffer entry per element stored.&lt;/p&gt;

&lt;h3 id=&quot;scheduler&quot;&gt;Scheduler&lt;/h3&gt;

&lt;p&gt;After an op is issued, it sits in the reservation station (scheduler) until it is able to execute. This structure is generally much smaller than the &lt;abbr title=&quot;Re-order buffer: n ordered buffer which stores in-progress instructions on an out-of-order processor.&quot;&gt;ROB&lt;/abbr&gt;, about 40-90 entries on modern chips. If this structure fills up, no more operations can issue, even if the other structures have plenty of room. This will occur if there are too many instructions dependent on earlier instructions which haven’t completed yet. A typical example is a load which misses in the cache, followed by many instructions which depend on that load. Those instructions won’t leave the scheduler until the load completes, and if they are enough to fill the structure no further instructions will be evaluated.&lt;/p&gt;

&lt;h4 id=&quot;remedies-10&quot;&gt;Remedies&lt;/h4&gt;

&lt;p&gt;Organize your code so that there are some independent instructions to execute following long latency operations, which don’t depend on the result of those operations.&lt;/p&gt;

&lt;p&gt;Consider replacing data dependencies (e.g., conditional moves or other arithmetic) with control dependencies, since the latter are predicted and don’t cause a dependency. This also has the advantage of executing many more instructions in parallel, but may lead to branch mispredictions.&lt;/p&gt;

&lt;h3 id=&quot;register-file-size-limit&quot;&gt;Register File Size Limit&lt;/h3&gt;

&lt;p&gt;Every instruction with a destination register requires a renamed physical register, which is only reclaimed when the instruction is retired. These registers come from the &lt;em&gt;physical regsiter file&lt;/em&gt; (&lt;abbr title=&quot;Physical register file: The hardware registers used for renaming architectural (source visible) registers, usually much larger in number than the architectural register count.&quot;&gt;PRF&lt;/abbr&gt;). So to fill the entire &lt;abbr title=&quot;Re-order buffer: n ordered buffer which stores in-progress instructions on an out-of-order processor.&quot;&gt;ROB&lt;/abbr&gt; with operations that require a destination register, you’ll need a &lt;abbr title=&quot;Physical register file: The hardware registers used for renaming architectural (source visible) registers, usually much larger in number than the architectural register count.&quot;&gt;PRF&lt;/abbr&gt; as large as the &lt;abbr title=&quot;Re-order buffer: n ordered buffer which stores in-progress instructions on an out-of-order processor.&quot;&gt;ROB&lt;/abbr&gt;. In practice, there are two separate register files on Intel and AMD chips: the integer registers file used for scalar registers such as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;rax&lt;/code&gt; and the vector register file used for &lt;abbr title=&quot;Single Instruction Multiple Data: an ISA type or ISA extension like Intel's AVX or ARM's NEON that can perform multiple identical operations on elements packed into a SIMD register.&quot;&gt;SIMD&lt;/abbr&gt; registers such as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;xmm0&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ymm0&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;zmm0&lt;/code&gt;, and the sizes of these register files as shown above are somewhat smaller than the &lt;abbr title=&quot;Re-order buffer: n ordered buffer which stores in-progress instructions on an out-of-order processor.&quot;&gt;ROB&lt;/abbr&gt; size.&lt;/p&gt;

&lt;p&gt;Not all of the registers are actually available for renaming: some are used to store the non-speculative values of the architectural registers, or for other purposes, so the available number of register is about 16 to 32 less than the values shown above. Henry Wong has a great description of observed available registers on the &lt;a href=&quot;http://blog.stuffedcow.net/2013/05/measuring-rob-capacity/&quot;&gt;article&lt;/a&gt; I linked earlier, including some non-ideal behaviors that I’ve glossed over here. You can calculate the number of available registers on new architectures using the &lt;a href=&quot;https://github.com/travisdowns/robsize&quot;&gt;robsize tool&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The upshot is that for given &lt;abbr title=&quot;Re-order buffer: n ordered buffer which stores in-progress instructions on an out-of-order processor.&quot;&gt;ROB&lt;/abbr&gt; sizes, there are only enough registers available in each file for about 75% of the entries.&lt;/p&gt;

&lt;p&gt;In practice, some instructions such as branches, zeroing idioms&lt;sup id=&quot;fnref:rmwnote&quot;&gt;&lt;a href=&quot;#fn:rmwnote&quot; class=&quot;footnote&quot;&gt;24&lt;/a&gt;&lt;/sup&gt; don’t consume &lt;abbr title=&quot;Physical register file: The hardware registers used for renaming architectural (source visible) registers, usually much larger in number than the architectural register count.&quot;&gt;PRF&lt;/abbr&gt; entries, which limit you hit depends on that ratio. Since integer and FP PRFs are distinct on recent Intel, you can consume from each &lt;abbr title=&quot;Physical register file: The hardware registers used for renaming architectural (source visible) registers, usually much larger in number than the architectural register count.&quot;&gt;PRF&lt;/abbr&gt; independently: meaning that vectorized code mixed with at least some &lt;abbr title=&quot;General purpose: as opposed to SIMD or FP. On x86 often refers to instructions such as integer addition, or registers such as eax.&quot;&gt;GP&lt;/abbr&gt; code is unlikely to hit the &lt;abbr title=&quot;Physical register file: The hardware registers used for renaming architectural (source visible) registers, usually much larger in number than the architectural register count.&quot;&gt;PRF&lt;/abbr&gt; limit before it hits the &lt;abbr title=&quot;Re-order buffer: n ordered buffer which stores in-progress instructions on an out-of-order processor.&quot;&gt;ROB&lt;/abbr&gt; limit.&lt;/p&gt;

&lt;p&gt;The effect of hitting the &lt;abbr title=&quot;Physical register file: The hardware registers used for renaming architectural (source visible) registers, usually much larger in number than the architectural register count.&quot;&gt;PRF&lt;/abbr&gt; limit is the same as the &lt;abbr title=&quot;Re-order buffer: n ordered buffer which stores in-progress instructions on an out-of-order processor.&quot;&gt;ROB&lt;/abbr&gt; size limit.&lt;/p&gt;

&lt;h4 id=&quot;remedies-11&quot;&gt;Remedies&lt;/h4&gt;

&lt;p&gt;There’s not all much you can do for this one beyond the stuff discussed in the &lt;abbr title=&quot;Re-order buffer: n ordered buffer which stores in-progress instructions on an out-of-order processor.&quot;&gt;ROB&lt;/abbr&gt; limit entry section. Maybe try to mix integer and vector code so you consume from each register file. Make sure you are using zeroing idioms like &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;xor eax,eax&lt;/code&gt; rather than &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mov eax, 0&lt;/code&gt; but you should already be doing that.&lt;/p&gt;

&lt;h3 id=&quot;branches-in-flight&quot;&gt;Branches in Flight&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Intel: Maximum of 48 branches in flight&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Modern Intel chips seem to have a limit of branches &lt;em&gt;in flight&lt;/em&gt;, where &lt;em&gt;in flight&lt;/em&gt; refers to branches that have not yet retired, usually because some older operation hasn’t yet completed. I first saw this limit described and measured &lt;a href=&quot;http://blog.stuffedcow.net/2018/04/ras-microbenchmarks/#inflight&quot;&gt;here&lt;/a&gt;, although it seems like &lt;a href=&quot;https://www.realworldtech.com/haswell-cpu/3/&quot;&gt;David Kanter had the scoop&lt;/a&gt; way back in 2012:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;The branch order buffer, which is used to rollback to known good architectural state in the case of a misprediction is still 48 entries, as with Sandy Bridge.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The effects of exceeding the branch order buffer limit are the same as for the &lt;abbr title=&quot;Re-order buffer: n ordered buffer which stores in-progress instructions on an out-of-order processor.&quot;&gt;ROB&lt;/abbr&gt; limit.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Branches&lt;/em&gt; here refers to both conditional jumps (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;jcc&lt;/code&gt; where &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;cc&lt;/code&gt; is some conditional code) and indirect jumps (things like &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;jmp [rax]&lt;/code&gt;).&lt;/p&gt;

&lt;h4 id=&quot;remedies-12&quot;&gt;Remedies&lt;/h4&gt;

&lt;p&gt;Although you will rarely hit this limit, the solution is fewer branches. Try to move unnecessary checks out of the hot path, or combine several checks into one. Try to organize multi-predicate conditions such that you can short-circuit the evaluation after the first check (so the subsequent checks don’t appear in the dynamic instruction stream). Consider replacing N 2-way (true/false) conditional jumps with one indirect jump with N^2 targets as this counts as only “one” instead of N against the branch limit. Consider conditional moves or other branch-free techniques.&lt;/p&gt;

&lt;p&gt;Ensure that branches can retire as soon as possible, although in practice there often isn’t much opportunity to do this when dealing with already well-compiled code.&lt;/p&gt;

&lt;p&gt;Note that many of these are the same things you might consider to reduce branch mispredictions, although they apply here even if there are no mispredictions.&lt;/p&gt;

&lt;h3 id=&quot;calls-in-flight&quot;&gt;Calls in Flight&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Intel: 14-15&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Only 14-15 calls can be in-flight at once, exactly analogous to the limitation on in-flight branches described above, except it applies to the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;call&lt;/code&gt; instruction rather than branches. As with the branches in-flight restriction, this comes from &lt;a href=&quot;http://blog.stuffedcow.net/2018/04/ras-microbenchmarks/#inflight&quot;&gt;testing&lt;/a&gt; by Henry Wong, and in this case I am not aware of an earlier source.&lt;/p&gt;

&lt;h4 id=&quot;remedies-13&quot;&gt;Remedies&lt;/h4&gt;

&lt;p&gt;Reduce the number of call instructions you make. Consider ensuring the calls can be inlined, or partial inlining (a fast path that can be inlined combined with a slow path that isn’t). In extreme cases you might want to replace &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;call&lt;/code&gt; + &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ret&lt;/code&gt; pairs with unconditional &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;jmp&lt;/code&gt;, saving the return address in a register, plus indirect branch to return to the saved address. I.e. replace the following:&lt;/p&gt;

&lt;div class=&quot;language-nasm highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;callee&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;; function code goes here&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;ret&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;; caller code&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;call&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;callee&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;With the following (which is essentially emulating the &lt;a href=&quot;https://en.wikibooks.org/wiki/MIPS_Assembly/Control_Flow_Instructions#Jump_and_Link&quot;&gt;JAL instruction&lt;/a&gt;:&lt;/p&gt;

&lt;div class=&quot;language-nasm highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;callee&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;c&quot;&gt;; function code goes here&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;jmp&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[r15]&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;; return to address stashed in r15&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;; caller code&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;movabs&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;r15&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;next&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;jmp&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;callee&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;next&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This pattern is hard to achieve in practice in a high level language, although you might have luck emulating it with gcc’s &lt;a href=&quot;https://gcc.gnu.org/onlinedocs/gcc/Labels-as-Values.html&quot;&gt;labels as values&lt;/a&gt; functionality.&lt;/p&gt;

&lt;h2 id=&quot;thank-you&quot;&gt;Thank You&lt;/h2&gt;

&lt;p&gt;That’s it for now, if you made it this far I hope you found it useful.&lt;/p&gt;

&lt;p&gt;Thanks to Paul A. Clayton, Adrian, Peter E. Fry, anon, nkurz, maztheman, hyperpape, Arseny Kapoulkine, Thomas Applencourt, haberman, caf, Nick Craver, pczarn, Bruce Dawson, Fabian Giesen, glaebhoerl and Matthew Fernandez for pointing out errors and other feedback.&lt;/p&gt;

&lt;p&gt;Thanks to Daniel Lemire for providing access to hardware on which I was able to test and verify some of these limits.&lt;/p&gt;

&lt;h2 id=&quot;comments&quot;&gt;Comments&lt;/h2&gt;

&lt;p&gt;I don’t have a comments system&lt;sup id=&quot;fnref:comments&quot;&gt;&lt;a href=&quot;#fn:comments&quot; class=&quot;footnote&quot;&gt;25&lt;/a&gt;&lt;/sup&gt; yet, so I’m basically just outsourcing discussion to HackerNews right now: &lt;a href=&quot;https://news.ycombinator.com/item?id=20157196&quot;&gt;here is the thread&lt;/a&gt; for this post.&lt;/p&gt;

&lt;p class=&quot;info&quot;&gt;If you liked this post, check out the &lt;a href=&quot;/&quot;&gt;homepage&lt;/a&gt; for others you might enjoy&lt;sup id=&quot;fnref:noenjoy&quot;&gt;&lt;a href=&quot;#fn:noenjoy&quot; class=&quot;footnote&quot;&gt;26&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;hr /&gt;
&lt;hr /&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;div class=&quot;footnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:speedlemire&quot;&gt;
      &lt;p&gt;I think this speed limit term came from &lt;a href=&quot;https://lemire.me&quot;&gt;Daniel Lemire&lt;/a&gt;. I guess I liked it because I have used it a lot since then. &lt;a href=&quot;#fnref:speedlemire&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:thatsaid&quot;&gt;
      &lt;p&gt;That said, I am quite sure you can reach or at least approach closely these limits as I’ve tested most of them myself. Sure, a lot of these are micro-benchmarks, but you can get there in real code too. If you find some code that you think should reach a limit, but can’t - I’m interested to hear about it. &lt;a href=&quot;#fnref:thatsaid&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:ICL&quot;&gt;
      &lt;p&gt;Reported to be increased to 5 on just-released-but-not-yet-tested Sunny Cove chips. &lt;a href=&quot;#fnref:ICL&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:fused-domain&quot;&gt;
      &lt;p&gt;The distinction between fused domain and unfused domain uops applies to instructions with a memory source or destination. For example, an instruction like &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;add eax, [rsp]&lt;/code&gt; means “add the value pointed to by &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;rsp&lt;/code&gt; to register &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;eax&lt;/code&gt;. During execution, two separate micro-operations are created: one for the load and for the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;add&lt;/code&gt; instruction, this is the so-called &lt;em&gt;unfused domain&lt;/em&gt;. However, prior to execution, the uops are kept together and only count as one against the pipeline width limit, this is the so called &lt;em&gt;fused domain&lt;/em&gt;. Good list of instruction characteristics like &lt;a href=&quot;https://www.agner.org/optimize/#manual_instr_tab&quot;&gt;Agner Fog’s instruction tables&lt;/a&gt; list both values. AMD macro-operations are largely similar to Intel fused-domain ops. &lt;a href=&quot;#fnref:fused-domain&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:sum-halves&quot;&gt;
      &lt;p&gt;You can this benchmark from &lt;a href=&quot;https://github.com/travisdowns/uarch-bench&quot;&gt;&lt;abbr title=&quot;Microarchitecture: a specific implementation of an ISA, e.g., &amp;quot;Haswell microarchitecture&amp;quot;.&quot;&gt;uarch&lt;/abbr&gt;-bench&lt;/a&gt;:  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;./uarch-bench.sh --test-name=cpp/sum-halves&lt;/code&gt; &lt;a href=&quot;#fnref:sum-halves&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:extra-3&quot;&gt;
      &lt;p&gt;Well it’s not exactly 14, it’s 14.03 - the extra 0.03 mostly comes from the fact that we call the benchmark repeatedly using an outer loop. Every time the sum loop terminates (it iterates over an array of 1024 elements), we suffer a branch misprediction and the CPU has gone ahead and executed an extra (falsely speculated) iteration or so of the loop, which is wasted work. You can see this by comparing with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;uops_retired.retire_slots&lt;/code&gt; which is also issued uops, but only counting ones which actually retired and not those which were on wrongly speculated path: this reads 14.01, so 2 out of 3 extra uops came from the misprediction. The other &lt;abbr title=&quot;Micro-operation: instructions are translated into one or more uops, which are simple operations executed by the CPU's execution units.&quot;&gt;uop&lt;/abbr&gt; is just the outer loop overhead. &lt;a href=&quot;#fnref:extra-3&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:gcc-notgreat&quot;&gt;
      &lt;p&gt;It puts in a ton of &lt;a href=&quot;https://gist.github.com/travisdowns/9216bffba33876ee578aa0bb74b3c8f2&quot;&gt;unnecessary shuffles&lt;/a&gt; probably to try reproduce the exact structure of the unrolled-by-two loop (removing the unroll is likely to help). In gcc’s defense, clang 5 does even worse, running almost twice as slow as gcc. &lt;a href=&quot;#fnref:gcc-notgreat&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:delamopt&quot;&gt;
      &lt;p&gt;Not &lt;em&gt;fully described&lt;/em&gt; is basically a ephemism for (partly) &lt;em&gt;wrong&lt;/em&gt;. The manual describes a test you can use to determine if &lt;abbr title=&quot;An situation where an instruction using an indexed addressing mode, that is otherwise eligible for micro-fusion, stays fused in the uop-cache, but then delaminates into two separate uops prior to issue, and so counts as two against the pipeline (rename) limit of four uops.&quot;&gt;delamination&lt;/abbr&gt; will occur, but it gives the wrong result for many instructions. &lt;a href=&quot;#fnref:delamopt&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:issued&quot;&gt;
      &lt;p&gt;Here I’m using &lt;em&gt;issued&lt;/em&gt; as Intel uses it, indicating the moment an operation is renamed and send to the reservation station (RS) awaiting execution (which can happen after all its operands are ready). At the moment the operation leaves the reservation station to execute it is said to be dispatched. This terminology is exactly reversed from that used by some other CPU documentation and most academic literature: the terms &lt;em&gt;issue&lt;/em&gt; and &lt;em&gt;dispatch&lt;/em&gt; are also used but with meanings flipped. &lt;a href=&quot;#fnref:issued&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:ports&quot;&gt;
      &lt;p&gt;We are going to mostly gloss over the difference between ports and execution units here. In practice, operations are not dispatched directly to an execution unit, but rather pass though a numbered port, and we treat the port as the constrained resource. The relationship between ports and execution units is normally 1:N (i.e., each port gates access a private group of EUs), but other arrangements are possible. &lt;a href=&quot;#fnref:ports&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:bankconf&quot;&gt;
      &lt;p&gt;Bank conflicts occur in a banked cache design when two loads try to access the same bank. &lt;a href=&quot;https://twitter.com/rygorous/status/1138934828198326272&quot;&gt;Per Fabian&lt;/a&gt; Ivy Bridge and earlier had banked cache designs, as does Zen1. The Intel chips have 16 banks per line (bank selected by bits &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;[5:2]&lt;/code&gt; of the address), while Zen1 has 8 banks per line (bits &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;[5:3]&lt;/code&gt; used). A load uses any bank it overlaps. &lt;a href=&quot;#fnref:bankconf&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:written-weirdly&quot;&gt;
      &lt;p&gt;Yes, it’s written kind of weirdly because this generates better assembly than say a forwards for loop. In particular, we want the final check to be for &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;i != 0&lt;/code&gt;, since that comes for free on x86 (the result of the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;i -= 2&lt;/code&gt; will set the zero flag), which ends up dictating the rest of the loop. Another possibility is to adjust the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;data&lt;/code&gt; pointer which lets you use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;i&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;i + 1&lt;/code&gt; as the indices into the array. &lt;a href=&quot;#fnref:written-weirdly&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:delam2&quot;&gt;
      &lt;p&gt;Well, on Haswell and later there are 5 fused-domain uops, but on earlier architectures there are 7, because &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ecx, DWORD PTR [rdi+r8*4]&lt;/code&gt; cannot fully fuse due to its use of an indexed addressing mode (i.e., so-called &lt;abbr title=&quot;An situation where an instruction using an indexed addressing mode, that is otherwise eligible for micro-fusion, stays fused in the uop-cache, but then delaminates into two separate uops prior to issue, and so counts as two against the pipeline (rename) limit of four uops.&quot;&gt;delamination&lt;/abbr&gt; occurs). &lt;a href=&quot;#fnref:delam2&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:add-indirect&quot;&gt;
      &lt;p&gt;You can run the tests for this item yourself in &lt;abbr title=&quot;Microarchitecture: a specific implementation of an ISA, e.g., &amp;quot;Haswell microarchitecture&amp;quot;.&quot;&gt;uarch&lt;/abbr&gt;-bench with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;./uarch-bench.sh --timer=perf --test-name=cpp/add-indirect*&lt;/code&gt;. &lt;a href=&quot;#fnref:add-indirect&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:vector-scalar&quot;&gt;
      &lt;p&gt;In fact, in a twist of irony, the most efficient way to get stuff from vector registers into scalar registers, in a throughput sense, is to store it to memory and then reload each item one-by-one into scalar registers. So vector loads cannot help you break the load speed limit if you need all loaded values in scalar registers. &lt;a href=&quot;#fnref:vector-scalar&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:portable&quot;&gt;
      &lt;p&gt;The dance with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;std::memcpy&lt;/code&gt; makes this &lt;em&gt;legal&lt;/em&gt; C++, but it’s still not portable in principle: it could produce different results on machines with really weird endianess (neither little nor big endian). The values of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sum1&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sum2&lt;/code&gt; will be reversed on little vs big-endian machines, although the final result &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sum1 + sum2&lt;/code&gt; will be the same. &lt;a href=&quot;#fnref:portable&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:srcdest&quot;&gt;
      &lt;p&gt;Note that in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;imul eax,DWORD PTR [rdi]&lt;/code&gt; register &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;eax&lt;/code&gt; is use both as one of the arguments for the multiply, and as the result register, in the same way as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;y *= x&lt;/code&gt; means &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;y = y * x&lt;/code&gt;. Such “two operand” instructions where the destination is the same as one of the operands forms are common in x86, especially in scalar code where they are usually the only option - although new &lt;abbr title=&quot;Single Instruction Multiple Data: an ISA type or ISA extension like Intel's AVX or ARM's NEON that can perform multiple identical operations on elements packed into a SIMD register.&quot;&gt;SIMD&lt;/abbr&gt; instructions using the AVX and subsequent instruction sets use three argument forms, like &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;vpaddb xmm0, xmm1, xmm2&lt;/code&gt; where the destination is distinct from the sources. Most other extant ISAs, usually RISC or RISC-influenced, have always used the three operand form. &lt;a href=&quot;#fnref:srcdest&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:assoc&quot;&gt;
      &lt;p&gt;I mention &lt;em&gt;integer&lt;/em&gt; multiplication for a reason: this property does not apply to floating point multiplication as performed by CPUs, and the same is true for most of the usual mathematic properties of operators when applied to floating point. For this reason, compilers often cannot perform transformations that they could for integer math, because it might change the result, even if only slightly. The result won’t necessarily be &lt;em&gt;worse&lt;/em&gt; - just different than if the operations had occurred in source order. You can loosen these chains that hold the compiler back with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-ffast-math&lt;/code&gt;. &lt;a href=&quot;#fnref:assoc&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:sklfe&quot;&gt;
      &lt;p&gt;Complete up until Broadwell more or less. The guide does not reflect some newer changes such as the &lt;abbr title=&quot;Micro-operation: instructions are translated into one or more uops, which are simple operations executed by the CPU's execution units.&quot;&gt;uop&lt;/abbr&gt; cache granularity being 64 bytes rather than 32 bytes on Skylake. &lt;a href=&quot;#fnref:sklfe&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:lsdno&quot;&gt;
      &lt;p&gt;Note that &lt;abbr title=&quot;Lysergic acid diethylamide or Loop stream detector, but in the context of this blog probably the latter: The so-called loop buffer that can cache small loops of up to ~64 uops on recent Intel architectures. Not actually a separate structure: the hardware justs locks the loop down in the IDQ.&quot;&gt;LSD&lt;/abbr&gt; doesn’t appear here, because there the measurement doesn’t make sense - these numbers represent the rate at which each decoding-type component can deliver uops to the &lt;abbr title=&quot;Queue that collects incoming instructions from the decoder, uop cache or microcode engine and delivers them to the renamer (RAT).&quot;&gt;IDQ&lt;/abbr&gt;, but the &lt;abbr title=&quot;Lysergic acid diethylamide or Loop stream detector, but in the context of this blog probably the latter: The so-called loop buffer that can cache small loops of up to ~64 uops on recent Intel architectures. Not actually a separate structure: the hardware justs locks the loop down in the IDQ.&quot;&gt;LSD&lt;/abbr&gt; is &lt;em&gt;inside&lt;/em&gt; the &lt;abbr title=&quot;Queue that collects incoming instructions from the decoder, uop cache or microcode engine and delivers them to the renamer (RAT).&quot;&gt;IDQ&lt;/abbr&gt;: when active the renamer repeatedly accesses the uops in the &lt;abbr title=&quot;Queue that collects incoming instructions from the decoder, uop cache or microcode engine and delivers them to the renamer (RAT).&quot;&gt;IDQ&lt;/abbr&gt; without consuming them. So there is no delivery rate to the &lt;abbr title=&quot;Queue that collects incoming instructions from the decoder, uop cache or microcode engine and delivers them to the renamer (RAT).&quot;&gt;IDQ&lt;/abbr&gt; because no uops are delivered. &lt;a href=&quot;#fnref:lsdno&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:snote&quot;&gt;
      &lt;p&gt;In this table, note that the Intel and Zen scheduler (RS) sizes are not directly comparable, since Intel uses a unified scheduler for &lt;abbr title=&quot;General purpose: as opposed to SIMD or FP. On x86 often refers to instructions such as integer addition, or registers such as eax.&quot;&gt;GP&lt;/abbr&gt; and &lt;abbr title=&quot;Single Instruction Multiple Data: an ISA type or ISA extension like Intel's AVX or ARM's NEON that can perform multiple identical operations on elements packed into a SIMD register.&quot;&gt;SIMD&lt;/abbr&gt; ops, with “all” (actually most) scheduler entries available to most uops, while AMD has one scheduler (RS) per port. The numbers shown for AMD are the sum of all the ALU and &lt;abbr title=&quot;Address Generation Unit&quot;&gt;AGU&lt;/abbr&gt; scheduler sizes. &lt;a href=&quot;#fnref:snote&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:zensched&quot;&gt;
      &lt;p&gt;The Zen scheduler has 4x14 &lt;abbr title=&quot;General purpose: as opposed to SIMD or FP. On x86 often refers to instructions such as integer addition, or registers such as eax.&quot;&gt;GP&lt;/abbr&gt; ALU schedulers, a 96 entry FP scheduler and 2x14 &lt;abbr title=&quot;Address Generation Unit&quot;&gt;AGU&lt;/abbr&gt;/mem schedulers. &lt;a href=&quot;#fnref:zensched&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:zen2sched&quot;&gt;
      &lt;p&gt;The Zen2 scheduler has 4x16 &lt;abbr title=&quot;General purpose: as opposed to SIMD or FP. On x86 often refers to instructions such as integer addition, or registers such as eax.&quot;&gt;GP&lt;/abbr&gt; ALU schedulers, a 96 entry (?) FP scheduler and a 1x28 &lt;abbr title=&quot;Address Generation Unit&quot;&gt;AGU&lt;/abbr&gt;/mem scheduler. &lt;a href=&quot;#fnref:zen2sched&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:rmwnote&quot;&gt;
      &lt;p&gt;Possibly also including RMW and compare-with-memory instructions, but it depends on the flags implementation. Current Intel chips seem to include flag register bits attached to each integer &lt;abbr title=&quot;Physical register file: The hardware registers used for renaming architectural (source visible) registers, usually much larger in number than the architectural register count.&quot;&gt;PRF&lt;/abbr&gt; entry, so an instruction that produces flags consumes a &lt;abbr title=&quot;Physical register file: The hardware registers used for renaming architectural (source visible) registers, usually much larger in number than the architectural register count.&quot;&gt;PRF&lt;/abbr&gt; entry even if it does not also produce an integer result. &lt;a href=&quot;#fnref:rmwnote&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:comments&quot;&gt;
      &lt;p&gt;If anyone has a recommendation or a if anyone knows of a comments system that works with static sites, and which is not Disqus, has no ads, is free and fast, and lets me own the comment data (or at least export it in a reasonable format), I am all ears. &lt;a href=&quot;#fnref:comments&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:noenjoy&quot;&gt;
      &lt;p&gt;On the other hand, if you didn’t enjoy it, I have to wonder how you made it this far! Maybe you started off liking it, then felt invested and compelled to finish it? Beware the sunk cost fallacy! &lt;a href=&quot;#fnref:noenjoy&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><author><name>Travis Downs</name><email>travis.downs@gmail.com</email></author><category term="blog" /><category term="performance" /><category term="benchmarking" /><summary type="html">How fast can it go?</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/assets/speed-limits/speed-limit-50-ns.png" /><media:content medium="image" url="http://localhost:4000/assets/speed-limits/speed-limit-50-ns.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Beating up on qsort</title><link href="http://localhost:4000/blog/2019/05/22/sorting.html" rel="alternate" type="text/html" title="Beating up on qsort" /><published>2019-05-22T19:00:00+02:00</published><updated>2019-05-22T19:00:00+02:00</updated><id>http://localhost:4000/blog/2019/05/22/sorting</id><content type="html" xml:base="http://localhost:4000/blog/2019/05/22/sorting.html">&lt;p&gt;Recently, Daniel Lemire &lt;a href=&quot;https://lemire.me/blog/2019/05/07/almost-picking-n-distinct-numbers-at-random/&quot;&gt;tackled the topic&lt;/a&gt; of selecting N &lt;em&gt;distinct&lt;/em&gt; numbers at random. In the case we want sorted output, an obvious solution presents itself: sorting randomly chosen values and de-duplicating the list, which is easy since identical values are now adjacent.&lt;sup id=&quot;fnref:distinct&quot;&gt;&lt;a href=&quot;#fn:distinct&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;p&gt;While Daniel suggests a clever method of avoiding a sort entirely&lt;sup id=&quot;fnref:danmethod&quot;&gt;&lt;a href=&quot;#fn:danmethod&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;, I’m also interested in they &lt;em&gt;why&lt;/em&gt; for the underlying performace of the sort method: it takes more than 100 ns per element, which means 100s of CPU clock cycles and usually even more instructions than that (on a superscalar processor)! As a sanity check, a quick benchmark (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;perf record ./bench &amp;amp;&amp;amp; perf report&lt;/code&gt;) shows that more than 90% of the time spent in this approach is in the sorting routine, &lt;a href=&quot;https://devdocs.io/c/algorithm/qsort&quot;&gt;qsort&lt;/a&gt; - so we are right to focus on this step, rather than say the de-duplication step or the initial random number generation. This naturally, this raises the question: how fast is qsort when it comes to sorting integers and can we do better?&lt;/p&gt;

&lt;p&gt;All of the code for this post &lt;a href=&quot;https://github.com/travisdowns/sort-bench&quot;&gt;is available on GitHub&lt;/a&gt;, so if you’d like to follow along with the code open in an editor, go right ahead (warning: there are obviously some spoilers if you dig through the code first).&lt;/p&gt;

&lt;h2 id=&quot;benchmarking-qsort&quot;&gt;Benchmarking Qsort&lt;/h2&gt;

&lt;p&gt;First, let’s take a look at what &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;qsort&lt;/code&gt; is doing, to see if there is any delicous low-hanging performance fruit. We use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;perf record ./bench qsort&lt;/code&gt; to capture profiling data, and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;perf report --stdio&lt;/code&gt; to print a summary&lt;sup id=&quot;fnref:long-tail&quot;&gt;&lt;a href=&quot;#fn:long-tail&quot; class=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt;:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# Samples: 101K of event 'cycles:ppp'
# Event count (approx.): 65312285835
#
# Overhead  Command  Shared Object      Symbol                                        
# ........  .......  .................  ..............................................
#
    64.90%  bench    libc-2.23.so       [.] msort_with_tmp.part.0
    21.45%  bench    bench              [.] compare_uint64_t
     8.65%  bench    libc-2.23.so       [.] __memcpy_sse2
     0.87%  bench    libc-2.23.so       [.] __memcpy_avx_unaligned
     0.83%  bench    bench              [.] main
     0.41%  bench    [kernel.kallsyms]  [k] clear_page_erms
     0.34%  bench    [kernel.kallsyms]  [k] native_irq_return_iret
     0.31%  bench    bench              [.] bench_one
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The assembly for the biggest offender, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;msort_with_tmp&lt;/code&gt; looks like this&lt;sup id=&quot;fnref:annotate-command&quot;&gt;&lt;a href=&quot;#fn:annotate-command&quot; class=&quot;footnote&quot;&gt;4&lt;/a&gt;&lt;/sup&gt; :&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt; Percent | Address      | Disassembly
--------------------------------------------------
   30.55 :   39200:       mov    rax,QWORD PTR [r15]
    0.61 :   39203:       sub    rbp,0x1
    0.52 :   39207:       add    r15,0x8
    7.30 :   3920b:       mov    QWORD PTR [rbx],rax
    0.39 :   3920e:       add    rbx,0x8
    0.07 :   39212:       test   r12,r12
    0.09 :   39215:       je     390e0   ; merge finished
    1.11 :   3921b:       test   rbp,rbp
    0.01 :   3921e:       je     390e0   ; merge finished
    5.24 :   39224:       mov    rdx,QWORD PTR [rsp+0x8]
    0.42 :   39229:       mov    rsi,r15
    0.19 :   3922c:       mov    rdi,r13
    6.08 :   3922f:       call   r14
    0.59 :   39232:       test   eax,eax
    3.52 :   39234:       jg     39200
   32.69 :   39236:       mov    rax,QWORD PTR [r13+0x0]
    1.31 :   3923a:       sub    r12,0x1
    1.01 :   3923e:       add    r13,0x8
    1.09 :   39242:       jmp    3920b &amp;lt;bsearch@@GLIBC_2.2.5+0x205b&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Depending on your level of assembly reading skill, it may not be obvious, but this is basically a classic merge routine: it is merging two lists by comparing the top elements of each list (pointed to by &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;r13&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;r15&lt;/code&gt;), and then storing the smaller element (the line &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;QWORD PTR [rbx],rax&lt;/code&gt;) and loading the next element from that list. There are also two checks for termination (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;test   r12,r12&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;test   rbp,rbp&lt;/code&gt;). This hot loop corresponds directly to this code from &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;glibc&lt;/code&gt; (from the file&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;msort.c&lt;/code&gt;&lt;sup id=&quot;fnref:msort-note&quot;&gt;&lt;a href=&quot;#fn:msort-note&quot; class=&quot;footnote&quot;&gt;5&lt;/a&gt;&lt;/sup&gt;) :&lt;/p&gt;

&lt;div class=&quot;language-c++ highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;while&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cmp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;arg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;uint64_t&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tmp&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;uint64_t&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;b1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;sizeof&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;uint64_t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
        &lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;uint64_t&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tmp&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;uint64_t&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;b2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;sizeof&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;uint64_t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
        &lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;tmp&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;sizeof&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;uint64_t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This loop suffers heavily from branch mispredictions, since the “which element is larger” branch is highly unpredictable (at least for random-looking input data). Indeed, we see roughly 128 million mispredicts while sorting ~11 million elements: close to 12 mispredicts per element.&lt;/p&gt;

&lt;p&gt;We also note the presence of the indirect call at the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;call r14&lt;/code&gt; line. This corresponds to the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;(*cmp) (b1, b2, arg)&lt;/code&gt; expression in the source: it is calling the user provided comparator function through a function pointer. Since the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;qsort()&lt;/code&gt; code is compiled ahead of time and is found inside the shared libc binary, there is no chance that the comparator, passed as a function pointer, can be inlined.&lt;/p&gt;

&lt;p&gt;The comparator function I provide looks like:&lt;/p&gt;

&lt;div class=&quot;language-c++ highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;compare_uint64_t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;r_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt; 
    &lt;span class=&quot;kt&quot;&gt;uint64_t&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;l&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;uint64_t&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;kt&quot;&gt;uint64_t&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;r&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;uint64_t&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;r_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;r&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;r&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt;  &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;which on gcc compiles to branch-free code:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;mov    rax,QWORD PTR [rsi]
mov    edx,0xffffffff
cmp    QWORD PTR [rdi],rax
seta   al
movzx  eax,al
cmovb  eax,edx
ret
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Note that the comparator has to redundantly load from memory the two locations to compare, something the merge loop already did (the merge loop reads them because it is responsible for moving the elements).&lt;/p&gt;

&lt;p&gt;How much better could things get if we inline the comparator into the merge loop? That’s what we do in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;qsort-inlined&lt;/code&gt;&lt;sup id=&quot;fnref:inline-hard&quot;&gt;&lt;a href=&quot;#fn:inline-hard&quot; class=&quot;footnote&quot;&gt;6&lt;/a&gt;&lt;/sup&gt;, and here’s the main loop which now includes the comparator function&lt;sup id=&quot;fnref:cmdline1&quot;&gt;&lt;a href=&quot;#fn:cmdline1&quot; class=&quot;footnote&quot;&gt;7&lt;/a&gt;&lt;/sup&gt; :&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-asm&quot;&gt; 0.07 :   401dc8:       test   rbp,rbp
 0.66 :   401dcb:       je     401e0c &amp;lt;void msort_with_tmp&amp;lt;CompareU64&amp;gt;(msort_param const*, void*, unsigned long, CompareU64)+0xbc&amp;gt;
 3.51 :   401dcd:       mov    rax,QWORD PTR [r9]
 5.00 :   401dd0:       lea    rdx,[rbx+0x8]
 1.62 :   401dd4:       mov    rcx,QWORD PTR [rbx]
 0.24 :   401dd7:       lea    r8,[r9+0x8]
 6.96 :   401ddb:       cmp    rax,rcx
20.83 :   401dde:       cmovbe r9,r8
 8.88 :   401de2:       cmova  rbx,rdx
 0.27 :   401de6:       cmp    rcx,rax
 6.23 :   401de9:       sbb    r8,r8
 0.74 :   401dec:       cmp    rcx,rax
 4.93 :   401def:       sbb    rdx,rdx
 0.24 :   401df2:       not    r8
 6.69 :   401df5:       add    rbp,rdx
 0.44 :   401df8:       cmp    rax,rcx
 5.34 :   401dfb:       cmova  rax,rcx
 5.96 :   401dff:       add    rdi,0x8
 7.48 :   401e03:       mov    QWORD PTR [rdi-0x8],rax
 0.00 :   401e07:       add    r15,r8
 0.71 :   401e0a:       jne    401dc8 &amp;lt;void msort_with_tmp&amp;lt;CompareU64&amp;gt;(msort_param const*, void*, unsigned long, CompareU64)+0x78&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;A key differences is that the loop is now basically branch free. There are still two conditional jumps, but they are both just checking for the termination condition (that one of the lists to merge is exhausted), so we expect this loop to be free of branch mispredictions other than the final iteration. Indeed, we measure with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;perf stat&lt;/code&gt; that the misprediction rate has dropped from to close to 12 mispredicts per element to around 0.75 per element. The loop has only two loads and one store, so the memory access redundancy between the merge code and the comparator has been eliminated&lt;sup id=&quot;fnref:load-redundancy&quot;&gt;&lt;a href=&quot;#fn:load-redundancy&quot; class=&quot;footnote&quot;&gt;8&lt;/a&gt;&lt;/sup&gt;. Finally, the comparator does a three-way compare (returning distrinct results for &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&amp;lt;&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&amp;gt;&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;==&lt;/code&gt;), but the merge code only needs a two-way compare (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&amp;lt;=&lt;/code&gt; or &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&amp;gt;&lt;/code&gt;) - inlining the comparator manages to remove extra code associated with distinguishing the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&amp;lt;&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;==&lt;/code&gt; cases.&lt;/p&gt;

&lt;p&gt;What’s the payoff? It’s pretty big:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/2019-05-22/fig2.svg&quot; alt=&quot;Effect of comparator inlining&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The speedup hovers right around 1.77x. Note that this is much larger than simply eliminating all the time spent in the separate comparator function in the original version (about 17% of the time implying a speedup of 1.2x if all the function time disapeared). This is a good example of how inlining isn’t just about removing function call overhead but enabling further &lt;em&gt;knock on&lt;/em&gt; optimizations which can have a much larger effect than just removing the overhead associated with function calls.&lt;/p&gt;

&lt;h2 id=&quot;what-about-c&quot;&gt;What about C++?&lt;/h2&gt;

&lt;p&gt;Short of copying the existing glibc (note: LGPL licenced) sorting code to allow inlining, what else can we do to speed things up? I’m writing in C++, so how about the C++ sort functions available in the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&amp;lt;algorithm&amp;gt;&lt;/code&gt; header? Unlike C’s &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;qsort&lt;/code&gt; which is generic by virtue of taking a function pointer and information about the object size, the C++ sort functions use templates to achieve genericity and so are implemented directly in header files. Since the sort code and the comparator are being compiler together, we expect the comparator to be easily inlined, and perhaps other optimizations may occur.&lt;/p&gt;

&lt;p&gt;Without further ado, let’s just throw &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;std::sort&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;std::stable_sort&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;std::partial_sort&lt;/code&gt; into the mix:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/2019-05-22/fig3.svg&quot; alt=&quot;C vs C++ sort functions&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The C++ sort functions, other than perhaps &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;std::partial_sort&lt;/code&gt;&lt;sup id=&quot;fnref:partial-sort&quot;&gt;&lt;a href=&quot;#fn:partial-sort&quot; class=&quot;footnote&quot;&gt;9&lt;/a&gt;&lt;/sup&gt;, put in a good showing. It is interesting that &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;std::stable_sort&lt;/code&gt; which has &lt;em&gt;stricly more requirements&lt;/em&gt; on its implementation than &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;std::sort&lt;/code&gt; (i.e., any stable sort is also suitable for &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;std::sort&lt;/code&gt;) ends up faster. I re-wrote this paragaph several times, since sometimes after a reboot &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;stable_sort&lt;/code&gt; was slower and sometimes it was faster (as shown above). When it was “fast” it had less than 2% branch mispredictions, and when it was slow it was at 15%. So perhaps there was some type of aliasing issue in the branch predictor which depends on the physical addresses assigned, which can vary from run to run, I’m not sure. See &lt;sup id=&quot;fnref:stablesort&quot;&gt;&lt;a href=&quot;#fn:stablesort&quot; class=&quot;footnote&quot;&gt;10&lt;/a&gt;&lt;/sup&gt; for an old note from when &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;std::stable_sort&lt;/code&gt; was slower.&lt;/p&gt;

&lt;h2 id=&quot;can-we-do-better&quot;&gt;Can we do better?&lt;/h2&gt;

&lt;p&gt;So that’s as fast as it gets, right? We aren’t going to beat &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;std::sort&lt;/code&gt; or &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;std::stable_sort&lt;/code&gt; without a huge amount of effort, I think? After all, these are presumably highly optimized sorting routines written by the standard library implementors. Sure, we might expect to be able to beat &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;qsort()&lt;/code&gt;, but that’s mostly because of built-in disadvantages that &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;qsort&lt;/code&gt; has, lacking the ability to inline the comparator, etc.&lt;/p&gt;

&lt;h3 id=&quot;radix-sort-attempt-1&quot;&gt;Radix Sort Attempt 1&lt;/h3&gt;

&lt;p&gt;Well, one thing we can try is a non-comparison sort. We know we have integer keys, so why stick to comparing numbers pairwise - maybe we can use something like &lt;a href=&quot;https://en.wikipedia.org/wiki/Radix_sort&quot;&gt;radix sort&lt;/a&gt; to stick them directly in their final location.&lt;/p&gt;

&lt;p&gt;We can pretty much copy the description from the wikipedia article into C++ code that looks like this:&lt;/p&gt;

&lt;div class=&quot;language-c++ highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;size_t&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;RADIX_BITS&lt;/span&gt;   &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;size_t&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;RADIX_SIZE&lt;/span&gt;   &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;size_t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;RADIX_BITS&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;size_t&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;RADIX_LEVELS&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;63&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;RADIX_BITS&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;uint64_t&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;RADIX_MASK&lt;/span&gt;   &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;RADIX_SIZE&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;using&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;queuetype&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vector&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;uint64_t&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;

&lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;radix_sort1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;uint64_t&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;size_t&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;count&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;size_t&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pass&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pass&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;RADIX_LEVELS&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pass&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;++&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;kt&quot;&gt;uint64_t&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;shift&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pass&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;RADIX_BITS&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;queuetype&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;RADIX_SIZE&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;queues&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;

        &lt;span class=&quot;c1&quot;&gt;// copy each element into the appropriate queue based on the current RADIX_BITS sized&lt;/span&gt;
        &lt;span class=&quot;c1&quot;&gt;// &quot;digit&quot; within it&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;size_t&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;count&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;++&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
            &lt;span class=&quot;kt&quot;&gt;size_t&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;value&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;];&lt;/span&gt;
            &lt;span class=&quot;kt&quot;&gt;size_t&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;index&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;value&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;shift&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;amp;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;RADIX_MASK&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;queues&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;index&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;push_back&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

        &lt;span class=&quot;c1&quot;&gt;// copy all the queues back over top of the original array in order&lt;/span&gt;
        &lt;span class=&quot;kt&quot;&gt;uint64_t&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;aptr&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;auto&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;amp;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;queue&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;queues&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;aptr&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;copy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;queue&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;begin&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;queue&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;end&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;aptr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;That’s about as simple as it gets. We decide to use one byte (i.e., radix-256) as the size of our “digit” (although it’s easy to change by adjusting the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;RADIX_BITS&lt;/code&gt; constant) and so we make 8 passes over our &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;uint64_t&lt;/code&gt; array from the least to most significant byte. At each pass we assign the current value to one of 256 “queues” (vectors in this case) based on the value of the current byte, and once all elements have been processed we copy each queue in order back to the original array. We’re done - the list is sorted.&lt;/p&gt;

&lt;p&gt;How does it perform against the usual suspects?&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/2019-05-22/fig4.svg&quot; alt=&quot;Radix 1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Well it’s not &lt;em&gt;terrible&lt;/em&gt;, and while it certainly has some issues at low element counts, it actaully squeezes into first place at 1,000,000 elements and is competitive at 100,000 and 10,000,000. Not bad for a dozen lines of code.[^rewrite]&lt;/p&gt;

&lt;p&gt;A quick check of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;time ./bench Radix1&lt;/code&gt; shows something interesting:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;real    0m1.099s
user    0m0.552s
sys     0m0.548s
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We spent about the same amount of time in the kernel (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sys&lt;/code&gt; time) as in user space. The other algorithms spend only a few lonely % in the kernel, and almost most of that is in the setup code, not in the actual sort.&lt;/p&gt;

&lt;p&gt;A deeper look with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;perf record &amp;amp;&amp;amp; perf report&lt;/code&gt; shows:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# Samples: 4K of event 'cycles:ppp'
# Event count (approx.): 2858148287
#
# Overhead  Command  Shared Object        Symbol        
# ........  .......  ...................  ................................................
#
    29.02%  bench    bench                [.] radix_sort1
    26.16%  bench    libc-2.23.so         [.] __memmove_avx_unaligned
     4.93%  bench    [kernel.kallsyms]    [k] clear_page_erms
     4.46%  bench    [kernel.kallsyms]    [k] native_irq_return_iret
     3.61%  bench    [kernel.kallsyms]    [k] error_entry
     3.04%  bench    [kernel.kallsyms]    [k] swapgs_restore_regs_and_return_to_usermode
     2.99%  bench    [kernel.kallsyms]    [k] sync_regs
     1.94%  bench    bench                [.] main
     1.91%  bench    [kernel.kallsyms]    [k] get_page_from_freelist
     1.64%  bench    libc-2.23.so         [.] __memcpy_avx_unaligned
     1.59%  bench    [kernel.kallsyms]    [k] release_pages
     1.40%  bench    [kernel.kallsyms]    [k] __handle_mm_fault
     1.37%  bench    [kernel.kallsyms]    [k] _raw_spin_lock
     1.01%  bench    [kernel.kallsyms]    [k] __pagevec_lru_add_fn
     0.88%  bench    [kernel.kallsyms]    [k] handle_mm_fault
     0.86%  bench    [kernel.kallsyms]    [k] __alloc_pages_nodemask
     0.83%  bench    [kernel.kallsyms]    [k] unmap_page_range
     0.75%  bench    [kernel.kallsyms]    [k] try_charge
     0.74%  bench    [kernel.kallsyms]    [k] get_mem_cgroup_from_mm
     0.70%  bench    bench                [.] bench_one
     0.63%  bench    [kernel.kallsyms]    [k] __do_page_fault
     0.63%  bench    [kernel.kallsyms]    [k] __mod_zone_page_state
     0.49%  bench    [kernel.kallsyms]    [k] free_pcppages_bulk
     0.45%  bench    [kernel.kallsyms]    [k] page_add_new_anon_rmap
     0.43%  bench    [kernel.kallsyms]    [k] up_read
     0.40%  bench    [kernel.kallsyms]    [k] page_remove_rmap
     0.36%  bench    [kernel.kallsyms]    [k] __mod_node_page_state
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;I’m not even going to try to explain what &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;__pagevec_lru_add_fn&lt;/code&gt; does, but the basic idea here is that we are spending a lot of time in the kernel, and we are doing that because we are allocating and freeing &lt;em&gt;a lot&lt;/em&gt; of memory. Every pass we &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;push_back&lt;/code&gt; every element into one of 256 vectors, which will be constantly growing to accomodate new elements, and then finally all the now-giant vectors are freed at the end of every allocation. That’s a lot of stress on the memory allocation paths in the kernel&lt;sup id=&quot;fnref:memalloc&quot;&gt;&lt;a href=&quot;#fn:memalloc&quot; class=&quot;footnote&quot;&gt;11&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;h3 id=&quot;radix-sort-attempt-2&quot;&gt;Radix Sort Attempt 2&lt;/h3&gt;

&lt;p&gt;Let’s try the first-thing-you-do-when-vector-is-involved-and-performance-matters; that is, let us &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;reserve()&lt;/code&gt; memory for each vector before we start adding elements. Just throw this at the start of each pass:&lt;/p&gt;

&lt;div class=&quot;language-c++ highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;auto&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;amp;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;queue&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;queues&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;queue&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reserve&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;count&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;RADIX_SIZE&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1.2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Here, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;1.2&lt;/code&gt; is an arbitrary fudge factor to account for the fact that some vectors will get more than the average number of elements. The exact value doesn’t matter much as long as it’s not too small (0.9 is a bad value, almost evey vector needs a final doubling). This gives use &lt;a href=&quot;https://github.com/travisdowns/sort-bench/blob/master/radix2.cpp&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;radix_sort2&lt;/code&gt;&lt;/a&gt; and let’s jump straight to the results (I’ve removed a couple of the less interesting sorts to reduce clutter):&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/2019-05-22/fig5.svg&quot; alt=&quot;Radix 2&quot; /&gt;&lt;/p&gt;

&lt;p&gt;I guess it’s a bit better? It does better for small array sizes, probably because the overhead of constantly resizing the small vectors is more significant there, but it is actually a bit slower for the middle sizes. System time is lower but still quite high:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;real	0m0.904s
user	0m0.523s
sys	0m0.380s
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;What we really want is to stop throwing away the memory we allocated every pass. Let’s move the queues outside of the loop and just clear them every iteration:&lt;/p&gt;

&lt;div class=&quot;language-c++ highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;radix_sort3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;uint64_t&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;size_t&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;count&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;using&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;queuetype&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vector&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;uint64_t&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;queuetype&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;RADIX_SIZE&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;queues&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;// we keep the reservation code (now outside the loop),&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;// although it matters less now since the resizing will&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;// generally only happen in the first iteration&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;auto&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;amp;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;queue&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;queues&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;queue&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reserve&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;count&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;RADIX_SIZE&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1.2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
    
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;size_t&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pass&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pass&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;RADIX_LEVELS&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pass&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;++&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;c1&quot;&gt;// ... as before&lt;/span&gt;

        &lt;span class=&quot;c1&quot;&gt;// copy all the queues back over top of the original array in order&lt;/span&gt;
        &lt;span class=&quot;kt&quot;&gt;uint64_t&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;aptr&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;auto&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;amp;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;queue&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;queues&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;aptr&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;copy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;queue&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;begin&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;queue&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;end&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;aptr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;queue&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;clear&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;// &amp;lt;--- this is new, clear the queues&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Yes another graph with Radix3 included this time:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/2019-05-22/fig6.svg&quot; alt=&quot;Radix 3&quot; /&gt;&lt;/p&gt;

&lt;p&gt;That looks a lot better! This radix sort is always faster than our earlier attempts and the fastest overall for sizes 10,000 and above. It still falls behind the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;std::&lt;/code&gt; algorithms for the 1,000 element size, where the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;O(n)&lt;/code&gt; vs &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;O(n*log(n))&lt;/code&gt; difference doesn’t play as much of a role. Despite this minor victory, and while system is reduced, we are &lt;em&gt;still&lt;/em&gt; spending 30% of our time in the kernel:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;real    0m0.612s
user    0m0.428s
sys     0m0.184s
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;pre-sizing-the-queues&quot;&gt;Pre-sizing the Queues&lt;/h3&gt;

&lt;p&gt;Sorting should be about my code, not the kernel - so let’s get rid of this kernel time for good.&lt;/p&gt;

&lt;p&gt;To do that, we’ll move away from &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;std::vector&lt;/code&gt; entirely and just allocate one large temporary region for all of our queues. Although we know the &lt;em&gt;total&lt;/em&gt; final size of all the queues (it’s the same size as the input array), we don’t know how big any &lt;em&gt;particular&lt;/em&gt; queue will be. This means we don’t know exactly how to divide up the region. A well-known solution to this problem is to first count the number of number of values that will fall into each queue so they can be sized appropriately (also known as taking the histogram of the data). As a bonus, we can count the frequencies for all radix passes in a single trip over the data, so we expect this part to be much cheaper than the radix sort proper which needs a separate pass for each “digit”.&lt;/p&gt;

&lt;p&gt;Knowing the size of each queue allows us to pack all the values exactly within a single temporary region. The copy at the end of each stage is just a single linear copy. The code is longer now since we need to implement the frequency counting gives us &lt;a href=&quot;https://github.com/travisdowns/sort-bench/blob/f05c53d02f8f374486c0f445ef519c1f47be95ce/radix4.cpp#L31&quot;&gt;radix_sort4&lt;/a&gt;. Results:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/2019-05-22/fig7.svg&quot; alt=&quot;Radix 4&quot; /&gt;&lt;/p&gt;

&lt;p&gt;It’s a significant speedup over Radix 3, especially at small sizes (speedup about 3x) but still good at large sizes (about 1.3x for 10m elements). The speedup over poor &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;qsort&lt;/code&gt; ranges from 3.7x to 5.45x, increasing at larger sizes. Even compared to the best contented from the standard library, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;std::stable_sort&lt;/code&gt;, the speedup averages about 2x.&lt;/p&gt;

&lt;h3 id=&quot;are-we-done-yet&quot;&gt;Are We Done Yet?&lt;/h3&gt;

&lt;p&gt;So are we done yet? Can we squeeze out some more performance?&lt;/p&gt;

&lt;p&gt;One little trick is to note that the temporary “queue area” and the original array are now of the same size and type, so rather than always performing the radix passes from the original array to the temporary area (which requires a copy back to the original array each time), we can instead copy back and forth between these two areas, alternating the “from” and “to” areas each time. This saves a copy each pass.&lt;/p&gt;

&lt;p&gt;The results in &lt;a href=&quot;https://github.com/travisdowns/sort-bench/blob/f05c53d02f8f374486c0f445ef519c1f47be95ce/radix5.cpp#L31&quot;&gt;radix_sort5&lt;/a&gt; and it provides a small but measurable benefit:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/2019-05-22/fig8.svg&quot; alt=&quot;Radix 5&quot; /&gt;&lt;/p&gt;

&lt;p&gt;It’s also interesting how &lt;em&gt;small&lt;/em&gt; the improvement is. This change actually cuts the memory bandwidth requirements of the algorithm almost exactly in half: rather than reading and writing each element twice during each pass (once during the sort and once in the final copy), we read them only once (it’s not &lt;em&gt;exactly&lt;/em&gt; half because the single histogramming pass adds another read). Yet the overall speedup is small, in the range of 1.05x to 1.2x. From this we can conclude that we are not approaching the memory bandwidth limits in the radix passes.&lt;/p&gt;

&lt;p&gt;There is a catch here: at the end of the sort, if we have done an &lt;em&gt;odd&lt;/em&gt; number of passes, the final sorted results will be in the temporary area, not in the original array, so we need to copy back to the original array - but 1 extra copy is better than 8! In any case, with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;RADIX_BITS == 8&lt;/code&gt; as we’ve chosen, there are an even number of copies, so this code never executes in our benchmark.&lt;/p&gt;

&lt;h3 id=&quot;pointless-work-is-pointless&quot;&gt;Pointless Work is Pointless&lt;/h3&gt;

&lt;p&gt;Another observation we can make is that for this input (and many inputs in the real world), many of the radix passes do nothing. All the input values are less than 40,000,000,000. In 64-bit hex that looks like &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;0x00000009502F9000&lt;/code&gt; - the top 28 bits are always zero. Any radix pass that uses these all-zero bits is pointless: every element will be copied to the first queue entry, one by one: essentially it’s a slow, convoluted &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;memcpy&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;We can simply skip these “trivial” passes by examining the frequency count: if all counts are zero except a single entry, the pass does nothing. This gives us &lt;a href=&quot;https://github.com/travisdowns/sort-bench/blob/master/radix6.cpp&quot;&gt;radix_sort6&lt;/a&gt;, which ends up cutting out 3 of the 8 radix passes leading to performance like this (I’ve changed the scale to emphasize the faster algorithms as they were getting crowed down at the bottom):&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/2019-05-22/fig9.svg&quot; alt=&quot;Radix 6&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In relative terms this provides a significant speedup ranging from 1.2x to 1.5x over &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;radix_sort5&lt;/code&gt;. The theoretical speedup from skipping 3 of the 8 passes is 1.6x, but we don’t achieve that because there is work outside of the core passes (counting the frequencies, for example) and also because the 3 trivial passes were actually slightly faster than the non-trivial ones because of better caching behavior.&lt;/p&gt;

&lt;h3 id=&quot;unpointless-prefetch&quot;&gt;Unpointless Prefetch&lt;/h3&gt;

&lt;p&gt;So how much more juice can we squeeze from this performance orange? Will this post ever come to an end? Has anyone even made it this far?&lt;/p&gt;

&lt;p&gt;As it turns out we’re not done yet, and the next change is perhaps the easiest one yet, a one-liner. First, let us observe that the core radix sort loop does a linear read through the elements (very prefetch and cacheline locality friendly), and then a &lt;em&gt;scattered store&lt;/em&gt; to one of 256 locations depending on the value. We might expect that those scattered stores are problematic, since we don’t expect the prefetcher to track 256 different streams. On the other hand, stores are somewhat “fire and forget”, because after we execute the store, they can just sit around in the store buffer for as long as needed while the associated cache lines are fetched: they don’t participate in any dependency chains&lt;sup id=&quot;fnref:not-sfw&quot;&gt;&lt;a href=&quot;#fn:not-sfw&quot; class=&quot;footnote&quot;&gt;12&lt;/a&gt;&lt;/sup&gt;. So maybe they aren’t causing a problem?&lt;/p&gt;

&lt;p&gt;Let’s check that theory using the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;resource_stalls.sb&lt;/code&gt; event, which tells us how many cycles we stalled store buffer was full, using this magical invocation:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;for i in {3..8}; do s=$((10**$i)); rm -f bench.o; echo &quot;SIZE=$s, KB=$(($s*8/1000))&quot;; make EFLAGS=-DSIZE=$s; perf stat -e cycles,instructions,resource_stalls.any,resource_stalls.sb ./bench Radix6; done
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This tests a variety of different sizes and here’s typical output when the array to sort has 1 million elements (8 MB):&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;SIZE=1000000, KB=8000
...
 Performance counter stats for './bench Radix6':

     5,544,710,461      cycles
     8,219,301,287      instructions              #    1.48  insn per cycle
     2,917,340,919      resource_stalls.any
     2,454,399,834      resource_stalls.sb
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;out of 5.54 billion cycles, we are stalled because the store buffer is full in 2.45 billion of them. So … a lot.&lt;/p&gt;

&lt;p&gt;One fix for this is a one-liner in the main radix-sort loop:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;for (size_t i = 0; i &amp;lt; count; i++) {
    size_t value = from[i];
    size_t index = (value &amp;gt;&amp;gt; shift) &amp;amp; RADIX_MASK;
    *queue_ptrs[index]++ = value;
    __builtin_prefetch(queue_ptrs[index] + 1); // &amp;lt;-- the magic happens here
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;That’s it. We prefetch the next plus one position in the queue after writing to the queue. 87.5% of the time this does nothing, since the next position is either in the same cache line (6 out of 8 times) or we already prefetched it the last time we wrote to this queue (1 out of 8 times).&lt;/p&gt;

&lt;p&gt;The other 12.5% of the time it helps, producing results like this:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/2019-05-22/fig10.svg&quot; alt=&quot;Radix 7&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The speedup is zero at the smallest size (1000 elements, aka 8 KB) which fits in L1, but ranges between 1.31x and 1.45x as soon as the data set exceeds L1. In principle, I wouldn’t expect prefetch to help here: we expect it to help for loads if we can start the load early, but for stores, with a full store buffer the CPU can already pick from 50+ stores to start prefetching. That is, the CPU already &lt;em&gt;knows&lt;/em&gt; what stores are coming becaue the store buffer is full of them. However, in practice, theory and practice are different and in particular Intel CPUs seem to struggle when loads that hit in L1 are interleaved with loads that don’t, &lt;a href=&quot;/blog/2019/03/19/random-writes-and-microcode-oh-my.html&quot;&gt;especially with recent microcode&lt;/a&gt;&lt;sup id=&quot;fnref:microcode&quot;&gt;&lt;a href=&quot;#fn:microcode&quot; class=&quot;footnote&quot;&gt;13&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;p&gt;That interleaved scenario will happen all the time with this type of scattered write pattern, and as noted in the earlier post, prefetch is one way of mitigating this. For the 8 MB working set we now have the following &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;perf stat&lt;/code&gt; results:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;SIZE=1000000, KB=8000
...
 Performance counter stats for './bench Radix7':

     4,342,298,986      cycles
     8,719,315,140      instructions              #    2.01  insn per cycle
     1,555,574,009      resource_stalls.any
       623,342,154      resource_stalls.sb

       1.675302704 seconds time elapsed
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;About a four-fold reduction in store-buffer stalls. Reductions are even more dramatic for other sizes: the 100,000 element (8 KB) size has an even larger reduction, from being stalled 43% of the time down to 5% after prefetching is added.&lt;/p&gt;

&lt;h2 id=&quot;whats-next&quot;&gt;What’s Next?&lt;/h2&gt;

&lt;p&gt;What’s next is that I have to eat. So while we’re not done here yet, the remaining part of this trip down the radix sort hole will have to wait for part 2.&lt;/p&gt;

&lt;p class=&quot;info&quot;&gt;If you liked this post, check out the &lt;a href=&quot;/&quot;&gt;homepage&lt;/a&gt; for others you might enjoy&lt;sup id=&quot;fnref:noenjoy&quot;&gt;&lt;a href=&quot;#fn:noenjoy&quot; class=&quot;footnote&quot;&gt;14&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;hr /&gt;
&lt;hr /&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;div class=&quot;footnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:distinct&quot;&gt;
      &lt;p&gt;Note that wanting &lt;em&gt;distinct&lt;/em&gt; values here is key. Without that restriction, the problem is simple: just use the output of any decent random number generator. Returning only distinct entries is trickier: you have to find a way to avoid or remove duplicate elements. Well it’s not all that tricky: you can simply remember existing elements using a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;std::set&lt;/code&gt; and reject any duplicate elements. What is tricky is doing it quickly. &lt;a href=&quot;#fnref:distinct&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:danmethod&quot;&gt;
      &lt;p&gt;In particular, the method he suggests picks random values one-at-a-time, already in sorted order, deciding on the gap to the next number by drawing randomly from a geometric distribution. Read his post for full details, but the one sentence summary is that Dan finds that the geometric distribution approach has favorable performace results compared to the sorted one (more than 2x as fast). Both this method and the sorting method have the little problem that the resulting list may be smaller than requested. For example in the case of the sorting method, the output after de-duplication is smaller if there are any collisions (and due to the &lt;a href=&quot;https://en.wikipedia.org/wiki/Birthday_problem&quot;&gt;Birthday Problem&lt;/a&gt; that is a lot likelier than you might think). One could cope with this by generating slightly more numbers than required, and then removing randomly selected elements until the list reaches its desired size. &lt;a href=&quot;#fnref:danmethod&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:long-tail&quot;&gt;
      &lt;p&gt;There is a long tail of other functions here, but they add up to only about 2% of the runtime, so you can safely ignore them. &lt;a href=&quot;#fnref:long-tail&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:annotate-command&quot;&gt;
      &lt;p&gt;I get the annotated assembly with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;perfc annotate -Mintel --stdio --symbol=msort_with_tmp.part.0&lt;/code&gt; - this only shows assmebly because the function is in glibc and there are no debug symbols for that library on this host. In any case, assembly is probably what we want. &lt;a href=&quot;#fnref:annotate-command&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:msort-note&quot;&gt;
      &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;msort.c&lt;/code&gt; implements a mergesort algoirhtm. There is also a file &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;qsort.c&lt;/code&gt; which implements a traditional partition-around-a-pivot based quicksort, but it seems not used to implement &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;qsort()&lt;/code&gt; on recent gcc versions. &lt;a href=&quot;#fnref:msort-note&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:inline-hard&quot;&gt;
      &lt;p&gt;It turns out that inlining the comparator is not simply a matter of compiling &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;msort.c&lt;/code&gt; and the comparator in the same translation unit while still passing the comparator as a function pointer, so the compiler can see the definition. This doens’t pan out because msort (a) is a recursive function, which means unlimited inlining isn’t possible (so at some point there is an out-of-line call where the identity of the comparator will be lost) and (b) the comparator function is saved to memory (in the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;msort_param&lt;/code&gt; struct) and used from there, which makes it harder for compilers to prove the comparator is always the one originally passed in. Instead, I use a template version of msort which takes the comparator of arbitrary type &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;C&lt;/code&gt;, and in the case that a functor object is passed, the comparator is built right into the signature of the function, making inlining the comparator basically automatic for any compiler. &lt;a href=&quot;#fnref:inline-hard&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:cmdline1&quot;&gt;
      &lt;p&gt;We obtain this output with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;perf record ./bench qsort-inlined &amp;amp;&amp;amp; perfc annotate -Mintel --stdio --no-source --symbol='msort_with_tmp&amp;lt;CompareU64&amp;gt;'&lt;/code&gt;, if your version of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;perf&lt;/code&gt; supports de-mangling names (otherwise you’ll have to use the mangled name). &lt;a href=&quot;#fnref:cmdline1&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:load-redundancy&quot;&gt;
      &lt;p&gt;Technically, there is still some redundancy since we load &lt;em&gt;two&lt;/em&gt; elements every iteration, whereas we really only need to load one: the new element from whichever list an element was removed from. You can still do that in a branch-free way, but the transformation is aparently beyond the capability of the compiler. &lt;a href=&quot;#fnref:load-redundancy&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:partial-sort&quot;&gt;
      &lt;p&gt;It’s hard to blame &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;std::partial_sort&lt;/code&gt; here, after all it is a specialized sort for cases where you want need sort only a subset of the input to be sorted, e.g., the first 100 elements of a 100,000 element sequence. However, we can use it as a full sort simply by specifying that we want the full range, but one would not expect the algorithm to be optimized for that case. &lt;a href=&quot;#fnref:partial-sort&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:stablesort&quot;&gt;
      &lt;p&gt;I had to edit this part of the blog entry because something weird happened: originally, my results always showed that &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;std::stable_sort&lt;/code&gt; was faster than &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;std::sort&lt;/code&gt; across all input sizes. After installing a bunch of OS packages and restarting, however, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;std::stable_sort&lt;/code&gt; performance came back down to earth, around 1.4x slower than before and now slower than &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;std::sort&lt;/code&gt; across all input sizes. I don’t know what changed. I did find that before the change &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;std::stable_sort&lt;/code&gt; had very few (&amp;lt; 1%) branch mispredictions, while after it had many mispredictions (about 15%). &lt;a href=&quot;#fnref:stablesort&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:memalloc&quot;&gt;
      &lt;p&gt;This behavior actually depends pretty heavily on your memory allocator. The default glibc allocator I’m using likes to give memory allocations above a certain size back to the OS whenever they are freed, which means this workload turns into a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mmap&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;munmap&lt;/code&gt; workout for the kernel. Using an allocator that wasn’t too worried about memory use and kept these pages around for its own use would result in a very different profile. &lt;a href=&quot;#fnref:memalloc&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:not-sfw&quot;&gt;
      &lt;p&gt;Stores &lt;em&gt;never&lt;/em&gt; participate in normal “register carried” dependency chains, since they do not write any registers, but they can still participate in chains through memory, e.g., if a store is followed by a load that reads the same location, the load depends on the store (and efficiently this dependency is handled depends on a lot on the hardware). This case doesn’t apply here because we don’t read the recently written queue locations any time soon: our stores are truly “fire and forget”. &lt;a href=&quot;#fnref:not-sfw&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:microcode&quot;&gt;
      &lt;p&gt;In fact, if you have an older microcode on your machine, you will see different results: there will be very little difference between Radix6 and Radix7 because Radix6 is considerably faster if you don’t update your microcode. &lt;a href=&quot;#fnref:microcode&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:noenjoy&quot;&gt;
      &lt;p&gt;On the other hand, if you didn’t enjoy it, I have to wonder how you made it this far! Maybe you started off liking it, then felt invested and compelled to finish it? Beware the sunk cost fallacy! &lt;a href=&quot;#fnref:noenjoy&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><author><name>Travis Downs</name><email>travis.downs@gmail.com</email></author><category term="blog" /><category term="algorithms" /><category term="memory" /><category term="performance" /><category term="perf" /><summary type="html">Recently, Daniel Lemire tackled the topic of selecting N distinct numbers at random. In the case we want sorted output, an obvious solution presents itself: sorting randomly chosen values and de-duplicating the list, which is easy since identical values are now adjacent.1 Note that wanting distinct values here is key. Without that restriction, the problem is simple: just use the output of any decent random number generator. Returning only distinct entries is trickier: you have to find a way to avoid or remove duplicate elements. Well it’s not all that tricky: you can simply remember existing elements using a std::set and reject any duplicate elements. What is tricky is doing it quickly. &amp;#8617;</summary></entry><entry><title type="html">What has your microcode done for you lately?</title><link href="http://localhost:4000/blog/2019/03/19/random-writes-and-microcode-oh-my.html" rel="alternate" type="text/html" title="What has your microcode done for you lately?" /><published>2019-03-19T17:00:00+02:00</published><updated>2019-03-19T17:00:00+02:00</updated><id>http://localhost:4000/blog/2019/03/19/random-writes-and-microcode-oh-my</id><content type="html" xml:base="http://localhost:4000/blog/2019/03/19/random-writes-and-microcode-oh-my.html">&lt;h2 id=&quot;microcode-mystery&quot;&gt;Microcode Mystery&lt;/h2&gt;

&lt;p&gt;Did you ever wonder what is &lt;em&gt;inside&lt;/em&gt; those microcode updates that get silently applied to your CPU via Windows update, BIOS upgrades, and various microcode packages on Linux?&lt;/p&gt;

&lt;p&gt;Well, you are in the wrong place, because this blog post won’t answer that question (you might like &lt;a href=&quot;https://www.emsec.ruhr-uni-bochum.de/media/emma/veroeffentlichungen/2017/08/16/usenix17-microcode.pdf&quot;&gt;this&lt;/a&gt; though).&lt;/p&gt;

&lt;p&gt;In fact, the overwhelming majority of this this post is about the performance of scattered writes, and not very much at all about the details of CPU microcode. Where the microcode comes in, and what might make this more interesting than usual, is that performance on a purely CPU-bound benchmark can vary dramatically &lt;em&gt;depending on microcode version&lt;/em&gt;. In particular, we will show that the most recent Intel microcode version can significantly slow down a store heavy workload when some stores hit in the L1 data cache, and some miss.&lt;/p&gt;

&lt;p&gt;My results are intended to be reproducible and the benchmarking and data collection code is available as described &lt;a href=&quot;#the-source&quot;&gt;at the bottom&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;a-series-of-random-writes&quot;&gt;A series of random writes&lt;/h2&gt;

&lt;p&gt;How fast can you perform a series of random writes? Because of the importance of caching, you might reasonably expect that it depends heavily on how big of a region the writes are scattered across, and you’d be right. For example, if we test a series of random writes to a region that fits entirely in L1, we find that random writes take almost exactly 1 cycle on modern Intel chips, matching the published limit of one write per cycle &lt;sup id=&quot;fnref:2&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;p&gt;If we use larger regions, we expect performance to slow down as many of the writes miss to outer cache levels. In fact, I measure roughly the following performance whether for linear (64 byte stride) or random writes to various sized regions:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Region Size&lt;/th&gt;
      &lt;th&gt;Cycles/Write&lt;/th&gt;
      &lt;th&gt;Typical Read Latency&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;L1&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;5&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;L2&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;12&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;L3&lt;/td&gt;
      &lt;td&gt;5-6&lt;/td&gt;
      &lt;td&gt;~35&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;RAM&lt;/td&gt;
      &lt;td&gt;15-20&lt;/td&gt;
      &lt;td&gt;~200&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;I’ve also included a third column in the table above which records typical read latency figures for each cache level. This gives an indication of roughly how &lt;em&gt;far away&lt;/em&gt; a cache is from the core, based on the round-trip read time. Since all normal stores&lt;sup id=&quot;fnref:normal-stores&quot;&gt;&lt;a href=&quot;#fn:normal-stores&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt; also involve a read (to get the cache line to write to into the L1 cache with its existing contents), the time to “complete” a single store should be at least that long&lt;sup id=&quot;fnref:4&quot;&gt;&lt;a href=&quot;#fn:4&quot; class=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt;. As the observed time per write is much less, these tests must exhibit significant &lt;a href=&quot;https://en.wikipedia.org/wiki/Memory-level_parallelism&quot;&gt;memory level parallelism&lt;/a&gt; (&lt;abbr title=&quot;Memory level parallelism: having multiple misses to memory outstanding from a single core. When used as a metric, it refers to the average number of outstanding requests over some period.&quot;&gt;MLP&lt;/abbr&gt;), i.e., several store misses are in-progress in the memory subsystem at once and their latencies overlap.  We usually care about &lt;abbr title=&quot;Memory level parallelism: having multiple misses to memory outstanding from a single core. When used as a metric, it refers to the average number of outstanding requests over some period.&quot;&gt;MLP&lt;/abbr&gt; when it comes to loads, but it is important also for a long stream of stores such as these benchmarks. The last line in above table implies that we may have requests for 10 or more stores in flight in the memory subsystem at once, in order to achieve average store time of 15-20 cycles with a memory latency of 200 cycles.&lt;/p&gt;

&lt;p&gt;You can reproduce this table yourself using the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;wrandom1-unroll&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;wlinear1&lt;/code&gt; tests.&lt;/p&gt;

&lt;h2 id=&quot;interleaved-writes&quot;&gt;Interleaved writes&lt;/h2&gt;

&lt;p&gt;Let’s move on to the case where we actually observe some interesting behavior. Here we tackle the same scenario that I asked about in a &lt;a href=&quot;https://twitter.com/trav_downs/status/1103396480994422784&quot;&gt;twitter poll&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Consider the following loop, which writes randomly to &lt;em&gt;two&lt;/em&gt; character arrays.&lt;/p&gt;

&lt;div class=&quot;language-c++ highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;writes_inter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;size_t&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;iters&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;char&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;size_t&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;size1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;char&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;size_t&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;size2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;rng_state&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rng&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;RAND_INIT&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;do&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;kt&quot;&gt;uint32_t&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;RAND_FUNC&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;amp;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rng&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;a1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;amp;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;size1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;a2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;amp;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;size2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;while&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;iters&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Let’s say we fix the size of the first array, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;size1&lt;/code&gt;, to something like half the size of the L2 cache, and evaluate the performance for a range of sizes for the second array, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;size2&lt;/code&gt;. What type of performance do we expect? We already know the time it takes for a single write to regions of various size, so in principle one might expect the above loop to perform something like the sum of the time of one write to an L2-sized region (the write to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;a1&lt;/code&gt;) and one write to a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;size2&lt;/code&gt; sized region (the write to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;a2&lt;/code&gt;).&lt;/p&gt;

&lt;p&gt;Let’s try it! Here’s a test of single stores vs interleaved stores (with one of the interleaved stores accessing a fixed 128 KiB region), varying the size of the other region, run on my Skylake i7-6700HQ.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/2019-03-19/skl/i-vs-s-old.svg&quot; alt=&quot;Interleaved vs Single stores&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Overall we see that behavior of the two benchmarks roughly track each other, with the interleaved version (twice as many stores) taking longer than the single store version, as expected.&lt;/p&gt;

&lt;p&gt;Especially for large region sizes (the right side of the graph), the assumption that interleaved accesses are more or less additive with the same accesses by themselves mostly pans out: there is a gap of about 4 cycles between the single stream and the stream with interleaved accesses, which is just slightly more than the cost of an L2 access. For small region sizes, the correspondence is less exact. In particular, the single stream drops down to ~1 cycle accesses when the region fits in L1, but in the interleaved case this doesn’t occur.&lt;/p&gt;

&lt;p&gt;At least part of this behavior makes sense: the two streams of stores will interact in the caches, and the L1 contained region isn’t really “L1 contained” in the interleaved case because the second stream of stores will be evicting lines from L1 constantly. So with a 16 KiB second region, the test really behaves as if a 16 + 128 = 144 KiB region was being accessed, i.e., L2 resident, but in a biased way (with the 16 KiB block being accessed much more frequently), so there is no sharp decrease in iteration time at the 32 KiB boundary&lt;sup id=&quot;fnref:5&quot;&gt;&lt;a href=&quot;#fn:5&quot; class=&quot;footnote&quot;&gt;4&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;h2 id=&quot;the-weirdness-begins&quot;&gt;The weirdness begins&lt;/h2&gt;

&lt;p&gt;So far, so good and nothing too weird. However, starting now, it &lt;em&gt;is&lt;/em&gt; about to get weird!&lt;/p&gt;

&lt;p&gt;Everything above is a reduced version of a benchmark I was using to test some &lt;em&gt;real code&lt;/em&gt;&lt;sup id=&quot;fnref:1&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot;&gt;5&lt;/a&gt;&lt;/sup&gt;, about a year ago. This code had a tight loop with a table lookup and then writes to two different arrays. When I benchmarked this code, performance was usually consistent with the performance of “interleaved” benchmark plotted above.&lt;/p&gt;

&lt;p&gt;Recently, I returned to the benchmark to check the performance on newer CPU architectures. First, I went back to check the results on the original hardware (the &lt;a href=&quot;https://ark.intel.com/products/88967&quot;&gt;Skylake i7-6700HQ&lt;/a&gt; in my laptop). I failed to reproduce it – I wasn’t able to achieve the same performance, with the same test and on the same hardware as before: it was always running significantly slower (about half the original speed).&lt;/p&gt;

&lt;p&gt;With some help from user Adrian on the &lt;a href=&quot;https://www.realworldtech.com/forum/?roomid=1&quot;&gt;RWT forums&lt;/a&gt; I was able to bisect the difference down to a CPU microcode update. In particular, with newest microcode version &lt;sup id=&quot;fnref:7&quot;&gt;&lt;a href=&quot;#fn:7&quot; class=&quot;footnote&quot;&gt;6&lt;/a&gt;&lt;/sup&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;0xc6&lt;/code&gt; the interleaved stores scenario runs &lt;em&gt;much&lt;/em&gt; slower. For example, the same benchmark as above now looks like this, every time you run it:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/2019-03-19/skl/i-vs-s-new.svg&quot; alt=&quot;Interleaved vs Single Stores (New Microcode)&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The behavior of interleaved for small regions (left hand side of chart) is drastically different - the throughput is less than half of the old microcode. It is not obvious just by visual comparison it, but performance is actually reduced across the range of tested sizes for the interleaved case, albeit by only a few cycles as the region size becomes large. I tested various microcode versions and found that only the most recent &lt;abbr title=&quot;Intel's Skylake (client) architecture, aka 6th Generation Intel Core i3,i5,i7&quot;&gt;SKL&lt;/abbr&gt; microcode, revision &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;0xc6&lt;/code&gt; and released in August 2018 exhibits the “always slow” behavior shown above. The preceding version &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;0xc2&lt;/code&gt; usually results in the fast behavior.&lt;/p&gt;

&lt;p&gt;What’s up with that?&lt;/p&gt;

&lt;h3 id=&quot;performance-counters&quot;&gt;Performance Counters&lt;/h3&gt;

&lt;p&gt;We can check the performance counters to see if they reveal anything. We’ll use the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;l2_rqsts.references&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;l2_rqsts.all_rfo&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;l2_rqsts.rfo_miss&lt;/code&gt; counters, which count the total number of accesses (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;references&lt;/code&gt;) and total accesses related to &lt;abbr title=&quot;Request for ownership: when a request for a cache line originates from a store, or a type of prefetch that predicts the location is likely to be the target of a store, an RFO is performed which gets the line in an exclusive MESI state.&quot;&gt;RFO&lt;/abbr&gt; requests (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;all_rfo&lt;/code&gt; aka &lt;em&gt;stores&lt;/em&gt;) from the core as well as the number that miss (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;rfo_miss&lt;/code&gt;). Since we are only performing stores, we expect these counts to match and to correspond to the number of L1 store misses, since any store that misses in L1 ultimately contributes&lt;sup id=&quot;fnref:9&quot;&gt;&lt;a href=&quot;#fn:9&quot; class=&quot;footnote&quot;&gt;7&lt;/a&gt;&lt;/sup&gt; to an L2 access.&lt;/p&gt;

&lt;p&gt;Here’s the old microcode:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/2019-03-19/skl/i-plus-counters-old.svg&quot; alt=&quot;Interleaved Stores w/ Perf Counters (old microcode)&quot; /&gt;&lt;/p&gt;

&lt;p&gt;… and with the new microcode (note the change in the y axis, it’s about 3x slower for the L1 hit region):&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/2019-03-19/skl/i-plus-counters-new.svg&quot; alt=&quot;Interleaved Stores w/ Perf Counters (new microcode)&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Despite the large difference in performance, there is very little to no difference in the relevant performance counters. In both cases, the number of L1 misses (i.e., L2 references) approaches 0.75 as the second region size approaches zero as we’d expect (all L1 hits in the second region, and about 25% L1 hits in the 128 KiB fixed region as the L1D is 25% of the size of L2). On the right side, the number of L1 misses approaches something like 1.875, as the L1 hits in the 128 KiB region are cut in half by competition with with the other large region.&lt;/p&gt;

&lt;p&gt;So despite the much slower performance, for L1-sized second regions, the difference doesn’t obviously originate in different cache hit behavior. Indeed, with the new microcode, performance goes &lt;em&gt;down&lt;/em&gt; as the L1 hit rate goes &lt;em&gt;up&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;So it seems that the likeliest explanation is that &lt;em&gt;the presence of an L1 hit in the store buffer prevents overlapping of miss handling for stores on either side&lt;/em&gt;, at least with the new microcode, on &lt;abbr title=&quot;Intel's Skylake (client) architecture, aka 6th Generation Intel Core i3,i5,i7&quot;&gt;SKL&lt;/abbr&gt; hardware. That is, a series of consecutive stores can be handled in parallel only if none of them is an L1 hit. In this way L1 store hits somehow act as a store fence with the new microcode. The performance is in line with each store going alone to the memory hierarchy: roughly the L2 latency plus a few cycles.&lt;/p&gt;

&lt;h3 id=&quot;will-the-real-sfence-please-stand-up&quot;&gt;Will the real sfence please stand up&lt;/h3&gt;

&lt;p&gt;Let’s test the “L1 hits act as a store fence” theory. In fact, there is already an instruction that acts as a store force in the x86 ISA: &lt;a href=&quot;https://www.felixcloutier.com/x86/sfence&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sfence&lt;/code&gt;&lt;/a&gt;. Repeatedly executed back-to-back this instruction only takes a &lt;a href=&quot;http://uops.info/html-instr/SFENCE-1063.html&quot;&gt;few cycles&lt;/a&gt; but its most interesting effect occurs when stores are in the pipeline: this instruction blocks dispatch of subsequent stores until all earlier stores have committed to the L1 cache, implying that stores on different sides of the fence cannot overlap&lt;sup id=&quot;fnref:sfence-note&quot;&gt;&lt;a href=&quot;#fn:sfence-note&quot; class=&quot;footnote&quot;&gt;8&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;p&gt;We will look at two version of the interleaved loop with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sfence&lt;/code&gt;: one with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sfence&lt;/code&gt; inserted right after the store to the first region (fixed 128 KiB), and the other inserted after the store to the second region - let’s call them sfenceA and sfenceB respectively. Both have the same number of fences (one per iteration, i.e., per pair of stores) and only differ in what store happens to be last in the store buffer when the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sfence&lt;/code&gt; executes. Here’s the result on the new microcode (the results on the old microcode are &lt;a href=&quot;/assets/2019-03-19/skl/i-sfence-old.svg&quot;&gt;over here&lt;/a&gt;):&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/2019-03-19/skl/i-sfence-new.svg&quot; alt=&quot;Interleaved Stores w/ SFENCE&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The right side of the graph is fairly unremarkable: both versions with sfence perform roughly at the latency for the associated cache level because there is zero memory level parallelism (no, I don’t know why one performs better than other or why the performance crosses over near 64 KiB). The left part is pretty amazing though: one of the sfence configurations is &lt;em&gt;faster than the same code without sfence&lt;/em&gt;. That’s right, adding a store serializing instruction like sfence, can speed up the code by several cycles. It doesn’t come close to the fast performance of the old microcode versions, but the behavior is very surprising nonetheless.&lt;/p&gt;

&lt;p&gt;The version that was faster, sfenceA, had the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sfence&lt;/code&gt; between the 128 KiB store and the L1 store. So perhaps there is some kind of penalty when an L1 hit store arrives right after a L1-miss-L2-hit store, in addition to the “no &lt;abbr title=&quot;Memory level parallelism: having multiple misses to memory outstanding from a single core. When used as a metric, it refers to the average number of outstanding requests over some period.&quot;&gt;MLP&lt;/abbr&gt;” penalty we normally see.&lt;/p&gt;

&lt;h2 id=&quot;larger-fixed-regions&quot;&gt;Larger fixed regions&lt;/h2&gt;

&lt;p&gt;To this point we’ve been we’ve been looking at the scenario where a write to a 128 KiB region is interleaved with a write to a region of varying size. The fixed size of 128 KiB means that most&lt;sup id=&quot;fnref:13&quot;&gt;&lt;a href=&quot;#fn:13&quot; class=&quot;footnote&quot;&gt;9&lt;/a&gt;&lt;/sup&gt; of those writes will be L2 hits. What if we make the fixed size region larger? Let’s say 2 MiB, which is much larger than L2 (256 KiB) but still fits easily in L3 (6 MiB on my CPU). Now we expect most writes to the fixed region to be L2 misses but L3 hits.&lt;/p&gt;

&lt;p&gt;What’s the behavior? Here’s the old microcode:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/2019-03-19/skl/i-vs-s-2mib-old.svg&quot; alt=&quot;Interleaved Stores w/ 2048 KiB Fixed Region&quot; /&gt;&lt;/p&gt;

&lt;p&gt;… and the new:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/2019-03-19/skl/i-vs-s-2mib-new.svg&quot; alt=&quot;Interleaved Stores w/ 2048 KiB Fixed Region&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Again we see a large performance impact with the new microcode, and the results are consistent with the theory that L1 hits in the store stream prevent overlapping of store misses on either side. In particular we see that the region with L1 hits takes about 37 cycles, almost exactly the L3 latency on this CPU. In this scenario, it is &lt;em&gt;slower to have L1 hits mixed in to the stream of accesses than to replace those L1 hits with misses to DRAM&lt;/em&gt;. That’s a remarkable demonstration of the power of memory level parallelism and of the potential impact of this change.&lt;/p&gt;

&lt;h2 id=&quot;why&quot;&gt;Why?&lt;/h2&gt;

&lt;p&gt;I can’t tell you for certain why the store related machinery acts the way it does in this case. Speculating is fun though, so lets do that. Here are a couple possibilities for why the memory model acts the way it does.&lt;/p&gt;

&lt;h3 id=&quot;the-x86-memory-model&quot;&gt;The x86 Memory Model&lt;/h3&gt;

&lt;p&gt;First, let’s quickly review the x86 memory model.&lt;/p&gt;

&lt;p&gt;The x86 has a relatively strong memory model. Intel doesn’t give it a handy name, but lets call it &lt;a href=&quot;https://www.cl.cam.ac.uk/~pes20/weakmemory/cacm.pdf&quot;&gt;x86-TSO&lt;/a&gt;. In x86-TSO, stores from all CPUs appear in a global total order with stores from each CPU consistent with program order. If a given CPU makes stores A and B in that order, all other CPUs will observe not only a consistent order of stores A and B, but the &lt;em&gt;same&lt;/em&gt; A-before-B order as the program order. All this store ordering complicates the pipeline. In weaker memory models like ARM and POWER, in the absence of fences, you can simply commit senior stores&lt;sup id=&quot;fnref:17&quot;&gt;&lt;a href=&quot;#fn:17&quot; class=&quot;footnote&quot;&gt;10&lt;/a&gt;&lt;/sup&gt; in whatever order is convenient. If some store locations are already present in L1, you can commit those, while making &lt;abbr title=&quot;Request for ownership: when a request for a cache line originates from a store, or a type of prefetch that predicts the location is likely to be the target of a store, an RFO is performed which gets the line in an exclusive MESI state.&quot;&gt;RFO&lt;/abbr&gt; requests for other store locations which aren’t in L1.&lt;/p&gt;

&lt;p&gt;An x86 CPU has a to take more conservative strategy. The basic idea is that stores are only made globally observable &lt;em&gt;in program order&lt;/em&gt; as they reach the head of the store buffer. The CPU may still try to get parallelism by prefetching upcoming stores, as described for example in Intel’s &lt;a href=&quot;https://patents.google.com/patent/US7130965/en&quot;&gt;US patent 7130965&lt;/a&gt;&lt;sup id=&quot;fnref:patent-note&quot;&gt;&lt;a href=&quot;#fn:patent-note&quot; class=&quot;footnote&quot;&gt;11&lt;/a&gt;&lt;/sup&gt; - but care must be taken. For example, any snoop request that comes in for any of the lines in flight must get a consistent result: whether the lines are in a write-back buffer being evicted from L1, in a fill buffer making their way to L2, in a write-combining buffer&lt;sup id=&quot;fnref:wc-note&quot;&gt;&lt;a href=&quot;#fn:wc-note&quot; class=&quot;footnote&quot;&gt;12&lt;/a&gt;&lt;/sup&gt; waiting to commit to L1, and so on.&lt;/p&gt;

&lt;h3 id=&quot;write-combining-buffers&quot;&gt;Write Combining Buffers&lt;/h3&gt;

&lt;p&gt;With that out of the way, let’s talk about how the store pipeline might actually work.&lt;/p&gt;

&lt;p&gt;Let’s assume that when a store misses in the L1 it allocates a &lt;em&gt;fill buffer&lt;/em&gt; to fetch the associated line from the outer levels of the memory hierarchy (we can be pretty sure this is true). Lets further assume that if another stores in the store buffer reaches the head of the store buffer and is to the same line, we get effectively a “fill buffer hit”, and that in this case the store is &lt;em&gt;merged into the existing fill buffer and removed from the store buffer&lt;/em&gt;*. That is, the fill buffer entry itself keeps track of the written bytes, and merges those bytes with any unwritten ones when the line returns from the memory hierarchy, before finally committing it to L1&lt;sup id=&quot;fnref:wc-stores&quot;&gt;&lt;a href=&quot;#fn:wc-stores&quot; class=&quot;footnote&quot;&gt;13&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;p&gt;In the scenario where there are outstanding fill buffers containing store stater, committing stores that hit in L1 is tricky: if you have several outstanding fill buffers for outstanding stores, as well as several interleaved L1-hit stores, the strong memory model used by x86 and described above means that you have to ensure that any incoming snoop requests see all those stores in the same order. You can’t just snoop all the fill buffers and then the L1 or vice-versa since that might change the apparent order. Additionally, stores become globally visible if they are committed to the L1, but the global observability point for stores whose data is being collected in the fill buffers is less clear.&lt;/p&gt;

&lt;p&gt;One simple approach for dealing with L1-hits stores when there are outstanding stores in the fill buffers is to delay the store until the outstanding stores complete and are committed to L1. This could prevent any parallelism between stores with an intervening L1 hit, unless &lt;abbr title=&quot;Request for ownership: when a request for a cache line originates from a store, or a type of prefetch that predicts the location is likely to be the target of a store, an RFO is performed which gets the line in an exclusive MESI state.&quot;&gt;RFO&lt;/abbr&gt; prefetching kicks in. So perhaps the difference is whether the &lt;abbr title=&quot;Request for ownership: when a request for a cache line originates from a store, or a type of prefetch that predicts the location is likely to be the target of a store, an RFO is performed which gets the line in an exclusive MESI state.&quot;&gt;RFO&lt;/abbr&gt; prefetch heuristic determines it is profitable to prefetch stores. Or perhaps the CPU is able to choose between two strategies in this scenario, one of which allows parallelism and one which doesn’t. For example, perhaps the L1 stores could themselves be buffered in fill buffers, which seems silly except that it may allow preserving the order among stores which both hit and miss in L1. For whatever reason the CPU choose the no-parallelism strategy more in the case of the new microcode.&lt;/p&gt;

&lt;p&gt;Perhaps the overlapping behavior was completely disabled to support some recent type of Spectre mitigation (see for example &lt;a href=&quot;https://en.wikipedia.org/wiki/Speculative_Store_Bypass&quot;&gt;SSB disable&lt;/a&gt; functionality which was probably added in this newest microcode version).&lt;/p&gt;

&lt;p&gt;Without more details on the mechanisms on modern Intel CPUs it is hard to say more, but there are certainly cases where extreme care has to be taken to preserve the order of writes. The &lt;em&gt;fill buffers&lt;/em&gt; used for L1 misses, as well as associated components in the outer cache layers already need to be ordered to support the memory model (which also disallows load-load reordering), so in that sense all the stores that miss L1 are already in good hands. Stores that want to commit directly to L1 are more problematic since they are no longer tracked and have become globally observable (a snoop may arrive at any moment and see the newly written) value. I did take a good long look at the patents, but didn’t find any smoking gun to explain the current behavior.&lt;/p&gt;

&lt;h2 id=&quot;workarounds&quot;&gt;Workarounds&lt;/h2&gt;

&lt;p&gt;Now that we’re aware of the problem, is there anything we can do in the case we are bitten by it? Yes.&lt;/p&gt;

&lt;h3 id=&quot;avoid-or-reduce-fine-grained-interleaving&quot;&gt;Avoid or reduce fine-grained interleaving&lt;/h3&gt;

&lt;p&gt;The problem occurs when you have &lt;em&gt;fine-grained&lt;/em&gt; interleaving between L1 hits and L1 misses. Sometimes you can avoid the interleaving entirely, but if not you can perhaps make it coarser grained. For example, the current interleaved test alternates between L1 misses and L1 misses, like &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;L1-hit, L1-miss, L1-hit, L1-miss&lt;/code&gt;. If you unroll by a factor of two and then move the writes to the same region to be adjacent in the source (which doesn’t change the semantics since the regions are not overlapping), you’ll coarser grained interleaving, like: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;L1-hit, L1-hit, L1-miss, L1-miss&lt;/code&gt;. Based on our theory of reduced memory level parallelism, grouping the stores in this way will allow at least &lt;em&gt;some&lt;/em&gt; overlapping (in this example, two stores can be overlapped).&lt;/p&gt;

&lt;p&gt;Let’s try this, comparing unrolling by a factor of two and four versus the plain unrolled version. The main loop in the factor of two unrolled version (the factor of 4 is equivalent) looks like:&lt;/p&gt;

&lt;div class=&quot;language-c++ highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;do&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;kt&quot;&gt;uint32_t&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;val1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;RAND_FUNC&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;amp;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rng&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
    &lt;span class=&quot;kt&quot;&gt;uint32_t&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;val2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;RAND_FUNC&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;amp;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rng&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;a1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;val1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;amp;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;size1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;a1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;val2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;amp;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;size1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;a2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;val1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;amp;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;size2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;a2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;val2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;amp;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;size2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;while&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;iters&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Here’s is the performance with a fixed array size of 2048 KiB (since the performance degradation is more dramatic with large fixed region sizes):&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/2019-03-19/skl/i-unrolled-2mib-new.svg&quot; alt=&quot;Interleaved Stores with Unrolling&quot; /&gt;&lt;/p&gt;

&lt;p&gt;For the region where L1 hits occur, the unroll by gives a 1.6x speedup, and the unroll by 4 a 2.5x speed. Even when unrolling by 4 we still see an impact from this issue (performance still improves once almost every store is an L1 miss) - but we are much closer to the expected the baseline performance before the microcode update.&lt;/p&gt;

&lt;p&gt;This change doesn’t come for free: unrolling the loop by hand has a cost in development complexity as the unrolled loop is more complicated. Indeed, the implementation in the benchmark doesn’t handle values of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;iters&lt;/code&gt; which aren’t a multiple or 2 or 4. It also has a cost in code size as the unrolled functions are larger:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Function&lt;/th&gt;
      &lt;th&gt;Loop Size in Bytes&lt;/th&gt;
      &lt;th&gt;Function Size in Bytes&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Original&lt;/td&gt;
      &lt;td&gt;40&lt;/td&gt;
      &lt;td&gt;74&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Unrolled 2x&lt;/td&gt;
      &lt;td&gt;72&lt;/td&gt;
      &lt;td&gt;108&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Unrolled 4x&lt;/td&gt;
      &lt;td&gt;140&lt;/td&gt;
      &lt;td&gt;191&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Finally, note that while more unrolling is faster in the region where L1 hists is faster, the situation reverses itself around 64 KiB, and after that point no unrolling is fastest.&lt;/p&gt;

&lt;p&gt;All this means that in this particular example you would face some tough tradeoffs if you want to reduce the impact by unrolling.&lt;/p&gt;

&lt;h3 id=&quot;prefetching&quot;&gt;Prefetching&lt;/h3&gt;

&lt;p&gt;You can solve this particular problem using software prefetching instructions. If you prefetch the lines you are going to store to, a totally different path is invoked: the same one that handles loads, and here the memory level parallelism will be available regardless of the the limitations of the store path. One complication is that, except for &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;prefetchw&lt;/code&gt;&lt;sup id=&quot;fnref:prefetchw&quot;&gt;&lt;a href=&quot;#fn:prefetchw&quot; class=&quot;footnote&quot;&gt;14&lt;/a&gt;&lt;/sup&gt;, such prefetches will be “shared OK” requests for the line, rather than an &lt;abbr title=&quot;Request for ownership: when a request for a cache line originates from a store, or a type of prefetch that predicts the location is likely to be the target of a store, an RFO is performed which gets the line in an exclusive MESI state.&quot;&gt;RFO&lt;/abbr&gt; (request for ownership). This means that the core might receive the line in the S MESI state, and then when the store occurs, a &lt;em&gt;second&lt;/em&gt; request may be incurred to change the line from S state to M state. In my testing this didn’t see to be a problem in practice, perhaps because the lines are not shared across cores so generally arrive in the E state, and the E-&amp;gt;M transition is cheap.&lt;/p&gt;

&lt;p&gt;We don’t even really have to &lt;strong&gt;pre&lt;/strong&gt;-fetch: that is, we don’t need to issue the prefetch instructions early (which would be hard in this case since we’d need to run ahead of the RNG) - we just issue the PF at the same spot we would have otherwise done the store. This transforms the nature of the request from a store to a load, which is the goal here - even though it doesn’t make the request visible to the CPU any earlier than before.&lt;/p&gt;

&lt;p&gt;One question is which of the two regions to prefetch? The fixed region, the variable region or both? It turns out that “both” is a fine strategy and is often the sole fastest approach and is generally tied in the remaining cases. Here’s a look at all three approaches against no prefetching at all on the new microcode (128 KiB fixed region size):&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/2019-03-19/skl/i-prefetch-new.svg&quot; alt=&quot;Interleaved Stores with Prefetching&quot; /&gt;&lt;/p&gt;

&lt;p&gt;A key observation is that if you had decided to only prefetch one &lt;em&gt;or&lt;/em&gt; the other of the two stores, you’d be slower than no prefetching at all over most of the range. It isn’t exactly clear to me why this is the case: perhaps the prefetches compete for fill buffers or otherwise result in a worse allocation of fill buffers to requests.&lt;/p&gt;

&lt;h3 id=&quot;avoiding-microcode-updates&quot;&gt;Avoiding Microcode Updates&lt;/h3&gt;

&lt;p&gt;The simplest solution is to simply avoid the newest microcode updates. These updates seem drive by new spectre mitigations, so if you are not enabling that functionality (e.g., SSDB is disabled by default in Linux, so if you aren’t explicitly enabling it, you won’t get it), perhaps you can do without these updates.&lt;/p&gt;

&lt;p&gt;This strategy is not feasible once the microcode update contains something you need.&lt;/p&gt;

&lt;p&gt;Additionally, as noted above, even the old microcodes &lt;em&gt;sometimes&lt;/em&gt; experience the same slower performance that new microcodes always exhibit. I cannot exactly characterize the conditions in which this occurs, but one should at least be aware that old microcodes aren’t &lt;em&gt;always&lt;/em&gt; fast.&lt;/p&gt;

&lt;h2 id=&quot;other-findings&quot;&gt;Other findings&lt;/h2&gt;

&lt;p&gt;This post is already longer than I wanted it to be. The idea is for posts closer in length to &lt;a href=&quot;https://shipilev.net/jvm/anatomy-quarks/&quot;&gt;JVM Anatomy Park&lt;/a&gt; than &lt;a href=&quot;https://en.wikipedia.org/wiki/War_and_Peace&quot;&gt;War and Peace&lt;/a&gt;. Still, there is a bunch of stuff uncovered which I’ll summarize here:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The current test uses regions whose addresses whose bottom 12 bits are identically zero, but whose 13th bit varies. That is, the regions “4K alias” but do not “8K alias”. Since the main loop uses the same random address for both regions (wrapped to region size by masking) in each iteration, this means that the stores alias as describe above. However, this is not the cause of the main effects reported here: you can remove the aliasing completely and the behavior is largely the same&lt;sup id=&quot;fnref:11&quot;&gt;&lt;a href=&quot;#fn:11&quot; class=&quot;footnote&quot;&gt;15&lt;/a&gt;&lt;/sup&gt;.&lt;/li&gt;
  &lt;li&gt;You can go the other way too: if you increase the aliasing (you can try this by setting environment variable &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ALLOW_ALIAS=1&lt;/code&gt;) up to 64 KiB (bottom 16 bits of the &lt;em&gt;physical&lt;/em&gt; address), I found a strong effect where performance was slower with the &lt;em&gt;old&lt;/em&gt; microcode. This effect seems to have disappeared with the new microcode. Now 64 KiB aliasing (especially &lt;em&gt;physical&lt;/em&gt; aliasing) is probably a lot more rare than mixed L1 hits and L1 misses in the stream of stores, so I’d rather the old behavior than the new - but this is probably interesting enough to write about separately.&lt;/li&gt;
  &lt;li&gt;I do sometimes see the “slow mode” behavior with earlier microcode versions. Almost a year ago, when the last several version of the microcode didn’t even exist, I experienced periodic slow mode behavior while benchmarking - the same type of performance in the L1 region as the current microcode shows all the time. On older microcode I can still reproduce this consistently: &lt;em&gt;if all CPUs are loaded when I start the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;bench&lt;/code&gt; process&lt;/em&gt;. For example &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;./bench interleaved&lt;/code&gt; consistently gives fast mode, but &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;stress -c 4 &amp;amp; ./bench interleaved&lt;/code&gt; consistently gives slow timings … &lt;em&gt;even when I kill the CPU using processes before the results roll in&lt;/em&gt;. In that case, the tests keep running in slow mode even though it’s the only thing running on the system.&lt;br /&gt;&lt;br /&gt;This seems to explain why I randomly got slow mode in the past. For example, I noticed that something like &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;./bench interleaved &amp;gt; data; plot-csv.py data&lt;/code&gt; would give fast mode results, but when I shortened it to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;./bench interleaved | plot-csv.py&lt;/code&gt; it would be in slow mode, because apparently launching the python interpreter in parallel on the RHS of the pipe used enough CPU to trigger the slow mode. I had a weird 10 minutes or so where I’d run &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;./bench&lt;/code&gt; without piping it and look at the data, and then try to plot it and it would be totally different, back and forth.&lt;/li&gt;
  &lt;li&gt;I considered the idea that this bad behavior only shows up when the store buffer is full, e.g., because of some interaction that occurs when renaming is stalled on store buffer entries, but versions of the test which periodically drain the store buffer with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sfence&lt;/code&gt; so it never becomes very full showed the same result.&lt;/li&gt;
  &lt;li&gt;I examined the values of a lot more performance counters than the few shown above, but none of them provided any smoking gun for the behavior: they were all consistent with L1 hits simply blocking overlap of L1 miss stores on either side.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;other-platforms&quot;&gt;Other platforms&lt;/h3&gt;

&lt;p&gt;An obvious and &lt;abbr title=&quot;When discussing assembly instructions an immediate is a value embedded in the instruction itself, e.g., the 1 in add eax, 1.&quot;&gt;immediate&lt;/abbr&gt; question is what happens on other micro-architectures, beyond my Skylake client core.&lt;/p&gt;

&lt;p&gt;On Haswell, the behavior is &lt;em&gt;always slow&lt;/em&gt;. That is, whether with old or new microcode, store misses mixed with L1 store hits were much slower than expected. So if you target Haswell or (perhaps) Broadwell era hardware, you might want to keep this in mind regardless of microcode version.&lt;/p&gt;

&lt;p&gt;On Skylake-X (Xeon W-2401), the behavior is &lt;em&gt;always fast&lt;/em&gt;. That is, even with the newest microcode version I did not see the slow behavior. I also was not able to trigger the behavior by starting the test with loaded CPUs as I was with Skylake client with old microcode.&lt;/p&gt;

&lt;p&gt;On Cannonlake I did not observe the slow behavior. I don’t know if I was using an “old” or “new” microcode as Intel does not publish microcode guidance for Cannonlake (and it isn’t clear to me if any Cannonlake microcodes have been released at all as very few chips were ever shipped).&lt;/p&gt;

&lt;p&gt;You can look at the results for all the platforms I tested in the &lt;a href=&quot;/assets/2019-03-19&quot;&gt;assets directory&lt;/a&gt;. The plots are the same as described above for Skylake plus some variants not show but which should be obvious from the title or filename.&lt;/p&gt;

&lt;h2 id=&quot;the-source&quot;&gt;The Source&lt;/h2&gt;

&lt;p&gt;You can have fun reproducing all these results yourself as my code is available in the store-bench project &lt;a href=&quot;https://github.com/travisdowns/store-bench&quot;&gt;on GitHub&lt;/a&gt;. Documentation is a bit lacking, but it shouldn’t be to hard to figure out. Open an issue if anything is unclear or you find a bug, and pull requests gladly accepted.&lt;/p&gt;

&lt;h2 id=&quot;thanks&quot;&gt;Thanks&lt;/h2&gt;

&lt;p&gt;Thanks to Nathan Kurz and Leonard Inkret who pointed out some errors in the text and to &lt;a href=&quot;https://lemire.me/en/&quot;&gt;Daniel Lemire&lt;/a&gt; who kindly provided additional hardware on which I was able to test these results.&lt;/p&gt;

&lt;h2 id=&quot;comments&quot;&gt;Comments&lt;/h2&gt;

&lt;p&gt;I don’t have a comment system, but I’ll reply to stuff posted in the &lt;a href=&quot;https://news.ycombinator.com/item?id=19445566&quot;&gt;Hacker News discussion&lt;/a&gt;, or &lt;a href=&quot;https://twitter.com/trav_downs/status/1108415599267450882&quot;&gt;on Twitter&lt;/a&gt;.&lt;/p&gt;

&lt;p class=&quot;info&quot;&gt;If you liked this post, check out the &lt;a href=&quot;/&quot;&gt;homepage&lt;/a&gt; for others you might enjoy&lt;sup id=&quot;fnref:noenjoy&quot;&gt;&lt;a href=&quot;#fn:noenjoy&quot; class=&quot;footnote&quot;&gt;16&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;hr /&gt;
&lt;hr /&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;div class=&quot;footnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:2&quot;&gt;
      &lt;p&gt;Of course, to achieve one-write per cycle, your benchmark has to be otherwise quite efficient: among other things the process by which you generate random addresses needs to have a throughput of one per cycle too, so usually you’ll want cheat a bit on the RNG side. I wrote such a test and you can run it with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;./bench wrandom1-unroll&lt;/code&gt;. For buffer sizes that fit within L1, it achieves very close to 1 cycle per write (roughly 1.01 cycles per write for most buffer sizes). &lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:normal-stores&quot;&gt;
      &lt;p&gt;Here “normal stores” basically means stores that are to write-back (WB) memory regions and which are not the special non-temporal stores that x86 offers. Almost every store you’ll do from a typical program falls into this category. &lt;a href=&quot;#fnref:normal-stores&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:4&quot;&gt;
      &lt;p&gt;In fact, we can test this - the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;wlinear1-sfence&lt;/code&gt; test is a linear write test like &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;wlinear1&lt;/code&gt; except with an &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sfence&lt;/code&gt; instruction between every store. This flushes the store buffer, preventing any overlap in the stores and the observed time per store is in all cases a couple of cycles above the corresponding read latency (probably corresponding to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sfence&lt;/code&gt; overhead). &lt;a href=&quot;#fnref:4&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:5&quot;&gt;
      &lt;p&gt;This isn’t really the whole story though. If the L1 cache misses explained it, we’d expect performance to approach ~4 cycles (1 cycle L1 + 3 cycle L2) as the size of the region approaches 0, since at some point the smaller region will stay in L2 regardless of interference from other stores. It doesn’t happen though: performance flatlines at 6 cycles, the cost of two stores to L2. Perhaps what happens is that the L1 stores in the store buffer reduce the &lt;abbr title=&quot;Memory level parallelism: having multiple misses to memory outstanding from a single core. When used as a metric, it refers to the average number of outstanding requests over some period.&quot;&gt;MLP&lt;/abbr&gt; of the interleaved L2 stores because the &lt;abbr title=&quot;Request for ownership: when a request for a cache line originates from a store, or a type of prefetch that predicts the location is likely to be the target of a store, an RFO is performed which gets the line in an exclusive MESI state.&quot;&gt;RFO&lt;/abbr&gt; prefetch mechanism only has a certain horizon it examines. For example, maybe it examines the 10 entries closest to the store buffer head for prefetch candidates, and with the L1-hitting stores in there, there are only half as many L2-hitting stores to fetch. &lt;a href=&quot;#fnref:5&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:1&quot;&gt;
      &lt;p&gt;By &lt;em&gt;real code&lt;/em&gt; I simply mean something that is not a benchmark, not necessarily anything actually useful. &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:7&quot;&gt;
      &lt;p&gt;See your microcode version on Linux using &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;cat /proc/cpuinfo | grep -m1 micro&lt;/code&gt; or &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;dmesg | grep micro&lt;/code&gt;. The latter option also helps you determine if the microcode was updated during boot by the Linux microcode driver. &lt;a href=&quot;#fnref:7&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:9&quot;&gt;
      &lt;p&gt;I used the weasel word “contributes” here rather than “ultimately results in an L1 miss” to cover the case where two stores occur to the same line in short succession and that line is not in L1. In this case, both stores will miss, but there will generally only be one reference to L2 since the fill buffers operate on whole cache lines, so both stores will be satisfied by the same miss. The same effect occurs for loads and can be measured explicitly by the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mem_load_retired.fb_hit&lt;/code&gt; event: those are loads that missed in L1, but subsequently hit the &lt;em&gt;fill buffer&lt;/em&gt; (aka miss status handling register) allocated for an earlier access to the same cache line that also missed. &lt;a href=&quot;#fnref:9&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:sfence-note&quot;&gt;
      &lt;p&gt;Actually, this doesn’t seem to be strictly true. The results on some CPUs are too good to represent zero overlapping between stores. E.g., the &lt;a href=&quot;/assets/2019-03-19/skl/i-sfence-old.svg&quot;&gt;old microcode results&lt;/a&gt;) show the sfenceB results staying under 30 cycles even for main-memory sized regions (and quite close to the no sfence results), which is only possible with a lot of store overlapping. So something remains to be discovered about sfence behavior. &lt;a href=&quot;#fnref:sfence-note&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:13&quot;&gt;
      &lt;p&gt;In particular, when the variable sized region is small, we expect the fixed region write to always hit in L1 or L2 (since the total working set fits in L2), with a ratio approaching 1:3 as the variable region goes to zero. When the variable region is large, we expect many fixed region writes to hit in L2 and less frequently L1, but some will miss even in L2 as the working set is larger than L2 and with random writes some fixed region lines will be evicted before they are written again. The cache related performance counters agree with this hand-waving explanation. &lt;a href=&quot;#fnref:13&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:17&quot;&gt;
      &lt;p&gt;&lt;em&gt;Senior stores&lt;/em&gt; are stores that have retired (the instruction has been completed in the &lt;abbr title=&quot;Out-of-order execution allows CPUs to execute instructions out of order with respect to the source.&quot;&gt;out-of-order&lt;/abbr&gt; engine), but whose value hasn’t yet been committed to the memory subsystem and hence are not globally observable. &lt;a href=&quot;#fnref:17&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:patent-note&quot;&gt;
      &lt;p&gt;This patent is interesting not least because the title is “Apparatus and method for store address for store address prefetch &lt;strong&gt;and line locking&lt;/strong&gt;”, but as far as I can tell the latter part about “line locking” is never mentioned again in the body of the patent. One might imagine that line locking involves something like delaying or nacking incoming snoops for a line that is about to be written. &lt;a href=&quot;#fnref:patent-note&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:wc-note&quot;&gt;
      &lt;p&gt;It is an open question whether normal writes which miss in L1 simply wait in the store buffer until the &lt;abbr title=&quot;Request for ownership: when a request for a cache line originates from a store, or a type of prefetch that predicts the location is likely to be the target of a store, an RFO is performed which gets the line in an exclusive MESI state.&quot;&gt;RFO&lt;/abbr&gt; request is complete, or whether they instead get stashed in a write combining buffer associated with the cache line, potentially collecting several store misses to the same line. The latter sounds more efficient but any combining of stores out of order with respect to the store buffer is problematic: committing multiple such WC buffers when the line is available in L1 could change the apparent order of stores unless all WC buffers are committed as a unit or some other approach is taken. &lt;a href=&quot;#fnref:wc-note&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:wc-stores&quot;&gt;
      &lt;p&gt;I am not 100% sure this is the mechanism, versus an alternative of say stalling the store buffer with the missing store at the head, until the fill buffer returns, but we have evidence both in wording from the Intel manuals, and &lt;a href=&quot;https://stackoverflow.com/a/53438221&quot;&gt;this answer on StackOverflow&lt;/a&gt;. The main argument in favor of this implementation would be performance: it prevents the store buffer from stalling, allowing more stores to commit or start additional requests to memory and keeping the store buffer smaller to avoid stalling the front-end. The main argument against is that it seems hard to maintain ordering in this scenario: if a stream of stores is coalesced into more than one fill buffer, the relative order between the stores is lost, and it is not in general possible to commit the store buffers to L1 “one at a time” while preserving the original store order, you’d basically have to commit all the fill buffers at once (atomically wrt the outside world), or put limits on what stores can be coalesced. &lt;a href=&quot;#fnref:wc-stores&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:prefetchw&quot;&gt;
      &lt;p&gt;The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;prefetchw&lt;/code&gt; instruction has long been supported by AMD, but on Intel it is only supported since Broadwell. Earlier Intel chips didn’t implement this functionality, but the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;prefetchw&lt;/code&gt; opcode was still accepted and executed as a no-op. &lt;a href=&quot;#fnref:prefetchw&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:11&quot;&gt;
      &lt;p&gt;I did notice &lt;em&gt;some&lt;/em&gt; differences when removing the aliasing: for example, sfenceA and sfenceB converged and finally performed the same as the region size increased, rather than sfenceB crossing over and being several cycles faster than sfenceA. &lt;a href=&quot;#fnref:11&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:noenjoy&quot;&gt;
      &lt;p&gt;On the other hand, if you didn’t enjoy it, I have to wonder how you made it this far! Maybe you started off liking it, then felt invested and compelled to finish it? Beware the sunk cost fallacy! &lt;a href=&quot;#fnref:noenjoy&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><author><name>Travis Downs</name><email>travis.downs@gmail.com</email></author><category term="blog" /><category term="intel" /><category term="memory" /><category term="performance" /><summary type="html">Microcode Mystery</summary></entry></feed>